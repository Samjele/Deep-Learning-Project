{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12533e4d",
   "metadata": {},
   "source": [
    "# Triage Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10cb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32a5cb",
   "metadata": {},
   "source": [
    "## Get and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54905a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"pre-processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310dad2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6697 entries, 0 to 6696\n",
      "Data columns (total 53 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   esi                     6697 non-null   int64  \n",
      " 1   age                     6697 non-null   int64  \n",
      " 2   gender                  6697 non-null   object \n",
      " 3   bronchitis              6697 non-null   int64  \n",
      " 4   cardiaarrst             6697 non-null   int64  \n",
      " 5   cardiacanom             6697 non-null   int64  \n",
      " 6   carditis                6697 non-null   int64  \n",
      " 7   chestpain               6697 non-null   int64  \n",
      " 8   chfnonhp                6697 non-null   int64  \n",
      " 9   copd                    6697 non-null   int64  \n",
      " 10  cysticfibro             6697 non-null   int64  \n",
      " 11  dizziness               6697 non-null   int64  \n",
      " 12  dysrhythmia             6697 non-null   int64  \n",
      " 13  fatigue                 6697 non-null   int64  \n",
      " 14  gastritis               6697 non-null   int64  \n",
      " 15  hepatitis               6697 non-null   int64  \n",
      " 16  htn                     6697 non-null   int64  \n",
      " 17  htncomplicn             6697 non-null   int64  \n",
      " 18  hyperlipidem            6697 non-null   int64  \n",
      " 19  influenza               6697 non-null   int64  \n",
      " 20  nauseavomit             6697 non-null   int64  \n",
      " 21  othheartdx              6697 non-null   int64  \n",
      " 22  phlebitis               6697 non-null   int64  \n",
      " 23  pulmhartdx              6697 non-null   int64  \n",
      " 24  syncope                 6697 non-null   int64  \n",
      " 25  glucose_median          6697 non-null   float64\n",
      " 26  triage_vital_hr         6697 non-null   int64  \n",
      " 27  triage_vital_sbp        6697 non-null   int64  \n",
      " 28  triage_vital_o2         6697 non-null   float64\n",
      " 29  triage_vital_temp       6697 non-null   float64\n",
      " 30  cc_asthma               6697 non-null   int64  \n",
      " 31  cc_backpain             6697 non-null   int64  \n",
      " 32  cc_breathingdifficulty  6697 non-null   int64  \n",
      " 33  cc_breathingproblem     6697 non-null   int64  \n",
      " 34  cc_cardiacarrest        6697 non-null   int64  \n",
      " 35  cc_chestpain            6697 non-null   int64  \n",
      " 36  cc_chesttightness       6697 non-null   int64  \n",
      " 37  cc_cough                6697 non-null   int64  \n",
      " 38  cc_dizziness            6697 non-null   int64  \n",
      " 39  cc_fatigue              6697 non-null   int64  \n",
      " 40  cc_fever                6697 non-null   int64  \n",
      " 41  cc_hyperglycemia        6697 non-null   int64  \n",
      " 42  cc_hypertension         6697 non-null   int64  \n",
      " 43  cc_hypotension          6697 non-null   int64  \n",
      " 44  cc_influenza            6697 non-null   int64  \n",
      " 45  cc_nausea               6697 non-null   int64  \n",
      " 46  cc_numbness             6697 non-null   int64  \n",
      " 47  cc_palpitations         6697 non-null   int64  \n",
      " 48  cc_rapidheartrate       6697 non-null   int64  \n",
      " 49  cc_shortnessofbreath    6697 non-null   int64  \n",
      " 50  cc_strokealert          6697 non-null   int64  \n",
      " 51  cc_syncope              6697 non-null   int64  \n",
      " 52  cc_weakness             6697 non-null   int64  \n",
      "dtypes: float64(3), int64(49), object(1)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1714a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>esi</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>bronchitis</th>\n",
       "      <th>cardiaarrst</th>\n",
       "      <th>cardiacanom</th>\n",
       "      <th>carditis</th>\n",
       "      <th>chestpain</th>\n",
       "      <th>chfnonhp</th>\n",
       "      <th>copd</th>\n",
       "      <th>...</th>\n",
       "      <th>cc_hypotension</th>\n",
       "      <th>cc_influenza</th>\n",
       "      <th>cc_nausea</th>\n",
       "      <th>cc_numbness</th>\n",
       "      <th>cc_palpitations</th>\n",
       "      <th>cc_rapidheartrate</th>\n",
       "      <th>cc_shortnessofbreath</th>\n",
       "      <th>cc_strokealert</th>\n",
       "      <th>cc_syncope</th>\n",
       "      <th>cc_weakness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   esi  age gender  bronchitis  cardiaarrst  cardiacanom  carditis  chestpain  \\\n",
       "0    3   66   Male           0            0            0         0          1   \n",
       "1    3   79   Male           0            0            0         0          1   \n",
       "2    3   79   Male           0            0            0         0          1   \n",
       "3    3   80   Male           0            0            0         0          1   \n",
       "4    3   80   Male           0            0            0         0          1   \n",
       "\n",
       "   chfnonhp  copd  ...  cc_hypotension  cc_influenza  cc_nausea  cc_numbness  \\\n",
       "0         1     0  ...               0             0          0            0   \n",
       "1         0     0  ...               0             0          0            0   \n",
       "2         0     0  ...               0             0          0            0   \n",
       "3         0     0  ...               0             0          0            0   \n",
       "4         0     0  ...               0             0          0            0   \n",
       "\n",
       "   cc_palpitations  cc_rapidheartrate  cc_shortnessofbreath  cc_strokealert  \\\n",
       "0                0                  0                     0               0   \n",
       "1                0                  0                     0               0   \n",
       "2                0                  0                     0               0   \n",
       "3                0                  0                     0               0   \n",
       "4                0                  0                     0               0   \n",
       "\n",
       "   cc_syncope  cc_weakness  \n",
       "0           0            0  \n",
       "1           0            0  \n",
       "2           0            0  \n",
       "3           0            0  \n",
       "4           0            0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73f075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.disposition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498c8d5",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b11aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38424ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset\n",
    "class TriageDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, file_path = \"Hospital Triage and Patient History.csv\"):\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Clean data (convert categorical to numeric, remove department name)\n",
    "        df.drop(columns=['dep_name'], inplace=True)\n",
    "        for col in df:\n",
    "            dt = df[col].dtype \n",
    "            if dt == int or dt == float:\n",
    "                df[col].fillna(0, inplace=True)\n",
    "            else:\n",
    "                df[col].fillna(\"\", inplace=True)\n",
    "        \n",
    "        categorical = []\n",
    "        for (key, value) in df.dtypes.items():\n",
    "            if (value == 'object'):\n",
    "                df[key] = df[key].astype('category')\n",
    "                categorical.append(key)\n",
    "                category_num = f'{key}_num'\n",
    "                df[category_num] = df[key].cat.codes.astype('float64')\n",
    "            elif (value == 'int64'):\n",
    "                df[key] = df[key].astype('float64')\n",
    "        \n",
    "        df = df.drop(columns=categorical)\n",
    "        label_ind = df.columns.get_loc('disposition_num')\n",
    "        features_ind = [i for i in range(df.shape[1])]\n",
    "        features_ind.remove(label_ind)\n",
    "        \n",
    "        # Set features and label\n",
    "        self.data = df\n",
    "        self.features = df.iloc[:, features_ind]\n",
    "        self.label = df.iloc[:, label_ind]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.features.iloc[index].values\n",
    "        y = self.label.iloc[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e897978a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create train and test datasets\n",
    "triage_dataset = TriageDataset()\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(triage_dataset, [0.8, 0.2], generator=generator)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d358f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SmallModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(970, 300) # 972 - 2 features\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(300, 100)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(100, 30)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.linear4 = torch.nn.Linear(30, 10)\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        self.linear5 = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.linear4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6276b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = SmallModel().to(device)\n",
    "# optimizer = torch.optim.Adam(small_model.parameters(), lr= 0.005, weight_decay= 0.001)\n",
    "# optimizer = torch.optim.Adam(small_model.parameters(), lr= 0.01, weight_decay= 0.005)\n",
    "optimizer = torch.optim.Adam(small_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e86bfc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "def trainer(model, train_loader, test_loader, num_epochs, optimizer):\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = small_model(features)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 1 == 0:\n",
    "                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in train_loader:\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        train_accuracy = correct/total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.4f}')\n",
    "        train_accs.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_accuracy = correct/total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {test_accuracy:.4f}')\n",
    "        test_accs.append(test_accuracy)\n",
    "    \n",
    "    return train_accs, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7201fbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/3504], Loss: 4.6991\n",
      "Epoch [1/1], Step [2/3504], Loss: 2.0554\n",
      "Epoch [1/1], Step [3/3504], Loss: 1.6278\n",
      "Epoch [1/1], Step [4/3504], Loss: 1.2485\n",
      "Epoch [1/1], Step [5/3504], Loss: 1.0909\n",
      "Epoch [1/1], Step [6/3504], Loss: 1.1258\n",
      "Epoch [1/1], Step [7/3504], Loss: 0.8954\n",
      "Epoch [1/1], Step [8/3504], Loss: 0.8939\n",
      "Epoch [1/1], Step [9/3504], Loss: 0.7887\n",
      "Epoch [1/1], Step [10/3504], Loss: 0.7711\n",
      "Epoch [1/1], Step [11/3504], Loss: 0.8744\n",
      "Epoch [1/1], Step [12/3504], Loss: 0.7351\n",
      "Epoch [1/1], Step [13/3504], Loss: 1.0488\n",
      "Epoch [1/1], Step [14/3504], Loss: 0.7797\n",
      "Epoch [1/1], Step [15/3504], Loss: 1.5159\n",
      "Epoch [1/1], Step [16/3504], Loss: 0.7280\n",
      "Epoch [1/1], Step [17/3504], Loss: 0.6958\n",
      "Epoch [1/1], Step [18/3504], Loss: 0.7316\n",
      "Epoch [1/1], Step [19/3504], Loss: 0.7513\n",
      "Epoch [1/1], Step [20/3504], Loss: 0.6588\n",
      "Epoch [1/1], Step [21/3504], Loss: 0.6386\n",
      "Epoch [1/1], Step [22/3504], Loss: 0.6673\n",
      "Epoch [1/1], Step [23/3504], Loss: 0.5934\n",
      "Epoch [1/1], Step [24/3504], Loss: 0.6136\n",
      "Epoch [1/1], Step [25/3504], Loss: 0.5984\n",
      "Epoch [1/1], Step [26/3504], Loss: 0.6162\n",
      "Epoch [1/1], Step [27/3504], Loss: 0.6706\n",
      "Epoch [1/1], Step [28/3504], Loss: 0.7439\n",
      "Epoch [1/1], Step [29/3504], Loss: 0.7980\n",
      "Epoch [1/1], Step [30/3504], Loss: 0.5974\n",
      "Epoch [1/1], Step [31/3504], Loss: 0.5200\n",
      "Epoch [1/1], Step [32/3504], Loss: 1.0995\n",
      "Epoch [1/1], Step [33/3504], Loss: 0.5811\n",
      "Epoch [1/1], Step [34/3504], Loss: 0.6226\n",
      "Epoch [1/1], Step [35/3504], Loss: 0.5968\n",
      "Epoch [1/1], Step [36/3504], Loss: 0.4942\n",
      "Epoch [1/1], Step [37/3504], Loss: 0.6012\n",
      "Epoch [1/1], Step [38/3504], Loss: 0.8656\n",
      "Epoch [1/1], Step [39/3504], Loss: 0.5208\n",
      "Epoch [1/1], Step [40/3504], Loss: 0.6509\n",
      "Epoch [1/1], Step [41/3504], Loss: 0.7832\n",
      "Epoch [1/1], Step [42/3504], Loss: 0.6440\n",
      "Epoch [1/1], Step [43/3504], Loss: 0.7317\n",
      "Epoch [1/1], Step [44/3504], Loss: 0.6290\n",
      "Epoch [1/1], Step [45/3504], Loss: 0.5046\n",
      "Epoch [1/1], Step [46/3504], Loss: 0.5709\n",
      "Epoch [1/1], Step [47/3504], Loss: 0.6174\n",
      "Epoch [1/1], Step [48/3504], Loss: 0.5866\n",
      "Epoch [1/1], Step [49/3504], Loss: 0.5205\n",
      "Epoch [1/1], Step [50/3504], Loss: 0.5480\n",
      "Epoch [1/1], Step [51/3504], Loss: 0.5430\n",
      "Epoch [1/1], Step [52/3504], Loss: 0.5657\n",
      "Epoch [1/1], Step [53/3504], Loss: 0.5574\n",
      "Epoch [1/1], Step [54/3504], Loss: 0.5093\n",
      "Epoch [1/1], Step [55/3504], Loss: 0.6293\n",
      "Epoch [1/1], Step [56/3504], Loss: 0.6432\n",
      "Epoch [1/1], Step [57/3504], Loss: 0.6414\n",
      "Epoch [1/1], Step [58/3504], Loss: 0.5833\n",
      "Epoch [1/1], Step [59/3504], Loss: 0.5694\n",
      "Epoch [1/1], Step [60/3504], Loss: 0.6264\n",
      "Epoch [1/1], Step [61/3504], Loss: 0.5302\n",
      "Epoch [1/1], Step [62/3504], Loss: 0.5527\n",
      "Epoch [1/1], Step [63/3504], Loss: 0.5082\n",
      "Epoch [1/1], Step [64/3504], Loss: 0.5499\n",
      "Epoch [1/1], Step [65/3504], Loss: 0.5269\n",
      "Epoch [1/1], Step [66/3504], Loss: 0.4517\n",
      "Epoch [1/1], Step [67/3504], Loss: 0.5921\n",
      "Epoch [1/1], Step [68/3504], Loss: 0.8358\n",
      "Epoch [1/1], Step [69/3504], Loss: 0.5321\n",
      "Epoch [1/1], Step [70/3504], Loss: 0.5463\n",
      "Epoch [1/1], Step [71/3504], Loss: 0.5113\n",
      "Epoch [1/1], Step [72/3504], Loss: 0.5882\n",
      "Epoch [1/1], Step [73/3504], Loss: 0.6978\n",
      "Epoch [1/1], Step [74/3504], Loss: 0.5941\n",
      "Epoch [1/1], Step [75/3504], Loss: 0.6929\n",
      "Epoch [1/1], Step [76/3504], Loss: 0.5598\n",
      "Epoch [1/1], Step [77/3504], Loss: 0.6193\n",
      "Epoch [1/1], Step [78/3504], Loss: 0.5398\n",
      "Epoch [1/1], Step [79/3504], Loss: 0.4742\n",
      "Epoch [1/1], Step [80/3504], Loss: 0.5103\n",
      "Epoch [1/1], Step [81/3504], Loss: 0.5873\n",
      "Epoch [1/1], Step [82/3504], Loss: 0.7240\n",
      "Epoch [1/1], Step [83/3504], Loss: 0.5157\n",
      "Epoch [1/1], Step [84/3504], Loss: 0.6313\n",
      "Epoch [1/1], Step [85/3504], Loss: 0.4821\n",
      "Epoch [1/1], Step [86/3504], Loss: 0.5357\n",
      "Epoch [1/1], Step [87/3504], Loss: 0.8311\n",
      "Epoch [1/1], Step [88/3504], Loss: 0.4936\n",
      "Epoch [1/1], Step [89/3504], Loss: 0.5651\n",
      "Epoch [1/1], Step [90/3504], Loss: 0.5473\n",
      "Epoch [1/1], Step [91/3504], Loss: 0.5562\n",
      "Epoch [1/1], Step [92/3504], Loss: 0.4944\n",
      "Epoch [1/1], Step [93/3504], Loss: 0.6394\n",
      "Epoch [1/1], Step [94/3504], Loss: 0.4808\n",
      "Epoch [1/1], Step [95/3504], Loss: 0.5068\n",
      "Epoch [1/1], Step [96/3504], Loss: 0.5303\n",
      "Epoch [1/1], Step [97/3504], Loss: 0.5808\n",
      "Epoch [1/1], Step [98/3504], Loss: 0.4651\n",
      "Epoch [1/1], Step [99/3504], Loss: 0.6043\n",
      "Epoch [1/1], Step [100/3504], Loss: 0.6107\n",
      "Epoch [1/1], Step [101/3504], Loss: 0.5600\n",
      "Epoch [1/1], Step [102/3504], Loss: 0.6043\n",
      "Epoch [1/1], Step [103/3504], Loss: 0.6287\n",
      "Epoch [1/1], Step [104/3504], Loss: 0.4877\n",
      "Epoch [1/1], Step [105/3504], Loss: 0.6160\n",
      "Epoch [1/1], Step [106/3504], Loss: 0.6577\n",
      "Epoch [1/1], Step [107/3504], Loss: 0.5636\n",
      "Epoch [1/1], Step [108/3504], Loss: 0.5211\n",
      "Epoch [1/1], Step [109/3504], Loss: 0.5931\n",
      "Epoch [1/1], Step [110/3504], Loss: 0.5541\n",
      "Epoch [1/1], Step [111/3504], Loss: 2.4700\n",
      "Epoch [1/1], Step [112/3504], Loss: 0.5676\n",
      "Epoch [1/1], Step [113/3504], Loss: 0.6595\n",
      "Epoch [1/1], Step [114/3504], Loss: 0.5108\n",
      "Epoch [1/1], Step [115/3504], Loss: 0.8556\n",
      "Epoch [1/1], Step [116/3504], Loss: 0.4975\n",
      "Epoch [1/1], Step [117/3504], Loss: 0.5707\n",
      "Epoch [1/1], Step [118/3504], Loss: 0.5997\n",
      "Epoch [1/1], Step [119/3504], Loss: 0.6085\n",
      "Epoch [1/1], Step [120/3504], Loss: 0.7873\n",
      "Epoch [1/1], Step [121/3504], Loss: 3.2514\n",
      "Epoch [1/1], Step [122/3504], Loss: 0.4930\n",
      "Epoch [1/1], Step [123/3504], Loss: 0.4993\n",
      "Epoch [1/1], Step [124/3504], Loss: 0.6305\n",
      "Epoch [1/1], Step [125/3504], Loss: 0.4981\n",
      "Epoch [1/1], Step [126/3504], Loss: 0.5932\n",
      "Epoch [1/1], Step [127/3504], Loss: 0.4853\n",
      "Epoch [1/1], Step [128/3504], Loss: 0.5233\n",
      "Epoch [1/1], Step [129/3504], Loss: 0.5139\n",
      "Epoch [1/1], Step [130/3504], Loss: 0.4564\n",
      "Epoch [1/1], Step [131/3504], Loss: 0.5116\n",
      "Epoch [1/1], Step [132/3504], Loss: 0.5446\n",
      "Epoch [1/1], Step [133/3504], Loss: 0.4750\n",
      "Epoch [1/1], Step [134/3504], Loss: 0.4739\n",
      "Epoch [1/1], Step [135/3504], Loss: 0.5426\n",
      "Epoch [1/1], Step [136/3504], Loss: 0.5091\n",
      "Epoch [1/1], Step [137/3504], Loss: 0.5523\n",
      "Epoch [1/1], Step [138/3504], Loss: 0.4965\n",
      "Epoch [1/1], Step [139/3504], Loss: 0.5170\n",
      "Epoch [1/1], Step [140/3504], Loss: 0.4022\n",
      "Epoch [1/1], Step [141/3504], Loss: 0.5598\n",
      "Epoch [1/1], Step [142/3504], Loss: 0.4453\n",
      "Epoch [1/1], Step [143/3504], Loss: 0.4451\n",
      "Epoch [1/1], Step [144/3504], Loss: 0.5341\n",
      "Epoch [1/1], Step [145/3504], Loss: 0.4376\n",
      "Epoch [1/1], Step [146/3504], Loss: 0.5708\n",
      "Epoch [1/1], Step [147/3504], Loss: 0.5553\n",
      "Epoch [1/1], Step [148/3504], Loss: 0.5263\n",
      "Epoch [1/1], Step [149/3504], Loss: 0.5763\n",
      "Epoch [1/1], Step [150/3504], Loss: 0.5157\n",
      "Epoch [1/1], Step [151/3504], Loss: 0.5489\n",
      "Epoch [1/1], Step [152/3504], Loss: 0.4978\n",
      "Epoch [1/1], Step [153/3504], Loss: 0.6906\n",
      "Epoch [1/1], Step [154/3504], Loss: 0.4824\n",
      "Epoch [1/1], Step [155/3504], Loss: 0.4355\n",
      "Epoch [1/1], Step [156/3504], Loss: 0.5882\n",
      "Epoch [1/1], Step [157/3504], Loss: 0.4694\n",
      "Epoch [1/1], Step [158/3504], Loss: 0.5958\n",
      "Epoch [1/1], Step [159/3504], Loss: 0.5585\n",
      "Epoch [1/1], Step [160/3504], Loss: 0.5954\n",
      "Epoch [1/1], Step [161/3504], Loss: 0.4638\n",
      "Epoch [1/1], Step [162/3504], Loss: 0.4531\n",
      "Epoch [1/1], Step [163/3504], Loss: 0.6074\n",
      "Epoch [1/1], Step [164/3504], Loss: 0.4598\n",
      "Epoch [1/1], Step [165/3504], Loss: 0.4895\n",
      "Epoch [1/1], Step [166/3504], Loss: 0.4919\n",
      "Epoch [1/1], Step [167/3504], Loss: 0.4484\n",
      "Epoch [1/1], Step [168/3504], Loss: 0.4576\n",
      "Epoch [1/1], Step [169/3504], Loss: 0.7741\n",
      "Epoch [1/1], Step [170/3504], Loss: 0.5608\n",
      "Epoch [1/1], Step [171/3504], Loss: 0.4586\n",
      "Epoch [1/1], Step [172/3504], Loss: 0.4895\n",
      "Epoch [1/1], Step [173/3504], Loss: 0.4928\n",
      "Epoch [1/1], Step [174/3504], Loss: 0.5195\n",
      "Epoch [1/1], Step [175/3504], Loss: 0.4139\n",
      "Epoch [1/1], Step [176/3504], Loss: 0.6076\n",
      "Epoch [1/1], Step [177/3504], Loss: 0.5127\n",
      "Epoch [1/1], Step [178/3504], Loss: 0.5778\n",
      "Epoch [1/1], Step [179/3504], Loss: 0.4847\n",
      "Epoch [1/1], Step [180/3504], Loss: 0.4653\n",
      "Epoch [1/1], Step [181/3504], Loss: 0.4209\n",
      "Epoch [1/1], Step [182/3504], Loss: 0.3933\n",
      "Epoch [1/1], Step [183/3504], Loss: 0.4382\n",
      "Epoch [1/1], Step [184/3504], Loss: 0.4475\n",
      "Epoch [1/1], Step [185/3504], Loss: 0.7438\n",
      "Epoch [1/1], Step [186/3504], Loss: 0.8075\n",
      "Epoch [1/1], Step [187/3504], Loss: 0.4871\n",
      "Epoch [1/1], Step [188/3504], Loss: 0.5098\n",
      "Epoch [1/1], Step [189/3504], Loss: 0.5338\n",
      "Epoch [1/1], Step [190/3504], Loss: 0.4673\n",
      "Epoch [1/1], Step [191/3504], Loss: 0.5743\n",
      "Epoch [1/1], Step [192/3504], Loss: 0.3946\n",
      "Epoch [1/1], Step [193/3504], Loss: 0.4567\n",
      "Epoch [1/1], Step [194/3504], Loss: 0.4273\n",
      "Epoch [1/1], Step [195/3504], Loss: 0.4759\n",
      "Epoch [1/1], Step [196/3504], Loss: 0.5782\n",
      "Epoch [1/1], Step [197/3504], Loss: 0.5947\n",
      "Epoch [1/1], Step [198/3504], Loss: 0.6501\n",
      "Epoch [1/1], Step [199/3504], Loss: 0.5194\n",
      "Epoch [1/1], Step [200/3504], Loss: 0.4309\n",
      "Epoch [1/1], Step [201/3504], Loss: 0.6737\n",
      "Epoch [1/1], Step [202/3504], Loss: 0.6183\n",
      "Epoch [1/1], Step [203/3504], Loss: 0.4660\n",
      "Epoch [1/1], Step [204/3504], Loss: 0.4229\n",
      "Epoch [1/1], Step [205/3504], Loss: 0.5123\n",
      "Epoch [1/1], Step [206/3504], Loss: 0.4168\n",
      "Epoch [1/1], Step [207/3504], Loss: 0.4593\n",
      "Epoch [1/1], Step [208/3504], Loss: 0.5908\n",
      "Epoch [1/1], Step [209/3504], Loss: 0.6323\n",
      "Epoch [1/1], Step [210/3504], Loss: 0.6728\n",
      "Epoch [1/1], Step [211/3504], Loss: 0.4387\n",
      "Epoch [1/1], Step [212/3504], Loss: 0.5558\n",
      "Epoch [1/1], Step [213/3504], Loss: 0.5238\n",
      "Epoch [1/1], Step [214/3504], Loss: 0.4445\n",
      "Epoch [1/1], Step [215/3504], Loss: 2.0310\n",
      "Epoch [1/1], Step [216/3504], Loss: 0.8945\n",
      "Epoch [1/1], Step [217/3504], Loss: 0.5370\n",
      "Epoch [1/1], Step [218/3504], Loss: 0.5257\n",
      "Epoch [1/1], Step [219/3504], Loss: 0.5752\n",
      "Epoch [1/1], Step [220/3504], Loss: 0.5509\n",
      "Epoch [1/1], Step [221/3504], Loss: 0.4889\n",
      "Epoch [1/1], Step [222/3504], Loss: 0.5716\n",
      "Epoch [1/1], Step [223/3504], Loss: 0.4467\n",
      "Epoch [1/1], Step [224/3504], Loss: 0.5152\n",
      "Epoch [1/1], Step [225/3504], Loss: 0.5603\n",
      "Epoch [1/1], Step [226/3504], Loss: 0.5344\n",
      "Epoch [1/1], Step [227/3504], Loss: 0.4378\n",
      "Epoch [1/1], Step [228/3504], Loss: 0.5054\n",
      "Epoch [1/1], Step [229/3504], Loss: 0.4919\n",
      "Epoch [1/1], Step [230/3504], Loss: 0.5557\n",
      "Epoch [1/1], Step [231/3504], Loss: 0.5943\n",
      "Epoch [1/1], Step [232/3504], Loss: 0.5082\n",
      "Epoch [1/1], Step [233/3504], Loss: 0.4775\n",
      "Epoch [1/1], Step [234/3504], Loss: 0.4541\n",
      "Epoch [1/1], Step [235/3504], Loss: 0.5705\n",
      "Epoch [1/1], Step [236/3504], Loss: 0.4930\n",
      "Epoch [1/1], Step [237/3504], Loss: 0.4802\n",
      "Epoch [1/1], Step [238/3504], Loss: 0.5169\n",
      "Epoch [1/1], Step [239/3504], Loss: 0.4705\n",
      "Epoch [1/1], Step [240/3504], Loss: 0.5389\n",
      "Epoch [1/1], Step [241/3504], Loss: 0.5129\n",
      "Epoch [1/1], Step [242/3504], Loss: 0.4749\n",
      "Epoch [1/1], Step [243/3504], Loss: 0.4942\n",
      "Epoch [1/1], Step [244/3504], Loss: 0.4939\n",
      "Epoch [1/1], Step [245/3504], Loss: 0.3921\n",
      "Epoch [1/1], Step [246/3504], Loss: 0.4723\n",
      "Epoch [1/1], Step [247/3504], Loss: 0.4382\n",
      "Epoch [1/1], Step [248/3504], Loss: 0.4448\n",
      "Epoch [1/1], Step [249/3504], Loss: 0.5108\n",
      "Epoch [1/1], Step [250/3504], Loss: 0.4689\n",
      "Epoch [1/1], Step [251/3504], Loss: 0.5371\n",
      "Epoch [1/1], Step [252/3504], Loss: 0.5532\n",
      "Epoch [1/1], Step [253/3504], Loss: 0.4259\n",
      "Epoch [1/1], Step [254/3504], Loss: 0.4794\n",
      "Epoch [1/1], Step [255/3504], Loss: 0.4789\n",
      "Epoch [1/1], Step [256/3504], Loss: 0.4804\n",
      "Epoch [1/1], Step [257/3504], Loss: 0.4908\n",
      "Epoch [1/1], Step [258/3504], Loss: 0.5472\n",
      "Epoch [1/1], Step [259/3504], Loss: 0.7098\n",
      "Epoch [1/1], Step [260/3504], Loss: 0.4613\n",
      "Epoch [1/1], Step [261/3504], Loss: 0.4558\n",
      "Epoch [1/1], Step [262/3504], Loss: 0.4557\n",
      "Epoch [1/1], Step [263/3504], Loss: 0.4974\n",
      "Epoch [1/1], Step [264/3504], Loss: 0.4163\n",
      "Epoch [1/1], Step [265/3504], Loss: 0.5481\n",
      "Epoch [1/1], Step [266/3504], Loss: 0.5255\n",
      "Epoch [1/1], Step [267/3504], Loss: 0.4517\n",
      "Epoch [1/1], Step [268/3504], Loss: 0.3512\n",
      "Epoch [1/1], Step [269/3504], Loss: 0.4632\n",
      "Epoch [1/1], Step [270/3504], Loss: 0.4901\n",
      "Epoch [1/1], Step [271/3504], Loss: 0.4335\n",
      "Epoch [1/1], Step [272/3504], Loss: 0.5623\n",
      "Epoch [1/1], Step [273/3504], Loss: 0.4191\n",
      "Epoch [1/1], Step [274/3504], Loss: 0.4137\n",
      "Epoch [1/1], Step [275/3504], Loss: 0.5559\n",
      "Epoch [1/1], Step [276/3504], Loss: 0.3916\n",
      "Epoch [1/1], Step [277/3504], Loss: 0.4155\n",
      "Epoch [1/1], Step [278/3504], Loss: 0.3940\n",
      "Epoch [1/1], Step [279/3504], Loss: 0.4575\n",
      "Epoch [1/1], Step [280/3504], Loss: 0.4044\n",
      "Epoch [1/1], Step [281/3504], Loss: 0.4739\n",
      "Epoch [1/1], Step [282/3504], Loss: 0.3556\n",
      "Epoch [1/1], Step [283/3504], Loss: 0.5398\n",
      "Epoch [1/1], Step [284/3504], Loss: 0.5183\n",
      "Epoch [1/1], Step [285/3504], Loss: 0.3471\n",
      "Epoch [1/1], Step [286/3504], Loss: 0.4205\n",
      "Epoch [1/1], Step [287/3504], Loss: 0.8254\n",
      "Epoch [1/1], Step [288/3504], Loss: 0.5127\n",
      "Epoch [1/1], Step [289/3504], Loss: 0.3736\n",
      "Epoch [1/1], Step [290/3504], Loss: 0.4775\n",
      "Epoch [1/1], Step [291/3504], Loss: 0.4632\n",
      "Epoch [1/1], Step [292/3504], Loss: 0.4583\n",
      "Epoch [1/1], Step [293/3504], Loss: 0.4573\n",
      "Epoch [1/1], Step [294/3504], Loss: 0.3752\n",
      "Epoch [1/1], Step [295/3504], Loss: 0.4924\n",
      "Epoch [1/1], Step [296/3504], Loss: 0.4603\n",
      "Epoch [1/1], Step [297/3504], Loss: 0.4471\n",
      "Epoch [1/1], Step [298/3504], Loss: 0.4293\n",
      "Epoch [1/1], Step [299/3504], Loss: 0.4173\n",
      "Epoch [1/1], Step [300/3504], Loss: 0.5284\n",
      "Epoch [1/1], Step [301/3504], Loss: 0.4423\n",
      "Epoch [1/1], Step [302/3504], Loss: 0.3874\n",
      "Epoch [1/1], Step [303/3504], Loss: 0.5538\n",
      "Epoch [1/1], Step [304/3504], Loss: 0.4805\n",
      "Epoch [1/1], Step [305/3504], Loss: 0.4000\n",
      "Epoch [1/1], Step [306/3504], Loss: 0.4640\n",
      "Epoch [1/1], Step [307/3504], Loss: 0.4299\n",
      "Epoch [1/1], Step [308/3504], Loss: 0.6067\n",
      "Epoch [1/1], Step [309/3504], Loss: 0.4639\n",
      "Epoch [1/1], Step [310/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [311/3504], Loss: 0.7870\n",
      "Epoch [1/1], Step [312/3504], Loss: 0.4969\n",
      "Epoch [1/1], Step [313/3504], Loss: 0.4371\n",
      "Epoch [1/1], Step [314/3504], Loss: 0.4380\n",
      "Epoch [1/1], Step [315/3504], Loss: 0.5470\n",
      "Epoch [1/1], Step [316/3504], Loss: 0.4681\n",
      "Epoch [1/1], Step [317/3504], Loss: 0.4550\n",
      "Epoch [1/1], Step [318/3504], Loss: 0.5560\n",
      "Epoch [1/1], Step [319/3504], Loss: 0.4670\n",
      "Epoch [1/1], Step [320/3504], Loss: 0.5631\n",
      "Epoch [1/1], Step [321/3504], Loss: 0.4391\n",
      "Epoch [1/1], Step [322/3504], Loss: 0.5053\n",
      "Epoch [1/1], Step [323/3504], Loss: 0.5309\n",
      "Epoch [1/1], Step [324/3504], Loss: 6.5430\n",
      "Epoch [1/1], Step [325/3504], Loss: 0.5259\n",
      "Epoch [1/1], Step [326/3504], Loss: 0.4285\n",
      "Epoch [1/1], Step [327/3504], Loss: 0.5759\n",
      "Epoch [1/1], Step [328/3504], Loss: 0.6142\n",
      "Epoch [1/1], Step [329/3504], Loss: 0.4636\n",
      "Epoch [1/1], Step [330/3504], Loss: 0.6097\n",
      "Epoch [1/1], Step [331/3504], Loss: 0.4081\n",
      "Epoch [1/1], Step [332/3504], Loss: 0.5322\n",
      "Epoch [1/1], Step [333/3504], Loss: 0.5479\n",
      "Epoch [1/1], Step [334/3504], Loss: 0.4851\n",
      "Epoch [1/1], Step [335/3504], Loss: 0.9008\n",
      "Epoch [1/1], Step [336/3504], Loss: 0.5604\n",
      "Epoch [1/1], Step [337/3504], Loss: 0.4014\n",
      "Epoch [1/1], Step [338/3504], Loss: 0.5668\n",
      "Epoch [1/1], Step [339/3504], Loss: 0.4719\n",
      "Epoch [1/1], Step [340/3504], Loss: 0.4216\n",
      "Epoch [1/1], Step [341/3504], Loss: 0.3731\n",
      "Epoch [1/1], Step [342/3504], Loss: 0.5040\n",
      "Epoch [1/1], Step [343/3504], Loss: 0.6045\n",
      "Epoch [1/1], Step [344/3504], Loss: 0.3898\n",
      "Epoch [1/1], Step [345/3504], Loss: 0.5054\n",
      "Epoch [1/1], Step [346/3504], Loss: 0.5462\n",
      "Epoch [1/1], Step [347/3504], Loss: 0.4837\n",
      "Epoch [1/1], Step [348/3504], Loss: 0.4473\n",
      "Epoch [1/1], Step [349/3504], Loss: 0.4560\n",
      "Epoch [1/1], Step [350/3504], Loss: 0.6007\n",
      "Epoch [1/1], Step [351/3504], Loss: 0.4547\n",
      "Epoch [1/1], Step [352/3504], Loss: 0.4551\n",
      "Epoch [1/1], Step [353/3504], Loss: 0.4607\n",
      "Epoch [1/1], Step [354/3504], Loss: 0.4383\n",
      "Epoch [1/1], Step [355/3504], Loss: 0.4268\n",
      "Epoch [1/1], Step [356/3504], Loss: 0.4730\n",
      "Epoch [1/1], Step [357/3504], Loss: 0.4933\n",
      "Epoch [1/1], Step [358/3504], Loss: 0.4599\n",
      "Epoch [1/1], Step [359/3504], Loss: 0.3641\n",
      "Epoch [1/1], Step [360/3504], Loss: 0.3828\n",
      "Epoch [1/1], Step [361/3504], Loss: 0.4197\n",
      "Epoch [1/1], Step [362/3504], Loss: 0.4259\n",
      "Epoch [1/1], Step [363/3504], Loss: 0.5556\n",
      "Epoch [1/1], Step [364/3504], Loss: 0.4576\n",
      "Epoch [1/1], Step [365/3504], Loss: 0.5628\n",
      "Epoch [1/1], Step [366/3504], Loss: 0.4368\n",
      "Epoch [1/1], Step [367/3504], Loss: 1.2928\n",
      "Epoch [1/1], Step [368/3504], Loss: 0.5513\n",
      "Epoch [1/1], Step [369/3504], Loss: 0.4606\n",
      "Epoch [1/1], Step [370/3504], Loss: 0.4346\n",
      "Epoch [1/1], Step [371/3504], Loss: 0.4443\n",
      "Epoch [1/1], Step [372/3504], Loss: 0.4366\n",
      "Epoch [1/1], Step [373/3504], Loss: 0.4752\n",
      "Epoch [1/1], Step [374/3504], Loss: 0.5122\n",
      "Epoch [1/1], Step [375/3504], Loss: 0.5034\n",
      "Epoch [1/1], Step [376/3504], Loss: 0.4036\n",
      "Epoch [1/1], Step [377/3504], Loss: 0.4238\n",
      "Epoch [1/1], Step [378/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [379/3504], Loss: 0.4031\n",
      "Epoch [1/1], Step [380/3504], Loss: 0.5302\n",
      "Epoch [1/1], Step [381/3504], Loss: 0.4045\n",
      "Epoch [1/1], Step [382/3504], Loss: 0.5430\n",
      "Epoch [1/1], Step [383/3504], Loss: 0.4112\n",
      "Epoch [1/1], Step [384/3504], Loss: 0.4046\n",
      "Epoch [1/1], Step [385/3504], Loss: 0.7427\n",
      "Epoch [1/1], Step [386/3504], Loss: 0.4849\n",
      "Epoch [1/1], Step [387/3504], Loss: 0.4570\n",
      "Epoch [1/1], Step [388/3504], Loss: 0.3500\n",
      "Epoch [1/1], Step [389/3504], Loss: 0.4051\n",
      "Epoch [1/1], Step [390/3504], Loss: 0.3511\n",
      "Epoch [1/1], Step [391/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [392/3504], Loss: 0.4536\n",
      "Epoch [1/1], Step [393/3504], Loss: 0.4361\n",
      "Epoch [1/1], Step [394/3504], Loss: 0.3538\n",
      "Epoch [1/1], Step [395/3504], Loss: 0.5714\n",
      "Epoch [1/1], Step [396/3504], Loss: 0.4272\n",
      "Epoch [1/1], Step [397/3504], Loss: 0.4685\n",
      "Epoch [1/1], Step [398/3504], Loss: 0.4540\n",
      "Epoch [1/1], Step [399/3504], Loss: 0.5144\n",
      "Epoch [1/1], Step [400/3504], Loss: 0.5526\n",
      "Epoch [1/1], Step [401/3504], Loss: 0.6770\n",
      "Epoch [1/1], Step [402/3504], Loss: 0.4193\n",
      "Epoch [1/1], Step [403/3504], Loss: 0.4532\n",
      "Epoch [1/1], Step [404/3504], Loss: 0.5605\n",
      "Epoch [1/1], Step [405/3504], Loss: 0.4466\n",
      "Epoch [1/1], Step [406/3504], Loss: 0.3791\n",
      "Epoch [1/1], Step [407/3504], Loss: 0.4079\n",
      "Epoch [1/1], Step [408/3504], Loss: 0.4955\n",
      "Epoch [1/1], Step [409/3504], Loss: 0.4145\n",
      "Epoch [1/1], Step [410/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [411/3504], Loss: 0.5128\n",
      "Epoch [1/1], Step [412/3504], Loss: 0.4382\n",
      "Epoch [1/1], Step [413/3504], Loss: 0.4532\n",
      "Epoch [1/1], Step [414/3504], Loss: 0.3754\n",
      "Epoch [1/1], Step [415/3504], Loss: 0.4958\n",
      "Epoch [1/1], Step [416/3504], Loss: 0.3847\n",
      "Epoch [1/1], Step [417/3504], Loss: 0.5881\n",
      "Epoch [1/1], Step [418/3504], Loss: 0.4150\n",
      "Epoch [1/1], Step [419/3504], Loss: 0.3704\n",
      "Epoch [1/1], Step [420/3504], Loss: 0.4404\n",
      "Epoch [1/1], Step [421/3504], Loss: 0.4403\n",
      "Epoch [1/1], Step [422/3504], Loss: 0.3889\n",
      "Epoch [1/1], Step [423/3504], Loss: 0.4564\n",
      "Epoch [1/1], Step [424/3504], Loss: 0.5041\n",
      "Epoch [1/1], Step [425/3504], Loss: 0.4475\n",
      "Epoch [1/1], Step [426/3504], Loss: 0.4503\n",
      "Epoch [1/1], Step [427/3504], Loss: 0.5007\n",
      "Epoch [1/1], Step [428/3504], Loss: 0.3646\n",
      "Epoch [1/1], Step [429/3504], Loss: 0.4309\n",
      "Epoch [1/1], Step [430/3504], Loss: 0.4211\n",
      "Epoch [1/1], Step [431/3504], Loss: 0.3956\n",
      "Epoch [1/1], Step [432/3504], Loss: 0.4820\n",
      "Epoch [1/1], Step [433/3504], Loss: 0.5315\n",
      "Epoch [1/1], Step [434/3504], Loss: 0.3915\n",
      "Epoch [1/1], Step [435/3504], Loss: 0.3865\n",
      "Epoch [1/1], Step [436/3504], Loss: 0.4224\n",
      "Epoch [1/1], Step [437/3504], Loss: 0.4795\n",
      "Epoch [1/1], Step [438/3504], Loss: 0.4583\n",
      "Epoch [1/1], Step [439/3504], Loss: 0.4202\n",
      "Epoch [1/1], Step [440/3504], Loss: 0.3490\n",
      "Epoch [1/1], Step [441/3504], Loss: 0.4287\n",
      "Epoch [1/1], Step [442/3504], Loss: 0.4478\n",
      "Epoch [1/1], Step [443/3504], Loss: 0.3979\n",
      "Epoch [1/1], Step [444/3504], Loss: 0.4719\n",
      "Epoch [1/1], Step [445/3504], Loss: 0.3986\n",
      "Epoch [1/1], Step [446/3504], Loss: 0.4347\n",
      "Epoch [1/1], Step [447/3504], Loss: 0.4801\n",
      "Epoch [1/1], Step [448/3504], Loss: 0.4588\n",
      "Epoch [1/1], Step [449/3504], Loss: 0.4538\n",
      "Epoch [1/1], Step [450/3504], Loss: 0.3918\n",
      "Epoch [1/1], Step [451/3504], Loss: 0.4959\n",
      "Epoch [1/1], Step [452/3504], Loss: 0.3892\n",
      "Epoch [1/1], Step [453/3504], Loss: 0.4832\n",
      "Epoch [1/1], Step [454/3504], Loss: 0.4949\n",
      "Epoch [1/1], Step [455/3504], Loss: 0.4563\n",
      "Epoch [1/1], Step [456/3504], Loss: 0.4852\n",
      "Epoch [1/1], Step [457/3504], Loss: 0.4600\n",
      "Epoch [1/1], Step [458/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [459/3504], Loss: 0.4541\n",
      "Epoch [1/1], Step [460/3504], Loss: 0.4468\n",
      "Epoch [1/1], Step [461/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [462/3504], Loss: 0.4065\n",
      "Epoch [1/1], Step [463/3504], Loss: 0.3724\n",
      "Epoch [1/1], Step [464/3504], Loss: 0.4682\n",
      "Epoch [1/1], Step [465/3504], Loss: 0.4772\n",
      "Epoch [1/1], Step [466/3504], Loss: 0.4882\n",
      "Epoch [1/1], Step [467/3504], Loss: 0.4247\n",
      "Epoch [1/1], Step [468/3504], Loss: 0.3982\n",
      "Epoch [1/1], Step [469/3504], Loss: 0.4156\n",
      "Epoch [1/1], Step [470/3504], Loss: 0.4020\n",
      "Epoch [1/1], Step [471/3504], Loss: 0.4627\n",
      "Epoch [1/1], Step [472/3504], Loss: 0.4553\n",
      "Epoch [1/1], Step [473/3504], Loss: 0.4968\n",
      "Epoch [1/1], Step [474/3504], Loss: 0.4175\n",
      "Epoch [1/1], Step [475/3504], Loss: 0.4373\n",
      "Epoch [1/1], Step [476/3504], Loss: 0.5006\n",
      "Epoch [1/1], Step [477/3504], Loss: 0.3025\n",
      "Epoch [1/1], Step [478/3504], Loss: 0.3809\n",
      "Epoch [1/1], Step [479/3504], Loss: 0.4589\n",
      "Epoch [1/1], Step [480/3504], Loss: 0.4704\n",
      "Epoch [1/1], Step [481/3504], Loss: 0.4665\n",
      "Epoch [1/1], Step [482/3504], Loss: 0.3895\n",
      "Epoch [1/1], Step [483/3504], Loss: 0.4541\n",
      "Epoch [1/1], Step [484/3504], Loss: 0.3891\n",
      "Epoch [1/1], Step [485/3504], Loss: 0.4646\n",
      "Epoch [1/1], Step [486/3504], Loss: 0.3784\n",
      "Epoch [1/1], Step [487/3504], Loss: 0.3786\n",
      "Epoch [1/1], Step [488/3504], Loss: 0.3986\n",
      "Epoch [1/1], Step [489/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [490/3504], Loss: 0.4596\n",
      "Epoch [1/1], Step [491/3504], Loss: 0.4123\n",
      "Epoch [1/1], Step [492/3504], Loss: 0.4369\n",
      "Epoch [1/1], Step [493/3504], Loss: 0.4703\n",
      "Epoch [1/1], Step [494/3504], Loss: 0.4694\n",
      "Epoch [1/1], Step [495/3504], Loss: 0.4272\n",
      "Epoch [1/1], Step [496/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [497/3504], Loss: 0.5366\n",
      "Epoch [1/1], Step [498/3504], Loss: 0.3899\n",
      "Epoch [1/1], Step [499/3504], Loss: 0.4408\n",
      "Epoch [1/1], Step [500/3504], Loss: 0.4336\n",
      "Epoch [1/1], Step [501/3504], Loss: 0.3706\n",
      "Epoch [1/1], Step [502/3504], Loss: 0.4571\n",
      "Epoch [1/1], Step [503/3504], Loss: 0.4578\n",
      "Epoch [1/1], Step [504/3504], Loss: 0.4647\n",
      "Epoch [1/1], Step [505/3504], Loss: 0.4000\n",
      "Epoch [1/1], Step [506/3504], Loss: 0.4490\n",
      "Epoch [1/1], Step [507/3504], Loss: 0.4422\n",
      "Epoch [1/1], Step [508/3504], Loss: 0.3784\n",
      "Epoch [1/1], Step [509/3504], Loss: 0.4438\n",
      "Epoch [1/1], Step [510/3504], Loss: 0.3630\n",
      "Epoch [1/1], Step [511/3504], Loss: 0.4223\n",
      "Epoch [1/1], Step [512/3504], Loss: 0.4152\n",
      "Epoch [1/1], Step [513/3504], Loss: 0.4079\n",
      "Epoch [1/1], Step [514/3504], Loss: 0.4997\n",
      "Epoch [1/1], Step [515/3504], Loss: 0.3341\n",
      "Epoch [1/1], Step [516/3504], Loss: 0.3581\n",
      "Epoch [1/1], Step [517/3504], Loss: 0.3833\n",
      "Epoch [1/1], Step [518/3504], Loss: 0.3791\n",
      "Epoch [1/1], Step [519/3504], Loss: 0.4427\n",
      "Epoch [1/1], Step [520/3504], Loss: 0.4078\n",
      "Epoch [1/1], Step [521/3504], Loss: 0.4156\n",
      "Epoch [1/1], Step [522/3504], Loss: 0.5123\n",
      "Epoch [1/1], Step [523/3504], Loss: 0.4195\n",
      "Epoch [1/1], Step [524/3504], Loss: 0.4687\n",
      "Epoch [1/1], Step [525/3504], Loss: 0.4198\n",
      "Epoch [1/1], Step [526/3504], Loss: 0.4218\n",
      "Epoch [1/1], Step [527/3504], Loss: 0.4447\n",
      "Epoch [1/1], Step [528/3504], Loss: 0.5395\n",
      "Epoch [1/1], Step [529/3504], Loss: 0.4265\n",
      "Epoch [1/1], Step [530/3504], Loss: 0.4252\n",
      "Epoch [1/1], Step [531/3504], Loss: 0.4538\n",
      "Epoch [1/1], Step [532/3504], Loss: 0.3880\n",
      "Epoch [1/1], Step [533/3504], Loss: 0.4391\n",
      "Epoch [1/1], Step [534/3504], Loss: 0.4268\n",
      "Epoch [1/1], Step [535/3504], Loss: 0.5754\n",
      "Epoch [1/1], Step [536/3504], Loss: 0.4291\n",
      "Epoch [1/1], Step [537/3504], Loss: 0.3939\n",
      "Epoch [1/1], Step [538/3504], Loss: 0.4330\n",
      "Epoch [1/1], Step [539/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [540/3504], Loss: 0.3851\n",
      "Epoch [1/1], Step [541/3504], Loss: 0.5353\n",
      "Epoch [1/1], Step [542/3504], Loss: 0.4799\n",
      "Epoch [1/1], Step [543/3504], Loss: 0.4320\n",
      "Epoch [1/1], Step [544/3504], Loss: 0.3578\n",
      "Epoch [1/1], Step [545/3504], Loss: 0.4390\n",
      "Epoch [1/1], Step [546/3504], Loss: 0.4195\n",
      "Epoch [1/1], Step [547/3504], Loss: 0.4223\n",
      "Epoch [1/1], Step [548/3504], Loss: 0.4176\n",
      "Epoch [1/1], Step [549/3504], Loss: 0.3976\n",
      "Epoch [1/1], Step [550/3504], Loss: 0.4385\n",
      "Epoch [1/1], Step [551/3504], Loss: 0.4131\n",
      "Epoch [1/1], Step [552/3504], Loss: 0.4179\n",
      "Epoch [1/1], Step [553/3504], Loss: 0.3684\n",
      "Epoch [1/1], Step [554/3504], Loss: 0.4110\n",
      "Epoch [1/1], Step [555/3504], Loss: 0.3998\n",
      "Epoch [1/1], Step [556/3504], Loss: 0.4442\n",
      "Epoch [1/1], Step [557/3504], Loss: 0.4000\n",
      "Epoch [1/1], Step [558/3504], Loss: 0.5071\n",
      "Epoch [1/1], Step [559/3504], Loss: 0.4426\n",
      "Epoch [1/1], Step [560/3504], Loss: 0.4983\n",
      "Epoch [1/1], Step [561/3504], Loss: 0.4373\n",
      "Epoch [1/1], Step [562/3504], Loss: 0.3731\n",
      "Epoch [1/1], Step [563/3504], Loss: 0.4746\n",
      "Epoch [1/1], Step [564/3504], Loss: 0.4330\n",
      "Epoch [1/1], Step [565/3504], Loss: 0.3780\n",
      "Epoch [1/1], Step [566/3504], Loss: 0.4023\n",
      "Epoch [1/1], Step [567/3504], Loss: 0.4060\n",
      "Epoch [1/1], Step [568/3504], Loss: 0.3881\n",
      "Epoch [1/1], Step [569/3504], Loss: 0.3830\n",
      "Epoch [1/1], Step [570/3504], Loss: 0.3475\n",
      "Epoch [1/1], Step [571/3504], Loss: 0.5759\n",
      "Epoch [1/1], Step [572/3504], Loss: 0.3541\n",
      "Epoch [1/1], Step [573/3504], Loss: 0.4551\n",
      "Epoch [1/1], Step [574/3504], Loss: 0.5316\n",
      "Epoch [1/1], Step [575/3504], Loss: 0.4916\n",
      "Epoch [1/1], Step [576/3504], Loss: 0.3259\n",
      "Epoch [1/1], Step [577/3504], Loss: 0.4371\n",
      "Epoch [1/1], Step [578/3504], Loss: 0.4192\n",
      "Epoch [1/1], Step [579/3504], Loss: 0.3118\n",
      "Epoch [1/1], Step [580/3504], Loss: 0.3747\n",
      "Epoch [1/1], Step [581/3504], Loss: 0.3887\n",
      "Epoch [1/1], Step [582/3504], Loss: 0.4859\n",
      "Epoch [1/1], Step [583/3504], Loss: 0.3959\n",
      "Epoch [1/1], Step [584/3504], Loss: 0.3302\n",
      "Epoch [1/1], Step [585/3504], Loss: 0.4201\n",
      "Epoch [1/1], Step [586/3504], Loss: 0.4965\n",
      "Epoch [1/1], Step [587/3504], Loss: 0.4076\n",
      "Epoch [1/1], Step [588/3504], Loss: 0.4446\n",
      "Epoch [1/1], Step [589/3504], Loss: 0.4300\n",
      "Epoch [1/1], Step [590/3504], Loss: 0.4490\n",
      "Epoch [1/1], Step [591/3504], Loss: 0.4217\n",
      "Epoch [1/1], Step [592/3504], Loss: 0.5226\n",
      "Epoch [1/1], Step [593/3504], Loss: 0.4800\n",
      "Epoch [1/1], Step [594/3504], Loss: 0.3125\n",
      "Epoch [1/1], Step [595/3504], Loss: 0.3635\n",
      "Epoch [1/1], Step [596/3504], Loss: 0.3917\n",
      "Epoch [1/1], Step [597/3504], Loss: 0.3760\n",
      "Epoch [1/1], Step [598/3504], Loss: 0.4103\n",
      "Epoch [1/1], Step [599/3504], Loss: 0.4691\n",
      "Epoch [1/1], Step [600/3504], Loss: 0.4302\n",
      "Epoch [1/1], Step [601/3504], Loss: 0.5360\n",
      "Epoch [1/1], Step [602/3504], Loss: 0.5734\n",
      "Epoch [1/1], Step [603/3504], Loss: 0.3716\n",
      "Epoch [1/1], Step [604/3504], Loss: 0.3671\n",
      "Epoch [1/1], Step [605/3504], Loss: 0.3699\n",
      "Epoch [1/1], Step [606/3504], Loss: 0.4334\n",
      "Epoch [1/1], Step [607/3504], Loss: 0.4118\n",
      "Epoch [1/1], Step [608/3504], Loss: 0.4060\n",
      "Epoch [1/1], Step [609/3504], Loss: 0.3977\n",
      "Epoch [1/1], Step [610/3504], Loss: 0.4235\n",
      "Epoch [1/1], Step [611/3504], Loss: 0.4258\n",
      "Epoch [1/1], Step [612/3504], Loss: 0.4672\n",
      "Epoch [1/1], Step [613/3504], Loss: 0.4710\n",
      "Epoch [1/1], Step [614/3504], Loss: 0.4940\n",
      "Epoch [1/1], Step [615/3504], Loss: 0.3539\n",
      "Epoch [1/1], Step [616/3504], Loss: 0.4179\n",
      "Epoch [1/1], Step [617/3504], Loss: 0.4034\n",
      "Epoch [1/1], Step [618/3504], Loss: 0.4449\n",
      "Epoch [1/1], Step [619/3504], Loss: 0.4707\n",
      "Epoch [1/1], Step [620/3504], Loss: 0.4242\n",
      "Epoch [1/1], Step [621/3504], Loss: 0.3897\n",
      "Epoch [1/1], Step [622/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [623/3504], Loss: 0.3703\n",
      "Epoch [1/1], Step [624/3504], Loss: 0.3989\n",
      "Epoch [1/1], Step [625/3504], Loss: 0.4811\n",
      "Epoch [1/1], Step [626/3504], Loss: 0.3918\n",
      "Epoch [1/1], Step [627/3504], Loss: 0.4917\n",
      "Epoch [1/1], Step [628/3504], Loss: 0.4462\n",
      "Epoch [1/1], Step [629/3504], Loss: 0.4229\n",
      "Epoch [1/1], Step [630/3504], Loss: 0.4591\n",
      "Epoch [1/1], Step [631/3504], Loss: 0.4467\n",
      "Epoch [1/1], Step [632/3504], Loss: 0.4048\n",
      "Epoch [1/1], Step [633/3504], Loss: 0.5596\n",
      "Epoch [1/1], Step [634/3504], Loss: 0.3755\n",
      "Epoch [1/1], Step [635/3504], Loss: 0.4337\n",
      "Epoch [1/1], Step [636/3504], Loss: 0.4277\n",
      "Epoch [1/1], Step [637/3504], Loss: 0.3612\n",
      "Epoch [1/1], Step [638/3504], Loss: 0.4360\n",
      "Epoch [1/1], Step [639/3504], Loss: 0.4782\n",
      "Epoch [1/1], Step [640/3504], Loss: 0.3865\n",
      "Epoch [1/1], Step [641/3504], Loss: 0.4332\n",
      "Epoch [1/1], Step [642/3504], Loss: 0.4345\n",
      "Epoch [1/1], Step [643/3504], Loss: 0.3816\n",
      "Epoch [1/1], Step [644/3504], Loss: 0.3309\n",
      "Epoch [1/1], Step [645/3504], Loss: 0.3098\n",
      "Epoch [1/1], Step [646/3504], Loss: 0.3721\n",
      "Epoch [1/1], Step [647/3504], Loss: 0.4509\n",
      "Epoch [1/1], Step [648/3504], Loss: 0.3459\n",
      "Epoch [1/1], Step [649/3504], Loss: 0.3685\n",
      "Epoch [1/1], Step [650/3504], Loss: 0.3452\n",
      "Epoch [1/1], Step [651/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [652/3504], Loss: 0.4885\n",
      "Epoch [1/1], Step [653/3504], Loss: 0.4278\n",
      "Epoch [1/1], Step [654/3504], Loss: 0.4128\n",
      "Epoch [1/1], Step [655/3504], Loss: 0.4720\n",
      "Epoch [1/1], Step [656/3504], Loss: 0.4445\n",
      "Epoch [1/1], Step [657/3504], Loss: 0.3917\n",
      "Epoch [1/1], Step [658/3504], Loss: 0.5443\n",
      "Epoch [1/1], Step [659/3504], Loss: 0.3291\n",
      "Epoch [1/1], Step [660/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [661/3504], Loss: 0.4029\n",
      "Epoch [1/1], Step [662/3504], Loss: 0.2377\n",
      "Epoch [1/1], Step [663/3504], Loss: 0.4099\n",
      "Epoch [1/1], Step [664/3504], Loss: 0.4253\n",
      "Epoch [1/1], Step [665/3504], Loss: 0.5069\n",
      "Epoch [1/1], Step [666/3504], Loss: 0.3931\n",
      "Epoch [1/1], Step [667/3504], Loss: 0.3983\n",
      "Epoch [1/1], Step [668/3504], Loss: 0.4496\n",
      "Epoch [1/1], Step [669/3504], Loss: 0.4097\n",
      "Epoch [1/1], Step [670/3504], Loss: 0.5460\n",
      "Epoch [1/1], Step [671/3504], Loss: 0.4363\n",
      "Epoch [1/1], Step [672/3504], Loss: 0.4505\n",
      "Epoch [1/1], Step [673/3504], Loss: 0.3762\n",
      "Epoch [1/1], Step [674/3504], Loss: 0.4953\n",
      "Epoch [1/1], Step [675/3504], Loss: 0.4112\n",
      "Epoch [1/1], Step [676/3504], Loss: 0.4517\n",
      "Epoch [1/1], Step [677/3504], Loss: 0.3535\n",
      "Epoch [1/1], Step [678/3504], Loss: 0.4464\n",
      "Epoch [1/1], Step [679/3504], Loss: 0.4456\n",
      "Epoch [1/1], Step [680/3504], Loss: 0.3819\n",
      "Epoch [1/1], Step [681/3504], Loss: 0.3407\n",
      "Epoch [1/1], Step [682/3504], Loss: 0.4147\n",
      "Epoch [1/1], Step [683/3504], Loss: 0.5280\n",
      "Epoch [1/1], Step [684/3504], Loss: 0.4408\n",
      "Epoch [1/1], Step [685/3504], Loss: 0.4024\n",
      "Epoch [1/1], Step [686/3504], Loss: 0.4996\n",
      "Epoch [1/1], Step [687/3504], Loss: 0.4196\n",
      "Epoch [1/1], Step [688/3504], Loss: 0.3707\n",
      "Epoch [1/1], Step [689/3504], Loss: 0.4003\n",
      "Epoch [1/1], Step [690/3504], Loss: 0.5384\n",
      "Epoch [1/1], Step [691/3504], Loss: 0.3622\n",
      "Epoch [1/1], Step [692/3504], Loss: 0.3006\n",
      "Epoch [1/1], Step [693/3504], Loss: 0.4272\n",
      "Epoch [1/1], Step [694/3504], Loss: 0.3444\n",
      "Epoch [1/1], Step [695/3504], Loss: 0.4479\n",
      "Epoch [1/1], Step [696/3504], Loss: 0.4560\n",
      "Epoch [1/1], Step [697/3504], Loss: 0.3823\n",
      "Epoch [1/1], Step [698/3504], Loss: 0.3891\n",
      "Epoch [1/1], Step [699/3504], Loss: 0.4248\n",
      "Epoch [1/1], Step [700/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [701/3504], Loss: 0.3761\n",
      "Epoch [1/1], Step [702/3504], Loss: 0.3006\n",
      "Epoch [1/1], Step [703/3504], Loss: 0.4379\n",
      "Epoch [1/1], Step [704/3504], Loss: 0.4270\n",
      "Epoch [1/1], Step [705/3504], Loss: 0.4387\n",
      "Epoch [1/1], Step [706/3504], Loss: 0.4075\n",
      "Epoch [1/1], Step [707/3504], Loss: 0.3667\n",
      "Epoch [1/1], Step [708/3504], Loss: 0.3359\n",
      "Epoch [1/1], Step [709/3504], Loss: 0.3472\n",
      "Epoch [1/1], Step [710/3504], Loss: 0.3136\n",
      "Epoch [1/1], Step [711/3504], Loss: 0.4175\n",
      "Epoch [1/1], Step [712/3504], Loss: 0.3987\n",
      "Epoch [1/1], Step [713/3504], Loss: 0.3986\n",
      "Epoch [1/1], Step [714/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [715/3504], Loss: 0.4475\n",
      "Epoch [1/1], Step [716/3504], Loss: 0.4226\n",
      "Epoch [1/1], Step [717/3504], Loss: 0.3763\n",
      "Epoch [1/1], Step [718/3504], Loss: 0.4399\n",
      "Epoch [1/1], Step [719/3504], Loss: 0.3710\n",
      "Epoch [1/1], Step [720/3504], Loss: 0.4525\n",
      "Epoch [1/1], Step [721/3504], Loss: 0.4614\n",
      "Epoch [1/1], Step [722/3504], Loss: 0.4276\n",
      "Epoch [1/1], Step [723/3504], Loss: 0.3481\n",
      "Epoch [1/1], Step [724/3504], Loss: 0.4296\n",
      "Epoch [1/1], Step [725/3504], Loss: 0.4138\n",
      "Epoch [1/1], Step [726/3504], Loss: 0.3196\n",
      "Epoch [1/1], Step [727/3504], Loss: 0.2899\n",
      "Epoch [1/1], Step [728/3504], Loss: 0.3409\n",
      "Epoch [1/1], Step [729/3504], Loss: 0.5081\n",
      "Epoch [1/1], Step [730/3504], Loss: 0.2868\n",
      "Epoch [1/1], Step [731/3504], Loss: 0.5066\n",
      "Epoch [1/1], Step [732/3504], Loss: 0.3663\n",
      "Epoch [1/1], Step [733/3504], Loss: 0.4637\n",
      "Epoch [1/1], Step [734/3504], Loss: 0.3857\n",
      "Epoch [1/1], Step [735/3504], Loss: 0.3679\n",
      "Epoch [1/1], Step [736/3504], Loss: 0.4091\n",
      "Epoch [1/1], Step [737/3504], Loss: 0.5669\n",
      "Epoch [1/1], Step [738/3504], Loss: 0.4384\n",
      "Epoch [1/1], Step [739/3504], Loss: 0.3016\n",
      "Epoch [1/1], Step [740/3504], Loss: 0.3466\n",
      "Epoch [1/1], Step [741/3504], Loss: 0.4882\n",
      "Epoch [1/1], Step [742/3504], Loss: 0.3636\n",
      "Epoch [1/1], Step [743/3504], Loss: 0.5169\n",
      "Epoch [1/1], Step [744/3504], Loss: 0.3593\n",
      "Epoch [1/1], Step [745/3504], Loss: 0.4632\n",
      "Epoch [1/1], Step [746/3504], Loss: 0.3560\n",
      "Epoch [1/1], Step [747/3504], Loss: 0.3530\n",
      "Epoch [1/1], Step [748/3504], Loss: 0.4537\n",
      "Epoch [1/1], Step [749/3504], Loss: 0.3994\n",
      "Epoch [1/1], Step [750/3504], Loss: 0.3782\n",
      "Epoch [1/1], Step [751/3504], Loss: 0.4017\n",
      "Epoch [1/1], Step [752/3504], Loss: 0.4161\n",
      "Epoch [1/1], Step [753/3504], Loss: 0.3173\n",
      "Epoch [1/1], Step [754/3504], Loss: 0.4348\n",
      "Epoch [1/1], Step [755/3504], Loss: 0.3925\n",
      "Epoch [1/1], Step [756/3504], Loss: 0.4398\n",
      "Epoch [1/1], Step [757/3504], Loss: 0.3104\n",
      "Epoch [1/1], Step [758/3504], Loss: 0.4159\n",
      "Epoch [1/1], Step [759/3504], Loss: 0.4690\n",
      "Epoch [1/1], Step [760/3504], Loss: 0.4492\n",
      "Epoch [1/1], Step [761/3504], Loss: 0.4960\n",
      "Epoch [1/1], Step [762/3504], Loss: 0.4405\n",
      "Epoch [1/1], Step [763/3504], Loss: 0.3929\n",
      "Epoch [1/1], Step [764/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [765/3504], Loss: 0.3603\n",
      "Epoch [1/1], Step [766/3504], Loss: 0.4162\n",
      "Epoch [1/1], Step [767/3504], Loss: 0.3887\n",
      "Epoch [1/1], Step [768/3504], Loss: 0.3136\n",
      "Epoch [1/1], Step [769/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [770/3504], Loss: 0.3273\n",
      "Epoch [1/1], Step [771/3504], Loss: 0.4297\n",
      "Epoch [1/1], Step [772/3504], Loss: 0.3424\n",
      "Epoch [1/1], Step [773/3504], Loss: 0.4328\n",
      "Epoch [1/1], Step [774/3504], Loss: 0.3452\n",
      "Epoch [1/1], Step [775/3504], Loss: 0.3052\n",
      "Epoch [1/1], Step [776/3504], Loss: 0.3622\n",
      "Epoch [1/1], Step [777/3504], Loss: 0.3772\n",
      "Epoch [1/1], Step [778/3504], Loss: 0.3201\n",
      "Epoch [1/1], Step [779/3504], Loss: 0.3167\n",
      "Epoch [1/1], Step [780/3504], Loss: 0.2772\n",
      "Epoch [1/1], Step [781/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [782/3504], Loss: 0.4763\n",
      "Epoch [1/1], Step [783/3504], Loss: 0.4294\n",
      "Epoch [1/1], Step [784/3504], Loss: 0.4694\n",
      "Epoch [1/1], Step [785/3504], Loss: 0.3521\n",
      "Epoch [1/1], Step [786/3504], Loss: 0.3609\n",
      "Epoch [1/1], Step [787/3504], Loss: 0.3706\n",
      "Epoch [1/1], Step [788/3504], Loss: 0.3704\n",
      "Epoch [1/1], Step [789/3504], Loss: 0.3226\n",
      "Epoch [1/1], Step [790/3504], Loss: 0.3818\n",
      "Epoch [1/1], Step [791/3504], Loss: 0.5435\n",
      "Epoch [1/1], Step [792/3504], Loss: 0.3526\n",
      "Epoch [1/1], Step [793/3504], Loss: 0.3100\n",
      "Epoch [1/1], Step [794/3504], Loss: 0.3962\n",
      "Epoch [1/1], Step [795/3504], Loss: 0.4433\n",
      "Epoch [1/1], Step [796/3504], Loss: 0.3823\n",
      "Epoch [1/1], Step [797/3504], Loss: 0.3932\n",
      "Epoch [1/1], Step [798/3504], Loss: 0.4029\n",
      "Epoch [1/1], Step [799/3504], Loss: 0.3848\n",
      "Epoch [1/1], Step [800/3504], Loss: 0.3412\n",
      "Epoch [1/1], Step [801/3504], Loss: 0.4195\n",
      "Epoch [1/1], Step [802/3504], Loss: 0.4109\n",
      "Epoch [1/1], Step [803/3504], Loss: 0.3452\n",
      "Epoch [1/1], Step [804/3504], Loss: 0.3820\n",
      "Epoch [1/1], Step [805/3504], Loss: 0.4538\n",
      "Epoch [1/1], Step [806/3504], Loss: 0.4145\n",
      "Epoch [1/1], Step [807/3504], Loss: 0.3960\n",
      "Epoch [1/1], Step [808/3504], Loss: 0.3535\n",
      "Epoch [1/1], Step [809/3504], Loss: 0.4327\n",
      "Epoch [1/1], Step [810/3504], Loss: 0.3433\n",
      "Epoch [1/1], Step [811/3504], Loss: 0.3999\n",
      "Epoch [1/1], Step [812/3504], Loss: 0.4579\n",
      "Epoch [1/1], Step [813/3504], Loss: 0.4792\n",
      "Epoch [1/1], Step [814/3504], Loss: 0.3456\n",
      "Epoch [1/1], Step [815/3504], Loss: 0.2949\n",
      "Epoch [1/1], Step [816/3504], Loss: 0.5279\n",
      "Epoch [1/1], Step [817/3504], Loss: 0.3895\n",
      "Epoch [1/1], Step [818/3504], Loss: 0.4115\n",
      "Epoch [1/1], Step [819/3504], Loss: 0.4043\n",
      "Epoch [1/1], Step [820/3504], Loss: 0.4698\n",
      "Epoch [1/1], Step [821/3504], Loss: 0.4139\n",
      "Epoch [1/1], Step [822/3504], Loss: 0.3208\n",
      "Epoch [1/1], Step [823/3504], Loss: 0.4108\n",
      "Epoch [1/1], Step [824/3504], Loss: 0.4124\n",
      "Epoch [1/1], Step [825/3504], Loss: 0.3848\n",
      "Epoch [1/1], Step [826/3504], Loss: 0.4250\n",
      "Epoch [1/1], Step [827/3504], Loss: 0.3624\n",
      "Epoch [1/1], Step [828/3504], Loss: 0.4238\n",
      "Epoch [1/1], Step [829/3504], Loss: 0.4852\n",
      "Epoch [1/1], Step [830/3504], Loss: 0.4051\n",
      "Epoch [1/1], Step [831/3504], Loss: 0.3860\n",
      "Epoch [1/1], Step [832/3504], Loss: 0.4003\n",
      "Epoch [1/1], Step [833/3504], Loss: 0.3780\n",
      "Epoch [1/1], Step [834/3504], Loss: 0.4891\n",
      "Epoch [1/1], Step [835/3504], Loss: 0.3886\n",
      "Epoch [1/1], Step [836/3504], Loss: 0.4377\n",
      "Epoch [1/1], Step [837/3504], Loss: 0.3411\n",
      "Epoch [1/1], Step [838/3504], Loss: 0.4205\n",
      "Epoch [1/1], Step [839/3504], Loss: 0.3416\n",
      "Epoch [1/1], Step [840/3504], Loss: 0.4630\n",
      "Epoch [1/1], Step [841/3504], Loss: 0.4079\n",
      "Epoch [1/1], Step [842/3504], Loss: 0.3536\n",
      "Epoch [1/1], Step [843/3504], Loss: 0.4280\n",
      "Epoch [1/1], Step [844/3504], Loss: 0.4305\n",
      "Epoch [1/1], Step [845/3504], Loss: 0.4313\n",
      "Epoch [1/1], Step [846/3504], Loss: 0.3271\n",
      "Epoch [1/1], Step [847/3504], Loss: 0.3149\n",
      "Epoch [1/1], Step [848/3504], Loss: 0.4540\n",
      "Epoch [1/1], Step [849/3504], Loss: 0.3711\n",
      "Epoch [1/1], Step [850/3504], Loss: 0.4859\n",
      "Epoch [1/1], Step [851/3504], Loss: 0.3450\n",
      "Epoch [1/1], Step [852/3504], Loss: 0.4100\n",
      "Epoch [1/1], Step [853/3504], Loss: 0.4731\n",
      "Epoch [1/1], Step [854/3504], Loss: 0.3784\n",
      "Epoch [1/1], Step [855/3504], Loss: 0.3015\n",
      "Epoch [1/1], Step [856/3504], Loss: 0.3250\n",
      "Epoch [1/1], Step [857/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [858/3504], Loss: 0.3616\n",
      "Epoch [1/1], Step [859/3504], Loss: 0.4115\n",
      "Epoch [1/1], Step [860/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [861/3504], Loss: 0.3228\n",
      "Epoch [1/1], Step [862/3504], Loss: 0.4282\n",
      "Epoch [1/1], Step [863/3504], Loss: 0.4206\n",
      "Epoch [1/1], Step [864/3504], Loss: 0.3360\n",
      "Epoch [1/1], Step [865/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [866/3504], Loss: 0.3966\n",
      "Epoch [1/1], Step [867/3504], Loss: 0.3352\n",
      "Epoch [1/1], Step [868/3504], Loss: 0.4602\n",
      "Epoch [1/1], Step [869/3504], Loss: 0.3780\n",
      "Epoch [1/1], Step [870/3504], Loss: 0.4922\n",
      "Epoch [1/1], Step [871/3504], Loss: 0.4389\n",
      "Epoch [1/1], Step [872/3504], Loss: 0.4264\n",
      "Epoch [1/1], Step [873/3504], Loss: 0.4207\n",
      "Epoch [1/1], Step [874/3504], Loss: 0.3844\n",
      "Epoch [1/1], Step [875/3504], Loss: 0.3446\n",
      "Epoch [1/1], Step [876/3504], Loss: 0.4338\n",
      "Epoch [1/1], Step [877/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [878/3504], Loss: 0.4186\n",
      "Epoch [1/1], Step [879/3504], Loss: 0.3888\n",
      "Epoch [1/1], Step [880/3504], Loss: 0.3373\n",
      "Epoch [1/1], Step [881/3504], Loss: 0.4446\n",
      "Epoch [1/1], Step [882/3504], Loss: 0.4138\n",
      "Epoch [1/1], Step [883/3504], Loss: 0.2716\n",
      "Epoch [1/1], Step [884/3504], Loss: 0.4904\n",
      "Epoch [1/1], Step [885/3504], Loss: 0.3985\n",
      "Epoch [1/1], Step [886/3504], Loss: 0.4808\n",
      "Epoch [1/1], Step [887/3504], Loss: 0.4295\n",
      "Epoch [1/1], Step [888/3504], Loss: 0.3837\n",
      "Epoch [1/1], Step [889/3504], Loss: 0.3521\n",
      "Epoch [1/1], Step [890/3504], Loss: 0.2979\n",
      "Epoch [1/1], Step [891/3504], Loss: 0.4438\n",
      "Epoch [1/1], Step [892/3504], Loss: 0.4120\n",
      "Epoch [1/1], Step [893/3504], Loss: 0.3918\n",
      "Epoch [1/1], Step [894/3504], Loss: 0.3329\n",
      "Epoch [1/1], Step [895/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [896/3504], Loss: 0.4541\n",
      "Epoch [1/1], Step [897/3504], Loss: 0.4443\n",
      "Epoch [1/1], Step [898/3504], Loss: 0.4783\n",
      "Epoch [1/1], Step [899/3504], Loss: 0.4903\n",
      "Epoch [1/1], Step [900/3504], Loss: 0.3863\n",
      "Epoch [1/1], Step [901/3504], Loss: 0.3878\n",
      "Epoch [1/1], Step [902/3504], Loss: 0.4209\n",
      "Epoch [1/1], Step [903/3504], Loss: 0.3903\n",
      "Epoch [1/1], Step [904/3504], Loss: 0.3178\n",
      "Epoch [1/1], Step [905/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [906/3504], Loss: 0.5650\n",
      "Epoch [1/1], Step [907/3504], Loss: 0.5411\n",
      "Epoch [1/1], Step [908/3504], Loss: 0.4178\n",
      "Epoch [1/1], Step [909/3504], Loss: 0.5286\n",
      "Epoch [1/1], Step [910/3504], Loss: 0.4685\n",
      "Epoch [1/1], Step [911/3504], Loss: 0.4193\n",
      "Epoch [1/1], Step [912/3504], Loss: 0.3951\n",
      "Epoch [1/1], Step [913/3504], Loss: 0.4115\n",
      "Epoch [1/1], Step [914/3504], Loss: 0.5293\n",
      "Epoch [1/1], Step [915/3504], Loss: 0.3977\n",
      "Epoch [1/1], Step [916/3504], Loss: 0.3645\n",
      "Epoch [1/1], Step [917/3504], Loss: 0.4777\n",
      "Epoch [1/1], Step [918/3504], Loss: 0.3622\n",
      "Epoch [1/1], Step [919/3504], Loss: 0.4230\n",
      "Epoch [1/1], Step [920/3504], Loss: 0.4637\n",
      "Epoch [1/1], Step [921/3504], Loss: 0.4120\n",
      "Epoch [1/1], Step [922/3504], Loss: 0.3161\n",
      "Epoch [1/1], Step [923/3504], Loss: 0.3792\n",
      "Epoch [1/1], Step [924/3504], Loss: 0.5080\n",
      "Epoch [1/1], Step [925/3504], Loss: 0.3669\n",
      "Epoch [1/1], Step [926/3504], Loss: 0.5298\n",
      "Epoch [1/1], Step [927/3504], Loss: 0.4101\n",
      "Epoch [1/1], Step [928/3504], Loss: 0.4252\n",
      "Epoch [1/1], Step [929/3504], Loss: 0.3802\n",
      "Epoch [1/1], Step [930/3504], Loss: 0.4126\n",
      "Epoch [1/1], Step [931/3504], Loss: 0.4262\n",
      "Epoch [1/1], Step [932/3504], Loss: 0.4265\n",
      "Epoch [1/1], Step [933/3504], Loss: 0.4623\n",
      "Epoch [1/1], Step [934/3504], Loss: 0.3898\n",
      "Epoch [1/1], Step [935/3504], Loss: 0.4357\n",
      "Epoch [1/1], Step [936/3504], Loss: 0.3721\n",
      "Epoch [1/1], Step [937/3504], Loss: 0.3292\n",
      "Epoch [1/1], Step [938/3504], Loss: 0.4628\n",
      "Epoch [1/1], Step [939/3504], Loss: 0.4134\n",
      "Epoch [1/1], Step [940/3504], Loss: 0.4019\n",
      "Epoch [1/1], Step [941/3504], Loss: 0.4196\n",
      "Epoch [1/1], Step [942/3504], Loss: 0.3879\n",
      "Epoch [1/1], Step [943/3504], Loss: 0.3746\n",
      "Epoch [1/1], Step [944/3504], Loss: 0.3890\n",
      "Epoch [1/1], Step [945/3504], Loss: 0.4189\n",
      "Epoch [1/1], Step [946/3504], Loss: 0.3064\n",
      "Epoch [1/1], Step [947/3504], Loss: 0.4164\n",
      "Epoch [1/1], Step [948/3504], Loss: 0.4067\n",
      "Epoch [1/1], Step [949/3504], Loss: 0.3836\n",
      "Epoch [1/1], Step [950/3504], Loss: 0.4508\n",
      "Epoch [1/1], Step [951/3504], Loss: 0.4070\n",
      "Epoch [1/1], Step [952/3504], Loss: 0.4341\n",
      "Epoch [1/1], Step [953/3504], Loss: 0.3814\n",
      "Epoch [1/1], Step [954/3504], Loss: 0.4794\n",
      "Epoch [1/1], Step [955/3504], Loss: 0.4010\n",
      "Epoch [1/1], Step [956/3504], Loss: 0.3853\n",
      "Epoch [1/1], Step [957/3504], Loss: 0.4024\n",
      "Epoch [1/1], Step [958/3504], Loss: 0.2972\n",
      "Epoch [1/1], Step [959/3504], Loss: 0.3508\n",
      "Epoch [1/1], Step [960/3504], Loss: 0.2632\n",
      "Epoch [1/1], Step [961/3504], Loss: 0.4457\n",
      "Epoch [1/1], Step [962/3504], Loss: 0.3917\n",
      "Epoch [1/1], Step [963/3504], Loss: 0.4263\n",
      "Epoch [1/1], Step [964/3504], Loss: 0.4058\n",
      "Epoch [1/1], Step [965/3504], Loss: 0.3735\n",
      "Epoch [1/1], Step [966/3504], Loss: 0.4429\n",
      "Epoch [1/1], Step [967/3504], Loss: 0.5437\n",
      "Epoch [1/1], Step [968/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [969/3504], Loss: 0.4208\n",
      "Epoch [1/1], Step [970/3504], Loss: 0.3916\n",
      "Epoch [1/1], Step [971/3504], Loss: 0.3723\n",
      "Epoch [1/1], Step [972/3504], Loss: 0.3936\n",
      "Epoch [1/1], Step [973/3504], Loss: 0.3257\n",
      "Epoch [1/1], Step [974/3504], Loss: 0.3735\n",
      "Epoch [1/1], Step [975/3504], Loss: 0.4006\n",
      "Epoch [1/1], Step [976/3504], Loss: 0.4346\n",
      "Epoch [1/1], Step [977/3504], Loss: 0.3285\n",
      "Epoch [1/1], Step [978/3504], Loss: 0.5032\n",
      "Epoch [1/1], Step [979/3504], Loss: 0.4083\n",
      "Epoch [1/1], Step [980/3504], Loss: 0.3851\n",
      "Epoch [1/1], Step [981/3504], Loss: 0.4408\n",
      "Epoch [1/1], Step [982/3504], Loss: 0.3481\n",
      "Epoch [1/1], Step [983/3504], Loss: 0.4206\n",
      "Epoch [1/1], Step [984/3504], Loss: 0.3621\n",
      "Epoch [1/1], Step [985/3504], Loss: 0.3974\n",
      "Epoch [1/1], Step [986/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [987/3504], Loss: 0.4430\n",
      "Epoch [1/1], Step [988/3504], Loss: 0.4377\n",
      "Epoch [1/1], Step [989/3504], Loss: 0.3275\n",
      "Epoch [1/1], Step [990/3504], Loss: 0.3661\n",
      "Epoch [1/1], Step [991/3504], Loss: 0.4407\n",
      "Epoch [1/1], Step [992/3504], Loss: 0.3683\n",
      "Epoch [1/1], Step [993/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [994/3504], Loss: 0.4180\n",
      "Epoch [1/1], Step [995/3504], Loss: 0.3732\n",
      "Epoch [1/1], Step [996/3504], Loss: 0.3767\n",
      "Epoch [1/1], Step [997/3504], Loss: 0.3293\n",
      "Epoch [1/1], Step [998/3504], Loss: 0.4347\n",
      "Epoch [1/1], Step [999/3504], Loss: 0.3137\n",
      "Epoch [1/1], Step [1000/3504], Loss: 0.3801\n",
      "Epoch [1/1], Step [1001/3504], Loss: 0.4717\n",
      "Epoch [1/1], Step [1002/3504], Loss: 0.3572\n",
      "Epoch [1/1], Step [1003/3504], Loss: 0.3695\n",
      "Epoch [1/1], Step [1004/3504], Loss: 0.2957\n",
      "Epoch [1/1], Step [1005/3504], Loss: 0.4204\n",
      "Epoch [1/1], Step [1006/3504], Loss: 0.4571\n",
      "Epoch [1/1], Step [1007/3504], Loss: 0.3183\n",
      "Epoch [1/1], Step [1008/3504], Loss: 0.4034\n",
      "Epoch [1/1], Step [1009/3504], Loss: 0.3324\n",
      "Epoch [1/1], Step [1010/3504], Loss: 0.4271\n",
      "Epoch [1/1], Step [1011/3504], Loss: 0.4580\n",
      "Epoch [1/1], Step [1012/3504], Loss: 0.3429\n",
      "Epoch [1/1], Step [1013/3504], Loss: 0.4316\n",
      "Epoch [1/1], Step [1014/3504], Loss: 0.4609\n",
      "Epoch [1/1], Step [1015/3504], Loss: 0.4496\n",
      "Epoch [1/1], Step [1016/3504], Loss: 0.4093\n",
      "Epoch [1/1], Step [1017/3504], Loss: 0.4071\n",
      "Epoch [1/1], Step [1018/3504], Loss: 0.5835\n",
      "Epoch [1/1], Step [1019/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [1020/3504], Loss: 0.3435\n",
      "Epoch [1/1], Step [1021/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [1022/3504], Loss: 0.3239\n",
      "Epoch [1/1], Step [1023/3504], Loss: 0.3510\n",
      "Epoch [1/1], Step [1024/3504], Loss: 0.3909\n",
      "Epoch [1/1], Step [1025/3504], Loss: 0.3717\n",
      "Epoch [1/1], Step [1026/3504], Loss: 0.3819\n",
      "Epoch [1/1], Step [1027/3504], Loss: 0.4343\n",
      "Epoch [1/1], Step [1028/3504], Loss: 0.3697\n",
      "Epoch [1/1], Step [1029/3504], Loss: 0.3597\n",
      "Epoch [1/1], Step [1030/3504], Loss: 0.3703\n",
      "Epoch [1/1], Step [1031/3504], Loss: 0.4107\n",
      "Epoch [1/1], Step [1032/3504], Loss: 0.4317\n",
      "Epoch [1/1], Step [1033/3504], Loss: 0.4222\n",
      "Epoch [1/1], Step [1034/3504], Loss: 0.4354\n",
      "Epoch [1/1], Step [1035/3504], Loss: 0.4077\n",
      "Epoch [1/1], Step [1036/3504], Loss: 0.4400\n",
      "Epoch [1/1], Step [1037/3504], Loss: 0.4280\n",
      "Epoch [1/1], Step [1038/3504], Loss: 0.4474\n",
      "Epoch [1/1], Step [1039/3504], Loss: 1.1599\n",
      "Epoch [1/1], Step [1040/3504], Loss: 0.3483\n",
      "Epoch [1/1], Step [1041/3504], Loss: 0.4619\n",
      "Epoch [1/1], Step [1042/3504], Loss: 0.5400\n",
      "Epoch [1/1], Step [1043/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [1044/3504], Loss: 0.4197\n",
      "Epoch [1/1], Step [1045/3504], Loss: 0.4165\n",
      "Epoch [1/1], Step [1046/3504], Loss: 0.4898\n",
      "Epoch [1/1], Step [1047/3504], Loss: 0.3923\n",
      "Epoch [1/1], Step [1048/3504], Loss: 0.4431\n",
      "Epoch [1/1], Step [1049/3504], Loss: 0.3435\n",
      "Epoch [1/1], Step [1050/3504], Loss: 0.3410\n",
      "Epoch [1/1], Step [1051/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [1052/3504], Loss: 0.3263\n",
      "Epoch [1/1], Step [1053/3504], Loss: 0.4055\n",
      "Epoch [1/1], Step [1054/3504], Loss: 0.3539\n",
      "Epoch [1/1], Step [1055/3504], Loss: 0.4500\n",
      "Epoch [1/1], Step [1056/3504], Loss: 0.2355\n",
      "Epoch [1/1], Step [1057/3504], Loss: 0.2669\n",
      "Epoch [1/1], Step [1058/3504], Loss: 0.4241\n",
      "Epoch [1/1], Step [1059/3504], Loss: 0.4422\n",
      "Epoch [1/1], Step [1060/3504], Loss: 0.4802\n",
      "Epoch [1/1], Step [1061/3504], Loss: 0.2935\n",
      "Epoch [1/1], Step [1062/3504], Loss: 0.4147\n",
      "Epoch [1/1], Step [1063/3504], Loss: 0.3699\n",
      "Epoch [1/1], Step [1064/3504], Loss: 0.3318\n",
      "Epoch [1/1], Step [1065/3504], Loss: 0.4505\n",
      "Epoch [1/1], Step [1066/3504], Loss: 0.5271\n",
      "Epoch [1/1], Step [1067/3504], Loss: 0.2646\n",
      "Epoch [1/1], Step [1068/3504], Loss: 0.3817\n",
      "Epoch [1/1], Step [1069/3504], Loss: 0.4674\n",
      "Epoch [1/1], Step [1070/3504], Loss: 0.3901\n",
      "Epoch [1/1], Step [1071/3504], Loss: 0.3472\n",
      "Epoch [1/1], Step [1072/3504], Loss: 0.4423\n",
      "Epoch [1/1], Step [1073/3504], Loss: 0.5646\n",
      "Epoch [1/1], Step [1074/3504], Loss: 0.4081\n",
      "Epoch [1/1], Step [1075/3504], Loss: 0.4238\n",
      "Epoch [1/1], Step [1076/3504], Loss: 0.4508\n",
      "Epoch [1/1], Step [1077/3504], Loss: 0.3868\n",
      "Epoch [1/1], Step [1078/3504], Loss: 0.3656\n",
      "Epoch [1/1], Step [1079/3504], Loss: 0.3908\n",
      "Epoch [1/1], Step [1080/3504], Loss: 0.3273\n",
      "Epoch [1/1], Step [1081/3504], Loss: 0.3576\n",
      "Epoch [1/1], Step [1082/3504], Loss: 0.3428\n",
      "Epoch [1/1], Step [1083/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [1084/3504], Loss: 0.4114\n",
      "Epoch [1/1], Step [1085/3504], Loss: 0.3064\n",
      "Epoch [1/1], Step [1086/3504], Loss: 0.4351\n",
      "Epoch [1/1], Step [1087/3504], Loss: 0.3372\n",
      "Epoch [1/1], Step [1088/3504], Loss: 0.3978\n",
      "Epoch [1/1], Step [1089/3504], Loss: 0.4188\n",
      "Epoch [1/1], Step [1090/3504], Loss: 0.4099\n",
      "Epoch [1/1], Step [1091/3504], Loss: 0.4071\n",
      "Epoch [1/1], Step [1092/3504], Loss: 0.3613\n",
      "Epoch [1/1], Step [1093/3504], Loss: 0.3578\n",
      "Epoch [1/1], Step [1094/3504], Loss: 0.3388\n",
      "Epoch [1/1], Step [1095/3504], Loss: 0.4150\n",
      "Epoch [1/1], Step [1096/3504], Loss: 0.3291\n",
      "Epoch [1/1], Step [1097/3504], Loss: 0.4020\n",
      "Epoch [1/1], Step [1098/3504], Loss: 0.4391\n",
      "Epoch [1/1], Step [1099/3504], Loss: 0.3735\n",
      "Epoch [1/1], Step [1100/3504], Loss: 0.3725\n",
      "Epoch [1/1], Step [1101/3504], Loss: 0.4532\n",
      "Epoch [1/1], Step [1102/3504], Loss: 0.3397\n",
      "Epoch [1/1], Step [1103/3504], Loss: 0.4207\n",
      "Epoch [1/1], Step [1104/3504], Loss: 0.4433\n",
      "Epoch [1/1], Step [1105/3504], Loss: 0.3487\n",
      "Epoch [1/1], Step [1106/3504], Loss: 0.3996\n",
      "Epoch [1/1], Step [1107/3504], Loss: 0.3954\n",
      "Epoch [1/1], Step [1108/3504], Loss: 0.4132\n",
      "Epoch [1/1], Step [1109/3504], Loss: 0.3915\n",
      "Epoch [1/1], Step [1110/3504], Loss: 0.3459\n",
      "Epoch [1/1], Step [1111/3504], Loss: 0.3664\n",
      "Epoch [1/1], Step [1112/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [1113/3504], Loss: 0.2943\n",
      "Epoch [1/1], Step [1114/3504], Loss: 0.4327\n",
      "Epoch [1/1], Step [1115/3504], Loss: 0.3195\n",
      "Epoch [1/1], Step [1116/3504], Loss: 0.3346\n",
      "Epoch [1/1], Step [1117/3504], Loss: 0.3890\n",
      "Epoch [1/1], Step [1118/3504], Loss: 0.2862\n",
      "Epoch [1/1], Step [1119/3504], Loss: 0.3628\n",
      "Epoch [1/1], Step [1120/3504], Loss: 0.3250\n",
      "Epoch [1/1], Step [1121/3504], Loss: 0.4860\n",
      "Epoch [1/1], Step [1122/3504], Loss: 0.4379\n",
      "Epoch [1/1], Step [1123/3504], Loss: 0.3745\n",
      "Epoch [1/1], Step [1124/3504], Loss: 0.3503\n",
      "Epoch [1/1], Step [1125/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [1126/3504], Loss: 0.3643\n",
      "Epoch [1/1], Step [1127/3504], Loss: 0.4328\n",
      "Epoch [1/1], Step [1128/3504], Loss: 0.3666\n",
      "Epoch [1/1], Step [1129/3504], Loss: 0.4802\n",
      "Epoch [1/1], Step [1130/3504], Loss: 0.4212\n",
      "Epoch [1/1], Step [1131/3504], Loss: 0.4109\n",
      "Epoch [1/1], Step [1132/3504], Loss: 0.3282\n",
      "Epoch [1/1], Step [1133/3504], Loss: 0.3698\n",
      "Epoch [1/1], Step [1134/3504], Loss: 0.3499\n",
      "Epoch [1/1], Step [1135/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [1136/3504], Loss: 0.3336\n",
      "Epoch [1/1], Step [1137/3504], Loss: 0.5382\n",
      "Epoch [1/1], Step [1138/3504], Loss: 0.4068\n",
      "Epoch [1/1], Step [1139/3504], Loss: 0.4970\n",
      "Epoch [1/1], Step [1140/3504], Loss: 0.5421\n",
      "Epoch [1/1], Step [1141/3504], Loss: 0.3352\n",
      "Epoch [1/1], Step [1142/3504], Loss: 0.4423\n",
      "Epoch [1/1], Step [1143/3504], Loss: 0.4133\n",
      "Epoch [1/1], Step [1144/3504], Loss: 0.5045\n",
      "Epoch [1/1], Step [1145/3504], Loss: 0.4107\n",
      "Epoch [1/1], Step [1146/3504], Loss: 0.3776\n",
      "Epoch [1/1], Step [1147/3504], Loss: 0.3188\n",
      "Epoch [1/1], Step [1148/3504], Loss: 0.3180\n",
      "Epoch [1/1], Step [1149/3504], Loss: 0.3749\n",
      "Epoch [1/1], Step [1150/3504], Loss: 0.4768\n",
      "Epoch [1/1], Step [1151/3504], Loss: 0.4566\n",
      "Epoch [1/1], Step [1152/3504], Loss: 0.3610\n",
      "Epoch [1/1], Step [1153/3504], Loss: 0.3877\n",
      "Epoch [1/1], Step [1154/3504], Loss: 0.3149\n",
      "Epoch [1/1], Step [1155/3504], Loss: 0.3528\n",
      "Epoch [1/1], Step [1156/3504], Loss: 0.3132\n",
      "Epoch [1/1], Step [1157/3504], Loss: 0.3700\n",
      "Epoch [1/1], Step [1158/3504], Loss: 0.3012\n",
      "Epoch [1/1], Step [1159/3504], Loss: 0.3605\n",
      "Epoch [1/1], Step [1160/3504], Loss: 0.4002\n",
      "Epoch [1/1], Step [1161/3504], Loss: 0.3586\n",
      "Epoch [1/1], Step [1162/3504], Loss: 0.2629\n",
      "Epoch [1/1], Step [1163/3504], Loss: 0.3973\n",
      "Epoch [1/1], Step [1164/3504], Loss: 0.3165\n",
      "Epoch [1/1], Step [1165/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [1166/3504], Loss: 0.2922\n",
      "Epoch [1/1], Step [1167/3504], Loss: 0.5066\n",
      "Epoch [1/1], Step [1168/3504], Loss: 0.3783\n",
      "Epoch [1/1], Step [1169/3504], Loss: 0.4803\n",
      "Epoch [1/1], Step [1170/3504], Loss: 0.4765\n",
      "Epoch [1/1], Step [1171/3504], Loss: 0.4231\n",
      "Epoch [1/1], Step [1172/3504], Loss: 0.4331\n",
      "Epoch [1/1], Step [1173/3504], Loss: 0.4234\n",
      "Epoch [1/1], Step [1174/3504], Loss: 0.4694\n",
      "Epoch [1/1], Step [1175/3504], Loss: 0.4277\n",
      "Epoch [1/1], Step [1176/3504], Loss: 0.4061\n",
      "Epoch [1/1], Step [1177/3504], Loss: 0.4350\n",
      "Epoch [1/1], Step [1178/3504], Loss: 0.4176\n",
      "Epoch [1/1], Step [1179/3504], Loss: 0.3432\n",
      "Epoch [1/1], Step [1180/3504], Loss: 0.4020\n",
      "Epoch [1/1], Step [1181/3504], Loss: 0.3386\n",
      "Epoch [1/1], Step [1182/3504], Loss: 0.3957\n",
      "Epoch [1/1], Step [1183/3504], Loss: 0.3623\n",
      "Epoch [1/1], Step [1184/3504], Loss: 0.4117\n",
      "Epoch [1/1], Step [1185/3504], Loss: 0.4676\n",
      "Epoch [1/1], Step [1186/3504], Loss: 0.3379\n",
      "Epoch [1/1], Step [1187/3504], Loss: 0.4188\n",
      "Epoch [1/1], Step [1188/3504], Loss: 0.4170\n",
      "Epoch [1/1], Step [1189/3504], Loss: 0.4108\n",
      "Epoch [1/1], Step [1190/3504], Loss: 0.3140\n",
      "Epoch [1/1], Step [1191/3504], Loss: 0.4531\n",
      "Epoch [1/1], Step [1192/3504], Loss: 0.4301\n",
      "Epoch [1/1], Step [1193/3504], Loss: 0.4088\n",
      "Epoch [1/1], Step [1194/3504], Loss: 0.3584\n",
      "Epoch [1/1], Step [1195/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [1196/3504], Loss: 0.3867\n",
      "Epoch [1/1], Step [1197/3504], Loss: 0.3977\n",
      "Epoch [1/1], Step [1198/3504], Loss: 0.3885\n",
      "Epoch [1/1], Step [1199/3504], Loss: 0.3500\n",
      "Epoch [1/1], Step [1200/3504], Loss: 0.3952\n",
      "Epoch [1/1], Step [1201/3504], Loss: 0.3666\n",
      "Epoch [1/1], Step [1202/3504], Loss: 0.3070\n",
      "Epoch [1/1], Step [1203/3504], Loss: 0.3752\n",
      "Epoch [1/1], Step [1204/3504], Loss: 0.4246\n",
      "Epoch [1/1], Step [1205/3504], Loss: 0.3192\n",
      "Epoch [1/1], Step [1206/3504], Loss: 0.4046\n",
      "Epoch [1/1], Step [1207/3504], Loss: 0.3906\n",
      "Epoch [1/1], Step [1208/3504], Loss: 0.3369\n",
      "Epoch [1/1], Step [1209/3504], Loss: 0.3124\n",
      "Epoch [1/1], Step [1210/3504], Loss: 0.3529\n",
      "Epoch [1/1], Step [1211/3504], Loss: 0.4568\n",
      "Epoch [1/1], Step [1212/3504], Loss: 0.4866\n",
      "Epoch [1/1], Step [1213/3504], Loss: 0.3981\n",
      "Epoch [1/1], Step [1214/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [1215/3504], Loss: 0.4641\n",
      "Epoch [1/1], Step [1216/3504], Loss: 0.3993\n",
      "Epoch [1/1], Step [1217/3504], Loss: 0.3927\n",
      "Epoch [1/1], Step [1218/3504], Loss: 0.4659\n",
      "Epoch [1/1], Step [1219/3504], Loss: 0.3535\n",
      "Epoch [1/1], Step [1220/3504], Loss: 0.3594\n",
      "Epoch [1/1], Step [1221/3504], Loss: 0.3538\n",
      "Epoch [1/1], Step [1222/3504], Loss: 0.4182\n",
      "Epoch [1/1], Step [1223/3504], Loss: 0.4159\n",
      "Epoch [1/1], Step [1224/3504], Loss: 0.3532\n",
      "Epoch [1/1], Step [1225/3504], Loss: 0.3699\n",
      "Epoch [1/1], Step [1226/3504], Loss: 0.4714\n",
      "Epoch [1/1], Step [1227/3504], Loss: 0.3317\n",
      "Epoch [1/1], Step [1228/3504], Loss: 0.3817\n",
      "Epoch [1/1], Step [1229/3504], Loss: 0.3749\n",
      "Epoch [1/1], Step [1230/3504], Loss: 0.4375\n",
      "Epoch [1/1], Step [1231/3504], Loss: 0.3850\n",
      "Epoch [1/1], Step [1232/3504], Loss: 0.4389\n",
      "Epoch [1/1], Step [1233/3504], Loss: 0.4587\n",
      "Epoch [1/1], Step [1234/3504], Loss: 0.3132\n",
      "Epoch [1/1], Step [1235/3504], Loss: 0.3454\n",
      "Epoch [1/1], Step [1236/3504], Loss: 0.3704\n",
      "Epoch [1/1], Step [1237/3504], Loss: 0.3649\n",
      "Epoch [1/1], Step [1238/3504], Loss: 0.3111\n",
      "Epoch [1/1], Step [1239/3504], Loss: 0.3576\n",
      "Epoch [1/1], Step [1240/3504], Loss: 0.3588\n",
      "Epoch [1/1], Step [1241/3504], Loss: 0.2855\n",
      "Epoch [1/1], Step [1242/3504], Loss: 0.3117\n",
      "Epoch [1/1], Step [1243/3504], Loss: 0.4050\n",
      "Epoch [1/1], Step [1244/3504], Loss: 0.3954\n",
      "Epoch [1/1], Step [1245/3504], Loss: 0.3332\n",
      "Epoch [1/1], Step [1246/3504], Loss: 0.4261\n",
      "Epoch [1/1], Step [1247/3504], Loss: 0.3679\n",
      "Epoch [1/1], Step [1248/3504], Loss: 0.4012\n",
      "Epoch [1/1], Step [1249/3504], Loss: 0.3637\n",
      "Epoch [1/1], Step [1250/3504], Loss: 0.4084\n",
      "Epoch [1/1], Step [1251/3504], Loss: 0.3606\n",
      "Epoch [1/1], Step [1252/3504], Loss: 0.3223\n",
      "Epoch [1/1], Step [1253/3504], Loss: 0.2908\n",
      "Epoch [1/1], Step [1254/3504], Loss: 0.3051\n",
      "Epoch [1/1], Step [1255/3504], Loss: 0.3125\n",
      "Epoch [1/1], Step [1256/3504], Loss: 0.2584\n",
      "Epoch [1/1], Step [1257/3504], Loss: 0.2740\n",
      "Epoch [1/1], Step [1258/3504], Loss: 0.3350\n",
      "Epoch [1/1], Step [1259/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [1260/3504], Loss: 0.2946\n",
      "Epoch [1/1], Step [1261/3504], Loss: 0.3813\n",
      "Epoch [1/1], Step [1262/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [1263/3504], Loss: 0.4333\n",
      "Epoch [1/1], Step [1264/3504], Loss: 0.4265\n",
      "Epoch [1/1], Step [1265/3504], Loss: 0.4531\n",
      "Epoch [1/1], Step [1266/3504], Loss: 0.3489\n",
      "Epoch [1/1], Step [1267/3504], Loss: 0.3855\n",
      "Epoch [1/1], Step [1268/3504], Loss: 0.3531\n",
      "Epoch [1/1], Step [1269/3504], Loss: 0.3488\n",
      "Epoch [1/1], Step [1270/3504], Loss: 0.4371\n",
      "Epoch [1/1], Step [1271/3504], Loss: 0.3940\n",
      "Epoch [1/1], Step [1272/3504], Loss: 0.3961\n",
      "Epoch [1/1], Step [1273/3504], Loss: 0.4219\n",
      "Epoch [1/1], Step [1274/3504], Loss: 0.3431\n",
      "Epoch [1/1], Step [1275/3504], Loss: 0.3508\n",
      "Epoch [1/1], Step [1276/3504], Loss: 0.3687\n",
      "Epoch [1/1], Step [1277/3504], Loss: 0.3810\n",
      "Epoch [1/1], Step [1278/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [1279/3504], Loss: 0.3708\n",
      "Epoch [1/1], Step [1280/3504], Loss: 0.3271\n",
      "Epoch [1/1], Step [1281/3504], Loss: 0.4051\n",
      "Epoch [1/1], Step [1282/3504], Loss: 0.5239\n",
      "Epoch [1/1], Step [1283/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [1284/3504], Loss: 0.2927\n",
      "Epoch [1/1], Step [1285/3504], Loss: 0.2750\n",
      "Epoch [1/1], Step [1286/3504], Loss: 0.4593\n",
      "Epoch [1/1], Step [1287/3504], Loss: 0.3466\n",
      "Epoch [1/1], Step [1288/3504], Loss: 0.3599\n",
      "Epoch [1/1], Step [1289/3504], Loss: 0.3222\n",
      "Epoch [1/1], Step [1290/3504], Loss: 0.4645\n",
      "Epoch [1/1], Step [1291/3504], Loss: 0.2984\n",
      "Epoch [1/1], Step [1292/3504], Loss: 0.4618\n",
      "Epoch [1/1], Step [1293/3504], Loss: 0.3949\n",
      "Epoch [1/1], Step [1294/3504], Loss: 0.3761\n",
      "Epoch [1/1], Step [1295/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [1296/3504], Loss: 0.2891\n",
      "Epoch [1/1], Step [1297/3504], Loss: 0.5087\n",
      "Epoch [1/1], Step [1298/3504], Loss: 0.3871\n",
      "Epoch [1/1], Step [1299/3504], Loss: 0.4184\n",
      "Epoch [1/1], Step [1300/3504], Loss: 0.4583\n",
      "Epoch [1/1], Step [1301/3504], Loss: 0.4312\n",
      "Epoch [1/1], Step [1302/3504], Loss: 0.4235\n",
      "Epoch [1/1], Step [1303/3504], Loss: 0.4256\n",
      "Epoch [1/1], Step [1304/3504], Loss: 0.4253\n",
      "Epoch [1/1], Step [1305/3504], Loss: 0.3226\n",
      "Epoch [1/1], Step [1306/3504], Loss: 0.4237\n",
      "Epoch [1/1], Step [1307/3504], Loss: 0.3672\n",
      "Epoch [1/1], Step [1308/3504], Loss: 0.4835\n",
      "Epoch [1/1], Step [1309/3504], Loss: 0.4349\n",
      "Epoch [1/1], Step [1310/3504], Loss: 0.4575\n",
      "Epoch [1/1], Step [1311/3504], Loss: 0.4272\n",
      "Epoch [1/1], Step [1312/3504], Loss: 0.5345\n",
      "Epoch [1/1], Step [1313/3504], Loss: 0.4633\n",
      "Epoch [1/1], Step [1314/3504], Loss: 0.3836\n",
      "Epoch [1/1], Step [1315/3504], Loss: 0.4197\n",
      "Epoch [1/1], Step [1316/3504], Loss: 0.4162\n",
      "Epoch [1/1], Step [1317/3504], Loss: 0.3751\n",
      "Epoch [1/1], Step [1318/3504], Loss: 0.3419\n",
      "Epoch [1/1], Step [1319/3504], Loss: 0.4688\n",
      "Epoch [1/1], Step [1320/3504], Loss: 0.3753\n",
      "Epoch [1/1], Step [1321/3504], Loss: 0.4788\n",
      "Epoch [1/1], Step [1322/3504], Loss: 0.3776\n",
      "Epoch [1/1], Step [1323/3504], Loss: 0.4081\n",
      "Epoch [1/1], Step [1324/3504], Loss: 0.4054\n",
      "Epoch [1/1], Step [1325/3504], Loss: 0.3299\n",
      "Epoch [1/1], Step [1326/3504], Loss: 0.4131\n",
      "Epoch [1/1], Step [1327/3504], Loss: 0.3034\n",
      "Epoch [1/1], Step [1328/3504], Loss: 0.3782\n",
      "Epoch [1/1], Step [1329/3504], Loss: 0.4311\n",
      "Epoch [1/1], Step [1330/3504], Loss: 0.3758\n",
      "Epoch [1/1], Step [1331/3504], Loss: 0.4185\n",
      "Epoch [1/1], Step [1332/3504], Loss: 0.3873\n",
      "Epoch [1/1], Step [1333/3504], Loss: 0.3856\n",
      "Epoch [1/1], Step [1334/3504], Loss: 0.4161\n",
      "Epoch [1/1], Step [1335/3504], Loss: 0.3562\n",
      "Epoch [1/1], Step [1336/3504], Loss: 0.3213\n",
      "Epoch [1/1], Step [1337/3504], Loss: 0.4312\n",
      "Epoch [1/1], Step [1338/3504], Loss: 0.3113\n",
      "Epoch [1/1], Step [1339/3504], Loss: 0.3497\n",
      "Epoch [1/1], Step [1340/3504], Loss: 0.4823\n",
      "Epoch [1/1], Step [1341/3504], Loss: 0.3831\n",
      "Epoch [1/1], Step [1342/3504], Loss: 0.4149\n",
      "Epoch [1/1], Step [1343/3504], Loss: 0.3292\n",
      "Epoch [1/1], Step [1344/3504], Loss: 0.4102\n",
      "Epoch [1/1], Step [1345/3504], Loss: 0.4648\n",
      "Epoch [1/1], Step [1346/3504], Loss: 0.4174\n",
      "Epoch [1/1], Step [1347/3504], Loss: 0.4098\n",
      "Epoch [1/1], Step [1348/3504], Loss: 0.3867\n",
      "Epoch [1/1], Step [1349/3504], Loss: 0.3947\n",
      "Epoch [1/1], Step [1350/3504], Loss: 0.4041\n",
      "Epoch [1/1], Step [1351/3504], Loss: 0.3850\n",
      "Epoch [1/1], Step [1352/3504], Loss: 0.3610\n",
      "Epoch [1/1], Step [1353/3504], Loss: 0.3479\n",
      "Epoch [1/1], Step [1354/3504], Loss: 0.2509\n",
      "Epoch [1/1], Step [1355/3504], Loss: 0.4918\n",
      "Epoch [1/1], Step [1356/3504], Loss: 0.4809\n",
      "Epoch [1/1], Step [1357/3504], Loss: 0.3086\n",
      "Epoch [1/1], Step [1358/3504], Loss: 0.4179\n",
      "Epoch [1/1], Step [1359/3504], Loss: 0.3147\n",
      "Epoch [1/1], Step [1360/3504], Loss: 0.4044\n",
      "Epoch [1/1], Step [1361/3504], Loss: 0.3310\n",
      "Epoch [1/1], Step [1362/3504], Loss: 0.2949\n",
      "Epoch [1/1], Step [1363/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [1364/3504], Loss: 0.2704\n",
      "Epoch [1/1], Step [1365/3504], Loss: 0.2954\n",
      "Epoch [1/1], Step [1366/3504], Loss: 0.4517\n",
      "Epoch [1/1], Step [1367/3504], Loss: 0.4260\n",
      "Epoch [1/1], Step [1368/3504], Loss: 0.4156\n",
      "Epoch [1/1], Step [1369/3504], Loss: 0.4892\n",
      "Epoch [1/1], Step [1370/3504], Loss: 0.3829\n",
      "Epoch [1/1], Step [1371/3504], Loss: 0.4231\n",
      "Epoch [1/1], Step [1372/3504], Loss: 0.3578\n",
      "Epoch [1/1], Step [1373/3504], Loss: 0.3222\n",
      "Epoch [1/1], Step [1374/3504], Loss: 0.4041\n",
      "Epoch [1/1], Step [1375/3504], Loss: 0.3641\n",
      "Epoch [1/1], Step [1376/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [1377/3504], Loss: 0.4466\n",
      "Epoch [1/1], Step [1378/3504], Loss: 0.3262\n",
      "Epoch [1/1], Step [1379/3504], Loss: 0.3974\n",
      "Epoch [1/1], Step [1380/3504], Loss: 0.3539\n",
      "Epoch [1/1], Step [1381/3504], Loss: 0.3622\n",
      "Epoch [1/1], Step [1382/3504], Loss: 0.3603\n",
      "Epoch [1/1], Step [1383/3504], Loss: 0.2562\n",
      "Epoch [1/1], Step [1384/3504], Loss: 0.4325\n",
      "Epoch [1/1], Step [1385/3504], Loss: 0.4075\n",
      "Epoch [1/1], Step [1386/3504], Loss: 0.4512\n",
      "Epoch [1/1], Step [1387/3504], Loss: 0.4288\n",
      "Epoch [1/1], Step [1388/3504], Loss: 0.2920\n",
      "Epoch [1/1], Step [1389/3504], Loss: 0.3830\n",
      "Epoch [1/1], Step [1390/3504], Loss: 0.3390\n",
      "Epoch [1/1], Step [1391/3504], Loss: 0.4332\n",
      "Epoch [1/1], Step [1392/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [1393/3504], Loss: 0.3674\n",
      "Epoch [1/1], Step [1394/3504], Loss: 0.3412\n",
      "Epoch [1/1], Step [1395/3504], Loss: 0.3764\n",
      "Epoch [1/1], Step [1396/3504], Loss: 0.3664\n",
      "Epoch [1/1], Step [1397/3504], Loss: 0.3230\n",
      "Epoch [1/1], Step [1398/3504], Loss: 0.3852\n",
      "Epoch [1/1], Step [1399/3504], Loss: 0.3063\n",
      "Epoch [1/1], Step [1400/3504], Loss: 0.3967\n",
      "Epoch [1/1], Step [1401/3504], Loss: 0.3383\n",
      "Epoch [1/1], Step [1402/3504], Loss: 0.3263\n",
      "Epoch [1/1], Step [1403/3504], Loss: 0.2817\n",
      "Epoch [1/1], Step [1404/3504], Loss: 0.4051\n",
      "Epoch [1/1], Step [1405/3504], Loss: 0.4143\n",
      "Epoch [1/1], Step [1406/3504], Loss: 0.3271\n",
      "Epoch [1/1], Step [1407/3504], Loss: 0.3876\n",
      "Epoch [1/1], Step [1408/3504], Loss: 0.3787\n",
      "Epoch [1/1], Step [1409/3504], Loss: 0.3899\n",
      "Epoch [1/1], Step [1410/3504], Loss: 0.3568\n",
      "Epoch [1/1], Step [1411/3504], Loss: 0.3826\n",
      "Epoch [1/1], Step [1412/3504], Loss: 0.3985\n",
      "Epoch [1/1], Step [1413/3504], Loss: 0.2891\n",
      "Epoch [1/1], Step [1414/3504], Loss: 0.3810\n",
      "Epoch [1/1], Step [1415/3504], Loss: 0.3175\n",
      "Epoch [1/1], Step [1416/3504], Loss: 0.4182\n",
      "Epoch [1/1], Step [1417/3504], Loss: 0.3781\n",
      "Epoch [1/1], Step [1418/3504], Loss: 0.3469\n",
      "Epoch [1/1], Step [1419/3504], Loss: 0.3400\n",
      "Epoch [1/1], Step [1420/3504], Loss: 0.2764\n",
      "Epoch [1/1], Step [1421/3504], Loss: 0.3631\n",
      "Epoch [1/1], Step [1422/3504], Loss: 0.3945\n",
      "Epoch [1/1], Step [1423/3504], Loss: 0.3580\n",
      "Epoch [1/1], Step [1424/3504], Loss: 0.3181\n",
      "Epoch [1/1], Step [1425/3504], Loss: 0.3527\n",
      "Epoch [1/1], Step [1426/3504], Loss: 0.3430\n",
      "Epoch [1/1], Step [1427/3504], Loss: 0.3793\n",
      "Epoch [1/1], Step [1428/3504], Loss: 0.4998\n",
      "Epoch [1/1], Step [1429/3504], Loss: 0.4409\n",
      "Epoch [1/1], Step [1430/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [1431/3504], Loss: 0.3752\n",
      "Epoch [1/1], Step [1432/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [1433/3504], Loss: 0.4790\n",
      "Epoch [1/1], Step [1434/3504], Loss: 0.3870\n",
      "Epoch [1/1], Step [1435/3504], Loss: 0.4596\n",
      "Epoch [1/1], Step [1436/3504], Loss: 0.4112\n",
      "Epoch [1/1], Step [1437/3504], Loss: 0.3617\n",
      "Epoch [1/1], Step [1438/3504], Loss: 0.3700\n",
      "Epoch [1/1], Step [1439/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [1440/3504], Loss: 0.4392\n",
      "Epoch [1/1], Step [1441/3504], Loss: 0.4132\n",
      "Epoch [1/1], Step [1442/3504], Loss: 0.3628\n",
      "Epoch [1/1], Step [1443/3504], Loss: 0.3959\n",
      "Epoch [1/1], Step [1444/3504], Loss: 0.3976\n",
      "Epoch [1/1], Step [1445/3504], Loss: 0.4088\n",
      "Epoch [1/1], Step [1446/3504], Loss: 0.3621\n",
      "Epoch [1/1], Step [1447/3504], Loss: 0.3428\n",
      "Epoch [1/1], Step [1448/3504], Loss: 0.3528\n",
      "Epoch [1/1], Step [1449/3504], Loss: 0.4174\n",
      "Epoch [1/1], Step [1450/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [1451/3504], Loss: 0.3134\n",
      "Epoch [1/1], Step [1452/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [1453/3504], Loss: 0.2860\n",
      "Epoch [1/1], Step [1454/3504], Loss: 0.3760\n",
      "Epoch [1/1], Step [1455/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [1456/3504], Loss: 0.3507\n",
      "Epoch [1/1], Step [1457/3504], Loss: 0.3406\n",
      "Epoch [1/1], Step [1458/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [1459/3504], Loss: 0.4053\n",
      "Epoch [1/1], Step [1460/3504], Loss: 0.3628\n",
      "Epoch [1/1], Step [1461/3504], Loss: 0.4300\n",
      "Epoch [1/1], Step [1462/3504], Loss: 0.3341\n",
      "Epoch [1/1], Step [1463/3504], Loss: 0.3922\n",
      "Epoch [1/1], Step [1464/3504], Loss: 0.3657\n",
      "Epoch [1/1], Step [1465/3504], Loss: 0.3460\n",
      "Epoch [1/1], Step [1466/3504], Loss: 0.3155\n",
      "Epoch [1/1], Step [1467/3504], Loss: 0.4718\n",
      "Epoch [1/1], Step [1468/3504], Loss: 0.5336\n",
      "Epoch [1/1], Step [1469/3504], Loss: 0.4056\n",
      "Epoch [1/1], Step [1470/3504], Loss: 0.3360\n",
      "Epoch [1/1], Step [1471/3504], Loss: 0.4604\n",
      "Epoch [1/1], Step [1472/3504], Loss: 0.3422\n",
      "Epoch [1/1], Step [1473/3504], Loss: 0.3875\n",
      "Epoch [1/1], Step [1474/3504], Loss: 0.3634\n",
      "Epoch [1/1], Step [1475/3504], Loss: 0.3585\n",
      "Epoch [1/1], Step [1476/3504], Loss: 0.3067\n",
      "Epoch [1/1], Step [1477/3504], Loss: 0.3045\n",
      "Epoch [1/1], Step [1478/3504], Loss: 0.4254\n",
      "Epoch [1/1], Step [1479/3504], Loss: 0.4429\n",
      "Epoch [1/1], Step [1480/3504], Loss: 0.2859\n",
      "Epoch [1/1], Step [1481/3504], Loss: 0.4136\n",
      "Epoch [1/1], Step [1482/3504], Loss: 0.4240\n",
      "Epoch [1/1], Step [1483/3504], Loss: 0.3428\n",
      "Epoch [1/1], Step [1484/3504], Loss: 0.3995\n",
      "Epoch [1/1], Step [1485/3504], Loss: 0.2564\n",
      "Epoch [1/1], Step [1486/3504], Loss: 0.2957\n",
      "Epoch [1/1], Step [1487/3504], Loss: 0.3581\n",
      "Epoch [1/1], Step [1488/3504], Loss: 0.3987\n",
      "Epoch [1/1], Step [1489/3504], Loss: 0.3374\n",
      "Epoch [1/1], Step [1490/3504], Loss: 0.5122\n",
      "Epoch [1/1], Step [1491/3504], Loss: 0.4227\n",
      "Epoch [1/1], Step [1492/3504], Loss: 0.2408\n",
      "Epoch [1/1], Step [1493/3504], Loss: 0.4464\n",
      "Epoch [1/1], Step [1494/3504], Loss: 0.4200\n",
      "Epoch [1/1], Step [1495/3504], Loss: 0.3470\n",
      "Epoch [1/1], Step [1496/3504], Loss: 0.3276\n",
      "Epoch [1/1], Step [1497/3504], Loss: 0.4142\n",
      "Epoch [1/1], Step [1498/3504], Loss: 0.4058\n",
      "Epoch [1/1], Step [1499/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [1500/3504], Loss: 0.3839\n",
      "Epoch [1/1], Step [1501/3504], Loss: 0.3738\n",
      "Epoch [1/1], Step [1502/3504], Loss: 0.4354\n",
      "Epoch [1/1], Step [1503/3504], Loss: 0.3605\n",
      "Epoch [1/1], Step [1504/3504], Loss: 0.4240\n",
      "Epoch [1/1], Step [1505/3504], Loss: 0.4393\n",
      "Epoch [1/1], Step [1506/3504], Loss: 0.3334\n",
      "Epoch [1/1], Step [1507/3504], Loss: 0.3373\n",
      "Epoch [1/1], Step [1508/3504], Loss: 0.3913\n",
      "Epoch [1/1], Step [1509/3504], Loss: 0.4375\n",
      "Epoch [1/1], Step [1510/3504], Loss: 0.3985\n",
      "Epoch [1/1], Step [1511/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [1512/3504], Loss: 0.4120\n",
      "Epoch [1/1], Step [1513/3504], Loss: 0.4466\n",
      "Epoch [1/1], Step [1514/3504], Loss: 0.3346\n",
      "Epoch [1/1], Step [1515/3504], Loss: 0.3351\n",
      "Epoch [1/1], Step [1516/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [1517/3504], Loss: 0.4355\n",
      "Epoch [1/1], Step [1518/3504], Loss: 0.3854\n",
      "Epoch [1/1], Step [1519/3504], Loss: 0.4219\n",
      "Epoch [1/1], Step [1520/3504], Loss: 0.4812\n",
      "Epoch [1/1], Step [1521/3504], Loss: 0.3867\n",
      "Epoch [1/1], Step [1522/3504], Loss: 0.3727\n",
      "Epoch [1/1], Step [1523/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [1524/3504], Loss: 0.4052\n",
      "Epoch [1/1], Step [1525/3504], Loss: 0.4028\n",
      "Epoch [1/1], Step [1526/3504], Loss: 0.3177\n",
      "Epoch [1/1], Step [1527/3504], Loss: 0.3288\n",
      "Epoch [1/1], Step [1528/3504], Loss: 0.3693\n",
      "Epoch [1/1], Step [1529/3504], Loss: 0.3225\n",
      "Epoch [1/1], Step [1530/3504], Loss: 0.3836\n",
      "Epoch [1/1], Step [1531/3504], Loss: 0.2688\n",
      "Epoch [1/1], Step [1532/3504], Loss: 0.2432\n",
      "Epoch [1/1], Step [1533/3504], Loss: 0.3165\n",
      "Epoch [1/1], Step [1534/3504], Loss: 0.3738\n",
      "Epoch [1/1], Step [1535/3504], Loss: 0.3522\n",
      "Epoch [1/1], Step [1536/3504], Loss: 0.3419\n",
      "Epoch [1/1], Step [1537/3504], Loss: 0.3799\n",
      "Epoch [1/1], Step [1538/3504], Loss: 0.3763\n",
      "Epoch [1/1], Step [1539/3504], Loss: 0.3561\n",
      "Epoch [1/1], Step [1540/3504], Loss: 0.3503\n",
      "Epoch [1/1], Step [1541/3504], Loss: 0.2774\n",
      "Epoch [1/1], Step [1542/3504], Loss: 0.3327\n",
      "Epoch [1/1], Step [1543/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [1544/3504], Loss: 0.3606\n",
      "Epoch [1/1], Step [1545/3504], Loss: 0.3859\n",
      "Epoch [1/1], Step [1546/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [1547/3504], Loss: 0.3714\n",
      "Epoch [1/1], Step [1548/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [1549/3504], Loss: 0.4274\n",
      "Epoch [1/1], Step [1550/3504], Loss: 0.4360\n",
      "Epoch [1/1], Step [1551/3504], Loss: 0.4125\n",
      "Epoch [1/1], Step [1552/3504], Loss: 0.4762\n",
      "Epoch [1/1], Step [1553/3504], Loss: 0.3805\n",
      "Epoch [1/1], Step [1554/3504], Loss: 0.4049\n",
      "Epoch [1/1], Step [1555/3504], Loss: 0.4467\n",
      "Epoch [1/1], Step [1556/3504], Loss: 0.3838\n",
      "Epoch [1/1], Step [1557/3504], Loss: 0.3069\n",
      "Epoch [1/1], Step [1558/3504], Loss: 0.3639\n",
      "Epoch [1/1], Step [1559/3504], Loss: 0.4199\n",
      "Epoch [1/1], Step [1560/3504], Loss: 0.4384\n",
      "Epoch [1/1], Step [1561/3504], Loss: 0.4362\n",
      "Epoch [1/1], Step [1562/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [1563/3504], Loss: 0.4492\n",
      "Epoch [1/1], Step [1564/3504], Loss: 0.4754\n",
      "Epoch [1/1], Step [1565/3504], Loss: 0.3305\n",
      "Epoch [1/1], Step [1566/3504], Loss: 0.3231\n",
      "Epoch [1/1], Step [1567/3504], Loss: 0.2888\n",
      "Epoch [1/1], Step [1568/3504], Loss: 0.2400\n",
      "Epoch [1/1], Step [1569/3504], Loss: 0.2991\n",
      "Epoch [1/1], Step [1570/3504], Loss: 0.4541\n",
      "Epoch [1/1], Step [1571/3504], Loss: 0.4134\n",
      "Epoch [1/1], Step [1572/3504], Loss: 0.3236\n",
      "Epoch [1/1], Step [1573/3504], Loss: 0.3554\n",
      "Epoch [1/1], Step [1574/3504], Loss: 0.3134\n",
      "Epoch [1/1], Step [1575/3504], Loss: 0.3896\n",
      "Epoch [1/1], Step [1576/3504], Loss: 0.3855\n",
      "Epoch [1/1], Step [1577/3504], Loss: 0.4054\n",
      "Epoch [1/1], Step [1578/3504], Loss: 0.2932\n",
      "Epoch [1/1], Step [1579/3504], Loss: 0.4672\n",
      "Epoch [1/1], Step [1580/3504], Loss: 0.4053\n",
      "Epoch [1/1], Step [1581/3504], Loss: 0.3861\n",
      "Epoch [1/1], Step [1582/3504], Loss: 0.3493\n",
      "Epoch [1/1], Step [1583/3504], Loss: 0.4674\n",
      "Epoch [1/1], Step [1584/3504], Loss: 0.3333\n",
      "Epoch [1/1], Step [1585/3504], Loss: 0.3278\n",
      "Epoch [1/1], Step [1586/3504], Loss: 0.3217\n",
      "Epoch [1/1], Step [1587/3504], Loss: 0.3941\n",
      "Epoch [1/1], Step [1588/3504], Loss: 0.3127\n",
      "Epoch [1/1], Step [1589/3504], Loss: 0.4561\n",
      "Epoch [1/1], Step [1590/3504], Loss: 0.3959\n",
      "Epoch [1/1], Step [1591/3504], Loss: 0.4208\n",
      "Epoch [1/1], Step [1592/3504], Loss: 0.3764\n",
      "Epoch [1/1], Step [1593/3504], Loss: 0.4334\n",
      "Epoch [1/1], Step [1594/3504], Loss: 0.2832\n",
      "Epoch [1/1], Step [1595/3504], Loss: 0.3921\n",
      "Epoch [1/1], Step [1596/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [1597/3504], Loss: 0.3388\n",
      "Epoch [1/1], Step [1598/3504], Loss: 0.3171\n",
      "Epoch [1/1], Step [1599/3504], Loss: 0.3795\n",
      "Epoch [1/1], Step [1600/3504], Loss: 0.4313\n",
      "Epoch [1/1], Step [1601/3504], Loss: 0.3917\n",
      "Epoch [1/1], Step [1602/3504], Loss: 0.3679\n",
      "Epoch [1/1], Step [1603/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [1604/3504], Loss: 0.4863\n",
      "Epoch [1/1], Step [1605/3504], Loss: 0.3225\n",
      "Epoch [1/1], Step [1606/3504], Loss: 0.3529\n",
      "Epoch [1/1], Step [1607/3504], Loss: 0.3730\n",
      "Epoch [1/1], Step [1608/3504], Loss: 0.3331\n",
      "Epoch [1/1], Step [1609/3504], Loss: 0.5592\n",
      "Epoch [1/1], Step [1610/3504], Loss: 0.4581\n",
      "Epoch [1/1], Step [1611/3504], Loss: 0.3476\n",
      "Epoch [1/1], Step [1612/3504], Loss: 0.4706\n",
      "Epoch [1/1], Step [1613/3504], Loss: 0.4601\n",
      "Epoch [1/1], Step [1614/3504], Loss: 0.4011\n",
      "Epoch [1/1], Step [1615/3504], Loss: 0.4266\n",
      "Epoch [1/1], Step [1616/3504], Loss: 0.4116\n",
      "Epoch [1/1], Step [1617/3504], Loss: 0.4041\n",
      "Epoch [1/1], Step [1618/3504], Loss: 0.4777\n",
      "Epoch [1/1], Step [1619/3504], Loss: 0.3460\n",
      "Epoch [1/1], Step [1620/3504], Loss: 0.4121\n",
      "Epoch [1/1], Step [1621/3504], Loss: 0.3529\n",
      "Epoch [1/1], Step [1622/3504], Loss: 0.3799\n",
      "Epoch [1/1], Step [1623/3504], Loss: 0.4837\n",
      "Epoch [1/1], Step [1624/3504], Loss: 0.3455\n",
      "Epoch [1/1], Step [1625/3504], Loss: 0.3373\n",
      "Epoch [1/1], Step [1626/3504], Loss: 0.4208\n",
      "Epoch [1/1], Step [1627/3504], Loss: 0.4238\n",
      "Epoch [1/1], Step [1628/3504], Loss: 0.3521\n",
      "Epoch [1/1], Step [1629/3504], Loss: 0.4288\n",
      "Epoch [1/1], Step [1630/3504], Loss: 0.4634\n",
      "Epoch [1/1], Step [1631/3504], Loss: 0.3638\n",
      "Epoch [1/1], Step [1632/3504], Loss: 0.4169\n",
      "Epoch [1/1], Step [1633/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [1634/3504], Loss: 0.3181\n",
      "Epoch [1/1], Step [1635/3504], Loss: 0.4060\n",
      "Epoch [1/1], Step [1636/3504], Loss: 0.3220\n",
      "Epoch [1/1], Step [1637/3504], Loss: 0.3534\n",
      "Epoch [1/1], Step [1638/3504], Loss: 0.3976\n",
      "Epoch [1/1], Step [1639/3504], Loss: 0.4234\n",
      "Epoch [1/1], Step [1640/3504], Loss: 0.3284\n",
      "Epoch [1/1], Step [1641/3504], Loss: 0.2710\n",
      "Epoch [1/1], Step [1642/3504], Loss: 0.3273\n",
      "Epoch [1/1], Step [1643/3504], Loss: 0.4184\n",
      "Epoch [1/1], Step [1644/3504], Loss: 0.3796\n",
      "Epoch [1/1], Step [1645/3504], Loss: 0.3459\n",
      "Epoch [1/1], Step [1646/3504], Loss: 0.4092\n",
      "Epoch [1/1], Step [1647/3504], Loss: 0.4348\n",
      "Epoch [1/1], Step [1648/3504], Loss: 0.3962\n",
      "Epoch [1/1], Step [1649/3504], Loss: 0.3286\n",
      "Epoch [1/1], Step [1650/3504], Loss: 0.4621\n",
      "Epoch [1/1], Step [1651/3504], Loss: 0.3936\n",
      "Epoch [1/1], Step [1652/3504], Loss: 0.3481\n",
      "Epoch [1/1], Step [1653/3504], Loss: 0.3546\n",
      "Epoch [1/1], Step [1654/3504], Loss: 0.3795\n",
      "Epoch [1/1], Step [1655/3504], Loss: 0.3913\n",
      "Epoch [1/1], Step [1656/3504], Loss: 0.3455\n",
      "Epoch [1/1], Step [1657/3504], Loss: 0.3412\n",
      "Epoch [1/1], Step [1658/3504], Loss: 0.3949\n",
      "Epoch [1/1], Step [1659/3504], Loss: 0.3796\n",
      "Epoch [1/1], Step [1660/3504], Loss: 0.2815\n",
      "Epoch [1/1], Step [1661/3504], Loss: 0.3351\n",
      "Epoch [1/1], Step [1662/3504], Loss: 0.4865\n",
      "Epoch [1/1], Step [1663/3504], Loss: 0.3961\n",
      "Epoch [1/1], Step [1664/3504], Loss: 0.3641\n",
      "Epoch [1/1], Step [1665/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [1666/3504], Loss: 0.5003\n",
      "Epoch [1/1], Step [1667/3504], Loss: 0.4224\n",
      "Epoch [1/1], Step [1668/3504], Loss: 0.4569\n",
      "Epoch [1/1], Step [1669/3504], Loss: 0.3696\n",
      "Epoch [1/1], Step [1670/3504], Loss: 0.3854\n",
      "Epoch [1/1], Step [1671/3504], Loss: 0.4178\n",
      "Epoch [1/1], Step [1672/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [1673/3504], Loss: 0.3696\n",
      "Epoch [1/1], Step [1674/3504], Loss: 0.3123\n",
      "Epoch [1/1], Step [1675/3504], Loss: 0.3256\n",
      "Epoch [1/1], Step [1676/3504], Loss: 0.3907\n",
      "Epoch [1/1], Step [1677/3504], Loss: 0.3367\n",
      "Epoch [1/1], Step [1678/3504], Loss: 0.3299\n",
      "Epoch [1/1], Step [1679/3504], Loss: 0.3964\n",
      "Epoch [1/1], Step [1680/3504], Loss: 0.5032\n",
      "Epoch [1/1], Step [1681/3504], Loss: 0.3793\n",
      "Epoch [1/1], Step [1682/3504], Loss: 0.3395\n",
      "Epoch [1/1], Step [1683/3504], Loss: 0.6257\n",
      "Epoch [1/1], Step [1684/3504], Loss: 0.4757\n",
      "Epoch [1/1], Step [1685/3504], Loss: 0.3655\n",
      "Epoch [1/1], Step [1686/3504], Loss: 0.3337\n",
      "Epoch [1/1], Step [1687/3504], Loss: 0.3637\n",
      "Epoch [1/1], Step [1688/3504], Loss: 0.4315\n",
      "Epoch [1/1], Step [1689/3504], Loss: 0.4692\n",
      "Epoch [1/1], Step [1690/3504], Loss: 0.4147\n",
      "Epoch [1/1], Step [1691/3504], Loss: 0.3945\n",
      "Epoch [1/1], Step [1692/3504], Loss: 0.3996\n",
      "Epoch [1/1], Step [1693/3504], Loss: 0.2987\n",
      "Epoch [1/1], Step [1694/3504], Loss: 0.3439\n",
      "Epoch [1/1], Step [1695/3504], Loss: 0.3944\n",
      "Epoch [1/1], Step [1696/3504], Loss: 0.3618\n",
      "Epoch [1/1], Step [1697/3504], Loss: 0.3555\n",
      "Epoch [1/1], Step [1698/3504], Loss: 0.3629\n",
      "Epoch [1/1], Step [1699/3504], Loss: 0.3049\n",
      "Epoch [1/1], Step [1700/3504], Loss: 0.3209\n",
      "Epoch [1/1], Step [1701/3504], Loss: 0.3180\n",
      "Epoch [1/1], Step [1702/3504], Loss: 0.3312\n",
      "Epoch [1/1], Step [1703/3504], Loss: 0.3601\n",
      "Epoch [1/1], Step [1704/3504], Loss: 0.3598\n",
      "Epoch [1/1], Step [1705/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [1706/3504], Loss: 0.4615\n",
      "Epoch [1/1], Step [1707/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [1708/3504], Loss: 0.4613\n",
      "Epoch [1/1], Step [1709/3504], Loss: 0.3509\n",
      "Epoch [1/1], Step [1710/3504], Loss: 0.3208\n",
      "Epoch [1/1], Step [1711/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [1712/3504], Loss: 0.4087\n",
      "Epoch [1/1], Step [1713/3504], Loss: 0.3947\n",
      "Epoch [1/1], Step [1714/3504], Loss: 0.3424\n",
      "Epoch [1/1], Step [1715/3504], Loss: 0.3357\n",
      "Epoch [1/1], Step [1716/3504], Loss: 0.4054\n",
      "Epoch [1/1], Step [1717/3504], Loss: 0.4464\n",
      "Epoch [1/1], Step [1718/3504], Loss: 0.3200\n",
      "Epoch [1/1], Step [1719/3504], Loss: 0.3428\n",
      "Epoch [1/1], Step [1720/3504], Loss: 0.5297\n",
      "Epoch [1/1], Step [1721/3504], Loss: 0.3949\n",
      "Epoch [1/1], Step [1722/3504], Loss: 0.4065\n",
      "Epoch [1/1], Step [1723/3504], Loss: 0.3123\n",
      "Epoch [1/1], Step [1724/3504], Loss: 0.3511\n",
      "Epoch [1/1], Step [1725/3504], Loss: 0.3539\n",
      "Epoch [1/1], Step [1726/3504], Loss: 0.4143\n",
      "Epoch [1/1], Step [1727/3504], Loss: 0.3603\n",
      "Epoch [1/1], Step [1728/3504], Loss: 0.3345\n",
      "Epoch [1/1], Step [1729/3504], Loss: 0.2994\n",
      "Epoch [1/1], Step [1730/3504], Loss: 0.3893\n",
      "Epoch [1/1], Step [1731/3504], Loss: 0.4141\n",
      "Epoch [1/1], Step [1732/3504], Loss: 0.4078\n",
      "Epoch [1/1], Step [1733/3504], Loss: 0.4976\n",
      "Epoch [1/1], Step [1734/3504], Loss: 0.4011\n",
      "Epoch [1/1], Step [1735/3504], Loss: 0.3054\n",
      "Epoch [1/1], Step [1736/3504], Loss: 0.2960\n",
      "Epoch [1/1], Step [1737/3504], Loss: 0.3721\n",
      "Epoch [1/1], Step [1738/3504], Loss: 0.2584\n",
      "Epoch [1/1], Step [1739/3504], Loss: 0.3700\n",
      "Epoch [1/1], Step [1740/3504], Loss: 0.3634\n",
      "Epoch [1/1], Step [1741/3504], Loss: 0.4356\n",
      "Epoch [1/1], Step [1742/3504], Loss: 0.5592\n",
      "Epoch [1/1], Step [1743/3504], Loss: 0.4046\n",
      "Epoch [1/1], Step [1744/3504], Loss: 0.4177\n",
      "Epoch [1/1], Step [1745/3504], Loss: 0.3542\n",
      "Epoch [1/1], Step [1746/3504], Loss: 0.4038\n",
      "Epoch [1/1], Step [1747/3504], Loss: 0.3193\n",
      "Epoch [1/1], Step [1748/3504], Loss: 0.3724\n",
      "Epoch [1/1], Step [1749/3504], Loss: 0.3992\n",
      "Epoch [1/1], Step [1750/3504], Loss: 0.3700\n",
      "Epoch [1/1], Step [1751/3504], Loss: 0.3782\n",
      "Epoch [1/1], Step [1752/3504], Loss: 0.4524\n",
      "Epoch [1/1], Step [1753/3504], Loss: 0.3345\n",
      "Epoch [1/1], Step [1754/3504], Loss: 0.3442\n",
      "Epoch [1/1], Step [1755/3504], Loss: 0.3511\n",
      "Epoch [1/1], Step [1756/3504], Loss: 0.4152\n",
      "Epoch [1/1], Step [1757/3504], Loss: 0.3015\n",
      "Epoch [1/1], Step [1758/3504], Loss: 0.4919\n",
      "Epoch [1/1], Step [1759/3504], Loss: 0.2325\n",
      "Epoch [1/1], Step [1760/3504], Loss: 0.3609\n",
      "Epoch [1/1], Step [1761/3504], Loss: 0.3212\n",
      "Epoch [1/1], Step [1762/3504], Loss: 0.4513\n",
      "Epoch [1/1], Step [1763/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [1764/3504], Loss: 0.3708\n",
      "Epoch [1/1], Step [1765/3504], Loss: 0.3405\n",
      "Epoch [1/1], Step [1766/3504], Loss: 0.3637\n",
      "Epoch [1/1], Step [1767/3504], Loss: 0.3407\n",
      "Epoch [1/1], Step [1768/3504], Loss: 0.3847\n",
      "Epoch [1/1], Step [1769/3504], Loss: 0.3606\n",
      "Epoch [1/1], Step [1770/3504], Loss: 0.2529\n",
      "Epoch [1/1], Step [1771/3504], Loss: 0.2990\n",
      "Epoch [1/1], Step [1772/3504], Loss: 0.4490\n",
      "Epoch [1/1], Step [1773/3504], Loss: 0.3791\n",
      "Epoch [1/1], Step [1774/3504], Loss: 0.2699\n",
      "Epoch [1/1], Step [1775/3504], Loss: 0.3654\n",
      "Epoch [1/1], Step [1776/3504], Loss: 0.4045\n",
      "Epoch [1/1], Step [1777/3504], Loss: 0.3379\n",
      "Epoch [1/1], Step [1778/3504], Loss: 0.4382\n",
      "Epoch [1/1], Step [1779/3504], Loss: 0.4350\n",
      "Epoch [1/1], Step [1780/3504], Loss: 0.3948\n",
      "Epoch [1/1], Step [1781/3504], Loss: 0.3246\n",
      "Epoch [1/1], Step [1782/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [1783/3504], Loss: 0.3532\n",
      "Epoch [1/1], Step [1784/3504], Loss: 0.3187\n",
      "Epoch [1/1], Step [1785/3504], Loss: 0.2958\n",
      "Epoch [1/1], Step [1786/3504], Loss: 0.3694\n",
      "Epoch [1/1], Step [1787/3504], Loss: 0.3531\n",
      "Epoch [1/1], Step [1788/3504], Loss: 0.2940\n",
      "Epoch [1/1], Step [1789/3504], Loss: 0.3771\n",
      "Epoch [1/1], Step [1790/3504], Loss: 0.4184\n",
      "Epoch [1/1], Step [1791/3504], Loss: 0.4269\n",
      "Epoch [1/1], Step [1792/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [1793/3504], Loss: 0.3201\n",
      "Epoch [1/1], Step [1794/3504], Loss: 0.3060\n",
      "Epoch [1/1], Step [1795/3504], Loss: 0.3885\n",
      "Epoch [1/1], Step [1796/3504], Loss: 0.4203\n",
      "Epoch [1/1], Step [1797/3504], Loss: 0.2403\n",
      "Epoch [1/1], Step [1798/3504], Loss: 0.3510\n",
      "Epoch [1/1], Step [1799/3504], Loss: 0.3633\n",
      "Epoch [1/1], Step [1800/3504], Loss: 0.3274\n",
      "Epoch [1/1], Step [1801/3504], Loss: 0.3244\n",
      "Epoch [1/1], Step [1802/3504], Loss: 0.5132\n",
      "Epoch [1/1], Step [1803/3504], Loss: 0.3823\n",
      "Epoch [1/1], Step [1804/3504], Loss: 0.3729\n",
      "Epoch [1/1], Step [1805/3504], Loss: 0.3076\n",
      "Epoch [1/1], Step [1806/3504], Loss: 0.3901\n",
      "Epoch [1/1], Step [1807/3504], Loss: 0.4280\n",
      "Epoch [1/1], Step [1808/3504], Loss: 0.3881\n",
      "Epoch [1/1], Step [1809/3504], Loss: 0.2620\n",
      "Epoch [1/1], Step [1810/3504], Loss: 0.3838\n",
      "Epoch [1/1], Step [1811/3504], Loss: 0.3454\n",
      "Epoch [1/1], Step [1812/3504], Loss: 0.3153\n",
      "Epoch [1/1], Step [1813/3504], Loss: 0.3702\n",
      "Epoch [1/1], Step [1814/3504], Loss: 0.3418\n",
      "Epoch [1/1], Step [1815/3504], Loss: 0.3583\n",
      "Epoch [1/1], Step [1816/3504], Loss: 0.4074\n",
      "Epoch [1/1], Step [1817/3504], Loss: 0.3087\n",
      "Epoch [1/1], Step [1818/3504], Loss: 0.3709\n",
      "Epoch [1/1], Step [1819/3504], Loss: 0.4980\n",
      "Epoch [1/1], Step [1820/3504], Loss: 0.3323\n",
      "Epoch [1/1], Step [1821/3504], Loss: 0.4076\n",
      "Epoch [1/1], Step [1822/3504], Loss: 0.3011\n",
      "Epoch [1/1], Step [1823/3504], Loss: 0.4289\n",
      "Epoch [1/1], Step [1824/3504], Loss: 0.4103\n",
      "Epoch [1/1], Step [1825/3504], Loss: 0.3484\n",
      "Epoch [1/1], Step [1826/3504], Loss: 0.4057\n",
      "Epoch [1/1], Step [1827/3504], Loss: 0.3903\n",
      "Epoch [1/1], Step [1828/3504], Loss: 0.3521\n",
      "Epoch [1/1], Step [1829/3504], Loss: 0.3715\n",
      "Epoch [1/1], Step [1830/3504], Loss: 0.3950\n",
      "Epoch [1/1], Step [1831/3504], Loss: 0.4873\n",
      "Epoch [1/1], Step [1832/3504], Loss: 0.3315\n",
      "Epoch [1/1], Step [1833/3504], Loss: 0.4194\n",
      "Epoch [1/1], Step [1834/3504], Loss: 0.3838\n",
      "Epoch [1/1], Step [1835/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [1836/3504], Loss: 0.3658\n",
      "Epoch [1/1], Step [1837/3504], Loss: 0.4294\n",
      "Epoch [1/1], Step [1838/3504], Loss: 0.3441\n",
      "Epoch [1/1], Step [1839/3504], Loss: 0.4602\n",
      "Epoch [1/1], Step [1840/3504], Loss: 0.4689\n",
      "Epoch [1/1], Step [1841/3504], Loss: 0.3028\n",
      "Epoch [1/1], Step [1842/3504], Loss: 0.4000\n",
      "Epoch [1/1], Step [1843/3504], Loss: 0.4181\n",
      "Epoch [1/1], Step [1844/3504], Loss: 0.4520\n",
      "Epoch [1/1], Step [1845/3504], Loss: 0.2926\n",
      "Epoch [1/1], Step [1846/3504], Loss: 0.4309\n",
      "Epoch [1/1], Step [1847/3504], Loss: 0.5500\n",
      "Epoch [1/1], Step [1848/3504], Loss: 0.4390\n",
      "Epoch [1/1], Step [1849/3504], Loss: 0.3571\n",
      "Epoch [1/1], Step [1850/3504], Loss: 0.3611\n",
      "Epoch [1/1], Step [1851/3504], Loss: 0.4091\n",
      "Epoch [1/1], Step [1852/3504], Loss: 0.4040\n",
      "Epoch [1/1], Step [1853/3504], Loss: 0.4040\n",
      "Epoch [1/1], Step [1854/3504], Loss: 0.4084\n",
      "Epoch [1/1], Step [1855/3504], Loss: 0.4134\n",
      "Epoch [1/1], Step [1856/3504], Loss: 0.3729\n",
      "Epoch [1/1], Step [1857/3504], Loss: 0.4300\n",
      "Epoch [1/1], Step [1858/3504], Loss: 0.3985\n",
      "Epoch [1/1], Step [1859/3504], Loss: 0.3806\n",
      "Epoch [1/1], Step [1860/3504], Loss: 0.4459\n",
      "Epoch [1/1], Step [1861/3504], Loss: 0.3746\n",
      "Epoch [1/1], Step [1862/3504], Loss: 0.4227\n",
      "Epoch [1/1], Step [1863/3504], Loss: 0.4232\n",
      "Epoch [1/1], Step [1864/3504], Loss: 0.3788\n",
      "Epoch [1/1], Step [1865/3504], Loss: 0.4073\n",
      "Epoch [1/1], Step [1866/3504], Loss: 0.3857\n",
      "Epoch [1/1], Step [1867/3504], Loss: 0.4977\n",
      "Epoch [1/1], Step [1868/3504], Loss: 0.3223\n",
      "Epoch [1/1], Step [1869/3504], Loss: 0.5160\n",
      "Epoch [1/1], Step [1870/3504], Loss: 0.4359\n",
      "Epoch [1/1], Step [1871/3504], Loss: 0.4068\n",
      "Epoch [1/1], Step [1872/3504], Loss: 0.2681\n",
      "Epoch [1/1], Step [1873/3504], Loss: 0.4864\n",
      "Epoch [1/1], Step [1874/3504], Loss: 0.3139\n",
      "Epoch [1/1], Step [1875/3504], Loss: 0.3668\n",
      "Epoch [1/1], Step [1876/3504], Loss: 0.3865\n",
      "Epoch [1/1], Step [1877/3504], Loss: 0.3385\n",
      "Epoch [1/1], Step [1878/3504], Loss: 0.4157\n",
      "Epoch [1/1], Step [1879/3504], Loss: 0.3800\n",
      "Epoch [1/1], Step [1880/3504], Loss: 0.3266\n",
      "Epoch [1/1], Step [1881/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [1882/3504], Loss: 0.3578\n",
      "Epoch [1/1], Step [1883/3504], Loss: 0.3164\n",
      "Epoch [1/1], Step [1884/3504], Loss: 0.3192\n",
      "Epoch [1/1], Step [1885/3504], Loss: 0.4605\n",
      "Epoch [1/1], Step [1886/3504], Loss: 0.2921\n",
      "Epoch [1/1], Step [1887/3504], Loss: 0.4365\n",
      "Epoch [1/1], Step [1888/3504], Loss: 0.3747\n",
      "Epoch [1/1], Step [1889/3504], Loss: 0.3359\n",
      "Epoch [1/1], Step [1890/3504], Loss: 0.3290\n",
      "Epoch [1/1], Step [1891/3504], Loss: 0.3028\n",
      "Epoch [1/1], Step [1892/3504], Loss: 0.3960\n",
      "Epoch [1/1], Step [1893/3504], Loss: 0.3173\n",
      "Epoch [1/1], Step [1894/3504], Loss: 0.4097\n",
      "Epoch [1/1], Step [1895/3504], Loss: 0.2828\n",
      "Epoch [1/1], Step [1896/3504], Loss: 0.4394\n",
      "Epoch [1/1], Step [1897/3504], Loss: 0.3460\n",
      "Epoch [1/1], Step [1898/3504], Loss: 0.3630\n",
      "Epoch [1/1], Step [1899/3504], Loss: 0.3607\n",
      "Epoch [1/1], Step [1900/3504], Loss: 0.3602\n",
      "Epoch [1/1], Step [1901/3504], Loss: 0.4328\n",
      "Epoch [1/1], Step [1902/3504], Loss: 0.4156\n",
      "Epoch [1/1], Step [1903/3504], Loss: 0.3258\n",
      "Epoch [1/1], Step [1904/3504], Loss: 0.3662\n",
      "Epoch [1/1], Step [1905/3504], Loss: 0.4032\n",
      "Epoch [1/1], Step [1906/3504], Loss: 0.3277\n",
      "Epoch [1/1], Step [1907/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [1908/3504], Loss: 0.4596\n",
      "Epoch [1/1], Step [1909/3504], Loss: 0.4361\n",
      "Epoch [1/1], Step [1910/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [1911/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [1912/3504], Loss: 0.4581\n",
      "Epoch [1/1], Step [1913/3504], Loss: 0.4115\n",
      "Epoch [1/1], Step [1914/3504], Loss: 0.3801\n",
      "Epoch [1/1], Step [1915/3504], Loss: 0.3159\n",
      "Epoch [1/1], Step [1916/3504], Loss: 0.3635\n",
      "Epoch [1/1], Step [1917/3504], Loss: 0.3874\n",
      "Epoch [1/1], Step [1918/3504], Loss: 0.3640\n",
      "Epoch [1/1], Step [1919/3504], Loss: 0.3556\n",
      "Epoch [1/1], Step [1920/3504], Loss: 0.3167\n",
      "Epoch [1/1], Step [1921/3504], Loss: 0.4090\n",
      "Epoch [1/1], Step [1922/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [1923/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [1924/3504], Loss: 0.3171\n",
      "Epoch [1/1], Step [1925/3504], Loss: 0.3201\n",
      "Epoch [1/1], Step [1926/3504], Loss: 0.3392\n",
      "Epoch [1/1], Step [1927/3504], Loss: 0.3825\n",
      "Epoch [1/1], Step [1928/3504], Loss: 0.4352\n",
      "Epoch [1/1], Step [1929/3504], Loss: 0.3231\n",
      "Epoch [1/1], Step [1930/3504], Loss: 0.3383\n",
      "Epoch [1/1], Step [1931/3504], Loss: 0.4401\n",
      "Epoch [1/1], Step [1932/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [1933/3504], Loss: 0.3409\n",
      "Epoch [1/1], Step [1934/3504], Loss: 0.3175\n",
      "Epoch [1/1], Step [1935/3504], Loss: 0.3244\n",
      "Epoch [1/1], Step [1936/3504], Loss: 0.3003\n",
      "Epoch [1/1], Step [1937/3504], Loss: 0.3887\n",
      "Epoch [1/1], Step [1938/3504], Loss: 0.3478\n",
      "Epoch [1/1], Step [1939/3504], Loss: 0.3007\n",
      "Epoch [1/1], Step [1940/3504], Loss: 0.4089\n",
      "Epoch [1/1], Step [1941/3504], Loss: 0.3904\n",
      "Epoch [1/1], Step [1942/3504], Loss: 0.3799\n",
      "Epoch [1/1], Step [1943/3504], Loss: 0.3843\n",
      "Epoch [1/1], Step [1944/3504], Loss: 0.3291\n",
      "Epoch [1/1], Step [1945/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [1946/3504], Loss: 0.3477\n",
      "Epoch [1/1], Step [1947/3504], Loss: 0.3277\n",
      "Epoch [1/1], Step [1948/3504], Loss: 0.3787\n",
      "Epoch [1/1], Step [1949/3504], Loss: 0.2679\n",
      "Epoch [1/1], Step [1950/3504], Loss: 0.3505\n",
      "Epoch [1/1], Step [1951/3504], Loss: 0.4160\n",
      "Epoch [1/1], Step [1952/3504], Loss: 0.3655\n",
      "Epoch [1/1], Step [1953/3504], Loss: 0.3206\n",
      "Epoch [1/1], Step [1954/3504], Loss: 0.2839\n",
      "Epoch [1/1], Step [1955/3504], Loss: 0.4799\n",
      "Epoch [1/1], Step [1956/3504], Loss: 0.4474\n",
      "Epoch [1/1], Step [1957/3504], Loss: 0.3515\n",
      "Epoch [1/1], Step [1958/3504], Loss: 0.4242\n",
      "Epoch [1/1], Step [1959/3504], Loss: 0.3419\n",
      "Epoch [1/1], Step [1960/3504], Loss: 0.3127\n",
      "Epoch [1/1], Step [1961/3504], Loss: 0.2770\n",
      "Epoch [1/1], Step [1962/3504], Loss: 0.3436\n",
      "Epoch [1/1], Step [1963/3504], Loss: 0.4414\n",
      "Epoch [1/1], Step [1964/3504], Loss: 0.3111\n",
      "Epoch [1/1], Step [1965/3504], Loss: 0.2662\n",
      "Epoch [1/1], Step [1966/3504], Loss: 0.4809\n",
      "Epoch [1/1], Step [1967/3504], Loss: 0.2992\n",
      "Epoch [1/1], Step [1968/3504], Loss: 0.4320\n",
      "Epoch [1/1], Step [1969/3504], Loss: 0.3171\n",
      "Epoch [1/1], Step [1970/3504], Loss: 0.3547\n",
      "Epoch [1/1], Step [1971/3504], Loss: 0.3121\n",
      "Epoch [1/1], Step [1972/3504], Loss: 0.3271\n",
      "Epoch [1/1], Step [1973/3504], Loss: 0.3986\n",
      "Epoch [1/1], Step [1974/3504], Loss: 0.4236\n",
      "Epoch [1/1], Step [1975/3504], Loss: 0.4903\n",
      "Epoch [1/1], Step [1976/3504], Loss: 0.3487\n",
      "Epoch [1/1], Step [1977/3504], Loss: 0.3857\n",
      "Epoch [1/1], Step [1978/3504], Loss: 0.3866\n",
      "Epoch [1/1], Step [1979/3504], Loss: 0.3661\n",
      "Epoch [1/1], Step [1980/3504], Loss: 0.3696\n",
      "Epoch [1/1], Step [1981/3504], Loss: 0.3897\n",
      "Epoch [1/1], Step [1982/3504], Loss: 0.4004\n",
      "Epoch [1/1], Step [1983/3504], Loss: 0.3739\n",
      "Epoch [1/1], Step [1984/3504], Loss: 0.3622\n",
      "Epoch [1/1], Step [1985/3504], Loss: 0.3901\n",
      "Epoch [1/1], Step [1986/3504], Loss: 0.3320\n",
      "Epoch [1/1], Step [1987/3504], Loss: 0.4447\n",
      "Epoch [1/1], Step [1988/3504], Loss: 0.3161\n",
      "Epoch [1/1], Step [1989/3504], Loss: 0.4403\n",
      "Epoch [1/1], Step [1990/3504], Loss: 0.4177\n",
      "Epoch [1/1], Step [1991/3504], Loss: 0.4628\n",
      "Epoch [1/1], Step [1992/3504], Loss: 0.3128\n",
      "Epoch [1/1], Step [1993/3504], Loss: 0.3645\n",
      "Epoch [1/1], Step [1994/3504], Loss: 0.4062\n",
      "Epoch [1/1], Step [1995/3504], Loss: 0.3553\n",
      "Epoch [1/1], Step [1996/3504], Loss: 0.4249\n",
      "Epoch [1/1], Step [1997/3504], Loss: 0.3883\n",
      "Epoch [1/1], Step [1998/3504], Loss: 0.4487\n",
      "Epoch [1/1], Step [1999/3504], Loss: 0.3112\n",
      "Epoch [1/1], Step [2000/3504], Loss: 0.2852\n",
      "Epoch [1/1], Step [2001/3504], Loss: 0.3747\n",
      "Epoch [1/1], Step [2002/3504], Loss: 0.3464\n",
      "Epoch [1/1], Step [2003/3504], Loss: 0.2960\n",
      "Epoch [1/1], Step [2004/3504], Loss: 0.3730\n",
      "Epoch [1/1], Step [2005/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [2006/3504], Loss: 0.3986\n",
      "Epoch [1/1], Step [2007/3504], Loss: 0.3207\n",
      "Epoch [1/1], Step [2008/3504], Loss: 0.3591\n",
      "Epoch [1/1], Step [2009/3504], Loss: 0.3736\n",
      "Epoch [1/1], Step [2010/3504], Loss: 0.4106\n",
      "Epoch [1/1], Step [2011/3504], Loss: 0.5060\n",
      "Epoch [1/1], Step [2012/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [2013/3504], Loss: 0.3837\n",
      "Epoch [1/1], Step [2014/3504], Loss: 0.3981\n",
      "Epoch [1/1], Step [2015/3504], Loss: 0.3888\n",
      "Epoch [1/1], Step [2016/3504], Loss: 0.3464\n",
      "Epoch [1/1], Step [2017/3504], Loss: 0.3472\n",
      "Epoch [1/1], Step [2018/3504], Loss: 0.3583\n",
      "Epoch [1/1], Step [2019/3504], Loss: 0.3308\n",
      "Epoch [1/1], Step [2020/3504], Loss: 0.3956\n",
      "Epoch [1/1], Step [2021/3504], Loss: 0.4889\n",
      "Epoch [1/1], Step [2022/3504], Loss: 0.2582\n",
      "Epoch [1/1], Step [2023/3504], Loss: 0.2910\n",
      "Epoch [1/1], Step [2024/3504], Loss: 0.3311\n",
      "Epoch [1/1], Step [2025/3504], Loss: 0.3927\n",
      "Epoch [1/1], Step [2026/3504], Loss: 0.3627\n",
      "Epoch [1/1], Step [2027/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [2028/3504], Loss: 0.2824\n",
      "Epoch [1/1], Step [2029/3504], Loss: 0.3527\n",
      "Epoch [1/1], Step [2030/3504], Loss: 0.3048\n",
      "Epoch [1/1], Step [2031/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [2032/3504], Loss: 0.4147\n",
      "Epoch [1/1], Step [2033/3504], Loss: 0.3308\n",
      "Epoch [1/1], Step [2034/3504], Loss: 0.3202\n",
      "Epoch [1/1], Step [2035/3504], Loss: 0.3854\n",
      "Epoch [1/1], Step [2036/3504], Loss: 0.3442\n",
      "Epoch [1/1], Step [2037/3504], Loss: 0.3606\n",
      "Epoch [1/1], Step [2038/3504], Loss: 0.3823\n",
      "Epoch [1/1], Step [2039/3504], Loss: 0.4106\n",
      "Epoch [1/1], Step [2040/3504], Loss: 0.3871\n",
      "Epoch [1/1], Step [2041/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [2042/3504], Loss: 0.4260\n",
      "Epoch [1/1], Step [2043/3504], Loss: 0.3555\n",
      "Epoch [1/1], Step [2044/3504], Loss: 0.3583\n",
      "Epoch [1/1], Step [2045/3504], Loss: 0.2870\n",
      "Epoch [1/1], Step [2046/3504], Loss: 0.3829\n",
      "Epoch [1/1], Step [2047/3504], Loss: 0.3602\n",
      "Epoch [1/1], Step [2048/3504], Loss: 0.3602\n",
      "Epoch [1/1], Step [2049/3504], Loss: 0.3288\n",
      "Epoch [1/1], Step [2050/3504], Loss: 0.3285\n",
      "Epoch [1/1], Step [2051/3504], Loss: 0.4456\n",
      "Epoch [1/1], Step [2052/3504], Loss: 0.3486\n",
      "Epoch [1/1], Step [2053/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [2054/3504], Loss: 0.3374\n",
      "Epoch [1/1], Step [2055/3504], Loss: 0.4321\n",
      "Epoch [1/1], Step [2056/3504], Loss: 0.3665\n",
      "Epoch [1/1], Step [2057/3504], Loss: 0.4100\n",
      "Epoch [1/1], Step [2058/3504], Loss: 0.4473\n",
      "Epoch [1/1], Step [2059/3504], Loss: 0.3326\n",
      "Epoch [1/1], Step [2060/3504], Loss: 0.4354\n",
      "Epoch [1/1], Step [2061/3504], Loss: 0.4482\n",
      "Epoch [1/1], Step [2062/3504], Loss: 0.5505\n",
      "Epoch [1/1], Step [2063/3504], Loss: 0.4339\n",
      "Epoch [1/1], Step [2064/3504], Loss: 0.3120\n",
      "Epoch [1/1], Step [2065/3504], Loss: 0.3974\n",
      "Epoch [1/1], Step [2066/3504], Loss: 0.3699\n",
      "Epoch [1/1], Step [2067/3504], Loss: 0.4013\n",
      "Epoch [1/1], Step [2068/3504], Loss: 0.3627\n",
      "Epoch [1/1], Step [2069/3504], Loss: 0.3152\n",
      "Epoch [1/1], Step [2070/3504], Loss: 0.2655\n",
      "Epoch [1/1], Step [2071/3504], Loss: 0.5025\n",
      "Epoch [1/1], Step [2072/3504], Loss: 0.5052\n",
      "Epoch [1/1], Step [2073/3504], Loss: 0.5417\n",
      "Epoch [1/1], Step [2074/3504], Loss: 0.3601\n",
      "Epoch [1/1], Step [2075/3504], Loss: 0.3717\n",
      "Epoch [1/1], Step [2076/3504], Loss: 0.4250\n",
      "Epoch [1/1], Step [2077/3504], Loss: 0.4002\n",
      "Epoch [1/1], Step [2078/3504], Loss: 0.4241\n",
      "Epoch [1/1], Step [2079/3504], Loss: 0.4344\n",
      "Epoch [1/1], Step [2080/3504], Loss: 0.4234\n",
      "Epoch [1/1], Step [2081/3504], Loss: 0.4391\n",
      "Epoch [1/1], Step [2082/3504], Loss: 0.4290\n",
      "Epoch [1/1], Step [2083/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [2084/3504], Loss: 0.3052\n",
      "Epoch [1/1], Step [2085/3504], Loss: 0.4033\n",
      "Epoch [1/1], Step [2086/3504], Loss: 0.4407\n",
      "Epoch [1/1], Step [2087/3504], Loss: 0.3385\n",
      "Epoch [1/1], Step [2088/3504], Loss: 0.3146\n",
      "Epoch [1/1], Step [2089/3504], Loss: 0.4360\n",
      "Epoch [1/1], Step [2090/3504], Loss: 0.4182\n",
      "Epoch [1/1], Step [2091/3504], Loss: 0.3833\n",
      "Epoch [1/1], Step [2092/3504], Loss: 0.3403\n",
      "Epoch [1/1], Step [2093/3504], Loss: 0.3391\n",
      "Epoch [1/1], Step [2094/3504], Loss: 0.3588\n",
      "Epoch [1/1], Step [2095/3504], Loss: 0.4295\n",
      "Epoch [1/1], Step [2096/3504], Loss: 0.2549\n",
      "Epoch [1/1], Step [2097/3504], Loss: 0.4427\n",
      "Epoch [1/1], Step [2098/3504], Loss: 0.5167\n",
      "Epoch [1/1], Step [2099/3504], Loss: 0.3300\n",
      "Epoch [1/1], Step [2100/3504], Loss: 0.4032\n",
      "Epoch [1/1], Step [2101/3504], Loss: 0.3641\n",
      "Epoch [1/1], Step [2102/3504], Loss: 0.4242\n",
      "Epoch [1/1], Step [2103/3504], Loss: 0.3600\n",
      "Epoch [1/1], Step [2104/3504], Loss: 0.3749\n",
      "Epoch [1/1], Step [2105/3504], Loss: 0.3260\n",
      "Epoch [1/1], Step [2106/3504], Loss: 0.4061\n",
      "Epoch [1/1], Step [2107/3504], Loss: 0.3435\n",
      "Epoch [1/1], Step [2108/3504], Loss: 0.3357\n",
      "Epoch [1/1], Step [2109/3504], Loss: 0.3320\n",
      "Epoch [1/1], Step [2110/3504], Loss: 0.3176\n",
      "Epoch [1/1], Step [2111/3504], Loss: 0.3357\n",
      "Epoch [1/1], Step [2112/3504], Loss: 0.4305\n",
      "Epoch [1/1], Step [2113/3504], Loss: 0.2920\n",
      "Epoch [1/1], Step [2114/3504], Loss: 0.3600\n",
      "Epoch [1/1], Step [2115/3504], Loss: 0.4178\n",
      "Epoch [1/1], Step [2116/3504], Loss: 0.3680\n",
      "Epoch [1/1], Step [2117/3504], Loss: 0.3787\n",
      "Epoch [1/1], Step [2118/3504], Loss: 0.3647\n",
      "Epoch [1/1], Step [2119/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [2120/3504], Loss: 0.2715\n",
      "Epoch [1/1], Step [2121/3504], Loss: 0.3469\n",
      "Epoch [1/1], Step [2122/3504], Loss: 0.2879\n",
      "Epoch [1/1], Step [2123/3504], Loss: 0.4068\n",
      "Epoch [1/1], Step [2124/3504], Loss: 0.3177\n",
      "Epoch [1/1], Step [2125/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [2126/3504], Loss: 0.4028\n",
      "Epoch [1/1], Step [2127/3504], Loss: 0.4651\n",
      "Epoch [1/1], Step [2128/3504], Loss: 0.3094\n",
      "Epoch [1/1], Step [2129/3504], Loss: 0.4059\n",
      "Epoch [1/1], Step [2130/3504], Loss: 0.3247\n",
      "Epoch [1/1], Step [2131/3504], Loss: 0.3470\n",
      "Epoch [1/1], Step [2132/3504], Loss: 0.3668\n",
      "Epoch [1/1], Step [2133/3504], Loss: 0.3411\n",
      "Epoch [1/1], Step [2134/3504], Loss: 0.4388\n",
      "Epoch [1/1], Step [2135/3504], Loss: 0.3255\n",
      "Epoch [1/1], Step [2136/3504], Loss: 0.3206\n",
      "Epoch [1/1], Step [2137/3504], Loss: 0.2966\n",
      "Epoch [1/1], Step [2138/3504], Loss: 0.3516\n",
      "Epoch [1/1], Step [2139/3504], Loss: 0.3476\n",
      "Epoch [1/1], Step [2140/3504], Loss: 0.3142\n",
      "Epoch [1/1], Step [2141/3504], Loss: 0.3213\n",
      "Epoch [1/1], Step [2142/3504], Loss: 0.2882\n",
      "Epoch [1/1], Step [2143/3504], Loss: 0.3287\n",
      "Epoch [1/1], Step [2144/3504], Loss: 0.2599\n",
      "Epoch [1/1], Step [2145/3504], Loss: 0.3895\n",
      "Epoch [1/1], Step [2146/3504], Loss: 0.4588\n",
      "Epoch [1/1], Step [2147/3504], Loss: 0.3415\n",
      "Epoch [1/1], Step [2148/3504], Loss: 0.4452\n",
      "Epoch [1/1], Step [2149/3504], Loss: 0.3021\n",
      "Epoch [1/1], Step [2150/3504], Loss: 0.2991\n",
      "Epoch [1/1], Step [2151/3504], Loss: 0.3950\n",
      "Epoch [1/1], Step [2152/3504], Loss: 0.5324\n",
      "Epoch [1/1], Step [2153/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [2154/3504], Loss: 0.3307\n",
      "Epoch [1/1], Step [2155/3504], Loss: 0.3302\n",
      "Epoch [1/1], Step [2156/3504], Loss: 0.3225\n",
      "Epoch [1/1], Step [2157/3504], Loss: 0.4623\n",
      "Epoch [1/1], Step [2158/3504], Loss: 0.2871\n",
      "Epoch [1/1], Step [2159/3504], Loss: 0.3002\n",
      "Epoch [1/1], Step [2160/3504], Loss: 0.3654\n",
      "Epoch [1/1], Step [2161/3504], Loss: 0.3628\n",
      "Epoch [1/1], Step [2162/3504], Loss: 0.3704\n",
      "Epoch [1/1], Step [2163/3504], Loss: 0.4488\n",
      "Epoch [1/1], Step [2164/3504], Loss: 0.3908\n",
      "Epoch [1/1], Step [2165/3504], Loss: 0.4667\n",
      "Epoch [1/1], Step [2166/3504], Loss: 0.3806\n",
      "Epoch [1/1], Step [2167/3504], Loss: 0.4002\n",
      "Epoch [1/1], Step [2168/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [2169/3504], Loss: 0.3859\n",
      "Epoch [1/1], Step [2170/3504], Loss: 0.4493\n",
      "Epoch [1/1], Step [2171/3504], Loss: 0.3957\n",
      "Epoch [1/1], Step [2172/3504], Loss: 0.3266\n",
      "Epoch [1/1], Step [2173/3504], Loss: 0.3761\n",
      "Epoch [1/1], Step [2174/3504], Loss: 0.3118\n",
      "Epoch [1/1], Step [2175/3504], Loss: 0.3143\n",
      "Epoch [1/1], Step [2176/3504], Loss: 0.3076\n",
      "Epoch [1/1], Step [2177/3504], Loss: 0.3465\n",
      "Epoch [1/1], Step [2178/3504], Loss: 0.3197\n",
      "Epoch [1/1], Step [2179/3504], Loss: 0.3445\n",
      "Epoch [1/1], Step [2180/3504], Loss: 0.3827\n",
      "Epoch [1/1], Step [2181/3504], Loss: 0.2912\n",
      "Epoch [1/1], Step [2182/3504], Loss: 0.3096\n",
      "Epoch [1/1], Step [2183/3504], Loss: 0.3968\n",
      "Epoch [1/1], Step [2184/3504], Loss: 0.2168\n",
      "Epoch [1/1], Step [2185/3504], Loss: 0.3569\n",
      "Epoch [1/1], Step [2186/3504], Loss: 0.3627\n",
      "Epoch [1/1], Step [2187/3504], Loss: 0.4502\n",
      "Epoch [1/1], Step [2188/3504], Loss: 0.5457\n",
      "Epoch [1/1], Step [2189/3504], Loss: 0.3280\n",
      "Epoch [1/1], Step [2190/3504], Loss: 0.2966\n",
      "Epoch [1/1], Step [2191/3504], Loss: 0.2574\n",
      "Epoch [1/1], Step [2192/3504], Loss: 0.3400\n",
      "Epoch [1/1], Step [2193/3504], Loss: 0.2725\n",
      "Epoch [1/1], Step [2194/3504], Loss: 0.3351\n",
      "Epoch [1/1], Step [2195/3504], Loss: 0.4433\n",
      "Epoch [1/1], Step [2196/3504], Loss: 0.2553\n",
      "Epoch [1/1], Step [2197/3504], Loss: 0.3218\n",
      "Epoch [1/1], Step [2198/3504], Loss: 0.3906\n",
      "Epoch [1/1], Step [2199/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [2200/3504], Loss: 0.3127\n",
      "Epoch [1/1], Step [2201/3504], Loss: 0.3463\n",
      "Epoch [1/1], Step [2202/3504], Loss: 0.3512\n",
      "Epoch [1/1], Step [2203/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [2204/3504], Loss: 0.4002\n",
      "Epoch [1/1], Step [2205/3504], Loss: 0.4368\n",
      "Epoch [1/1], Step [2206/3504], Loss: 0.3933\n",
      "Epoch [1/1], Step [2207/3504], Loss: 0.3778\n",
      "Epoch [1/1], Step [2208/3504], Loss: 0.3887\n",
      "Epoch [1/1], Step [2209/3504], Loss: 0.3282\n",
      "Epoch [1/1], Step [2210/3504], Loss: 0.3427\n",
      "Epoch [1/1], Step [2211/3504], Loss: 0.3496\n",
      "Epoch [1/1], Step [2212/3504], Loss: 0.3795\n",
      "Epoch [1/1], Step [2213/3504], Loss: 0.3602\n",
      "Epoch [1/1], Step [2214/3504], Loss: 0.3878\n",
      "Epoch [1/1], Step [2215/3504], Loss: 0.3535\n",
      "Epoch [1/1], Step [2216/3504], Loss: 0.3480\n",
      "Epoch [1/1], Step [2217/3504], Loss: 0.4950\n",
      "Epoch [1/1], Step [2218/3504], Loss: 0.3787\n",
      "Epoch [1/1], Step [2219/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [2220/3504], Loss: 0.2916\n",
      "Epoch [1/1], Step [2221/3504], Loss: 0.3706\n",
      "Epoch [1/1], Step [2222/3504], Loss: 0.4058\n",
      "Epoch [1/1], Step [2223/3504], Loss: 0.5351\n",
      "Epoch [1/1], Step [2224/3504], Loss: 0.5240\n",
      "Epoch [1/1], Step [2225/3504], Loss: 0.3375\n",
      "Epoch [1/1], Step [2226/3504], Loss: 0.4073\n",
      "Epoch [1/1], Step [2227/3504], Loss: 0.3407\n",
      "Epoch [1/1], Step [2228/3504], Loss: 0.5053\n",
      "Epoch [1/1], Step [2229/3504], Loss: 0.3905\n",
      "Epoch [1/1], Step [2230/3504], Loss: 0.3009\n",
      "Epoch [1/1], Step [2231/3504], Loss: 0.3443\n",
      "Epoch [1/1], Step [2232/3504], Loss: 0.3010\n",
      "Epoch [1/1], Step [2233/3504], Loss: 0.3771\n",
      "Epoch [1/1], Step [2234/3504], Loss: 0.5207\n",
      "Epoch [1/1], Step [2235/3504], Loss: 0.3372\n",
      "Epoch [1/1], Step [2236/3504], Loss: 0.4156\n",
      "Epoch [1/1], Step [2237/3504], Loss: 0.3723\n",
      "Epoch [1/1], Step [2238/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [2239/3504], Loss: 0.4331\n",
      "Epoch [1/1], Step [2240/3504], Loss: 0.4063\n",
      "Epoch [1/1], Step [2241/3504], Loss: 0.3650\n",
      "Epoch [1/1], Step [2242/3504], Loss: 0.3768\n",
      "Epoch [1/1], Step [2243/3504], Loss: 0.3244\n",
      "Epoch [1/1], Step [2244/3504], Loss: 0.2787\n",
      "Epoch [1/1], Step [2245/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [2246/3504], Loss: 0.5319\n",
      "Epoch [1/1], Step [2247/3504], Loss: 0.4389\n",
      "Epoch [1/1], Step [2248/3504], Loss: 0.3343\n",
      "Epoch [1/1], Step [2249/3504], Loss: 0.3674\n",
      "Epoch [1/1], Step [2250/3504], Loss: 0.5437\n",
      "Epoch [1/1], Step [2251/3504], Loss: 0.4019\n",
      "Epoch [1/1], Step [2252/3504], Loss: 0.4357\n",
      "Epoch [1/1], Step [2253/3504], Loss: 0.3122\n",
      "Epoch [1/1], Step [2254/3504], Loss: 0.3983\n",
      "Epoch [1/1], Step [2255/3504], Loss: 0.3782\n",
      "Epoch [1/1], Step [2256/3504], Loss: 0.2920\n",
      "Epoch [1/1], Step [2257/3504], Loss: 0.3528\n",
      "Epoch [1/1], Step [2258/3504], Loss: 0.4480\n",
      "Epoch [1/1], Step [2259/3504], Loss: 0.3254\n",
      "Epoch [1/1], Step [2260/3504], Loss: 0.3338\n",
      "Epoch [1/1], Step [2261/3504], Loss: 0.3891\n",
      "Epoch [1/1], Step [2262/3504], Loss: 0.3482\n",
      "Epoch [1/1], Step [2263/3504], Loss: 0.3446\n",
      "Epoch [1/1], Step [2264/3504], Loss: 0.3961\n",
      "Epoch [1/1], Step [2265/3504], Loss: 0.3622\n",
      "Epoch [1/1], Step [2266/3504], Loss: 0.3534\n",
      "Epoch [1/1], Step [2267/3504], Loss: 0.3543\n",
      "Epoch [1/1], Step [2268/3504], Loss: 0.3140\n",
      "Epoch [1/1], Step [2269/3504], Loss: 0.3118\n",
      "Epoch [1/1], Step [2270/3504], Loss: 0.4135\n",
      "Epoch [1/1], Step [2271/3504], Loss: 0.3294\n",
      "Epoch [1/1], Step [2272/3504], Loss: 0.4095\n",
      "Epoch [1/1], Step [2273/3504], Loss: 0.3489\n",
      "Epoch [1/1], Step [2274/3504], Loss: 0.3785\n",
      "Epoch [1/1], Step [2275/3504], Loss: 0.3567\n",
      "Epoch [1/1], Step [2276/3504], Loss: 0.4037\n",
      "Epoch [1/1], Step [2277/3504], Loss: 0.2983\n",
      "Epoch [1/1], Step [2278/3504], Loss: 0.3011\n",
      "Epoch [1/1], Step [2279/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [2280/3504], Loss: 0.4677\n",
      "Epoch [1/1], Step [2281/3504], Loss: 0.3418\n",
      "Epoch [1/1], Step [2282/3504], Loss: 0.3408\n",
      "Epoch [1/1], Step [2283/3504], Loss: 0.3041\n",
      "Epoch [1/1], Step [2284/3504], Loss: 0.4805\n",
      "Epoch [1/1], Step [2285/3504], Loss: 0.4910\n",
      "Epoch [1/1], Step [2286/3504], Loss: 0.3030\n",
      "Epoch [1/1], Step [2287/3504], Loss: 0.4037\n",
      "Epoch [1/1], Step [2288/3504], Loss: 0.3033\n",
      "Epoch [1/1], Step [2289/3504], Loss: 0.4823\n",
      "Epoch [1/1], Step [2290/3504], Loss: 0.4003\n",
      "Epoch [1/1], Step [2291/3504], Loss: 0.3163\n",
      "Epoch [1/1], Step [2292/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [2293/3504], Loss: 0.3549\n",
      "Epoch [1/1], Step [2294/3504], Loss: 0.3478\n",
      "Epoch [1/1], Step [2295/3504], Loss: 0.3348\n",
      "Epoch [1/1], Step [2296/3504], Loss: 0.3307\n",
      "Epoch [1/1], Step [2297/3504], Loss: 0.2805\n",
      "Epoch [1/1], Step [2298/3504], Loss: 0.3362\n",
      "Epoch [1/1], Step [2299/3504], Loss: 0.4362\n",
      "Epoch [1/1], Step [2300/3504], Loss: 0.3989\n",
      "Epoch [1/1], Step [2301/3504], Loss: 0.3346\n",
      "Epoch [1/1], Step [2302/3504], Loss: 0.4443\n",
      "Epoch [1/1], Step [2303/3504], Loss: 0.4060\n",
      "Epoch [1/1], Step [2304/3504], Loss: 0.3805\n",
      "Epoch [1/1], Step [2305/3504], Loss: 0.3550\n",
      "Epoch [1/1], Step [2306/3504], Loss: 0.3490\n",
      "Epoch [1/1], Step [2307/3504], Loss: 0.4480\n",
      "Epoch [1/1], Step [2308/3504], Loss: 0.4105\n",
      "Epoch [1/1], Step [2309/3504], Loss: 0.3533\n",
      "Epoch [1/1], Step [2310/3504], Loss: 0.3365\n",
      "Epoch [1/1], Step [2311/3504], Loss: 0.2970\n",
      "Epoch [1/1], Step [2312/3504], Loss: 0.4414\n",
      "Epoch [1/1], Step [2313/3504], Loss: 0.3334\n",
      "Epoch [1/1], Step [2314/3504], Loss: 0.4576\n",
      "Epoch [1/1], Step [2315/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [2316/3504], Loss: 0.4166\n",
      "Epoch [1/1], Step [2317/3504], Loss: 0.3448\n",
      "Epoch [1/1], Step [2318/3504], Loss: 0.3055\n",
      "Epoch [1/1], Step [2319/3504], Loss: 0.3781\n",
      "Epoch [1/1], Step [2320/3504], Loss: 0.3875\n",
      "Epoch [1/1], Step [2321/3504], Loss: 0.3791\n",
      "Epoch [1/1], Step [2322/3504], Loss: 0.3400\n",
      "Epoch [1/1], Step [2323/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [2324/3504], Loss: 0.3543\n",
      "Epoch [1/1], Step [2325/3504], Loss: 0.3134\n",
      "Epoch [1/1], Step [2326/3504], Loss: 0.2806\n",
      "Epoch [1/1], Step [2327/3504], Loss: 0.3591\n",
      "Epoch [1/1], Step [2328/3504], Loss: 0.4217\n",
      "Epoch [1/1], Step [2329/3504], Loss: 0.2829\n",
      "Epoch [1/1], Step [2330/3504], Loss: 0.3035\n",
      "Epoch [1/1], Step [2331/3504], Loss: 0.3661\n",
      "Epoch [1/1], Step [2332/3504], Loss: 0.3132\n",
      "Epoch [1/1], Step [2333/3504], Loss: 0.3153\n",
      "Epoch [1/1], Step [2334/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [2335/3504], Loss: 0.3243\n",
      "Epoch [1/1], Step [2336/3504], Loss: 0.3219\n",
      "Epoch [1/1], Step [2337/3504], Loss: 0.4214\n",
      "Epoch [1/1], Step [2338/3504], Loss: 0.3582\n",
      "Epoch [1/1], Step [2339/3504], Loss: 0.3629\n",
      "Epoch [1/1], Step [2340/3504], Loss: 0.5030\n",
      "Epoch [1/1], Step [2341/3504], Loss: 0.4329\n",
      "Epoch [1/1], Step [2342/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [2343/3504], Loss: 0.3535\n",
      "Epoch [1/1], Step [2344/3504], Loss: 0.3394\n",
      "Epoch [1/1], Step [2345/3504], Loss: 0.4020\n",
      "Epoch [1/1], Step [2346/3504], Loss: 0.3919\n",
      "Epoch [1/1], Step [2347/3504], Loss: 0.4223\n",
      "Epoch [1/1], Step [2348/3504], Loss: 0.3290\n",
      "Epoch [1/1], Step [2349/3504], Loss: 0.3965\n",
      "Epoch [1/1], Step [2350/3504], Loss: 0.2666\n",
      "Epoch [1/1], Step [2351/3504], Loss: 0.3403\n",
      "Epoch [1/1], Step [2352/3504], Loss: 0.3874\n",
      "Epoch [1/1], Step [2353/3504], Loss: 0.3318\n",
      "Epoch [1/1], Step [2354/3504], Loss: 0.2781\n",
      "Epoch [1/1], Step [2355/3504], Loss: 0.2974\n",
      "Epoch [1/1], Step [2356/3504], Loss: 0.3880\n",
      "Epoch [1/1], Step [2357/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [2358/3504], Loss: 0.2835\n",
      "Epoch [1/1], Step [2359/3504], Loss: 0.4430\n",
      "Epoch [1/1], Step [2360/3504], Loss: 0.3969\n",
      "Epoch [1/1], Step [2361/3504], Loss: 0.3598\n",
      "Epoch [1/1], Step [2362/3504], Loss: 0.3081\n",
      "Epoch [1/1], Step [2363/3504], Loss: 0.3096\n",
      "Epoch [1/1], Step [2364/3504], Loss: 0.4964\n",
      "Epoch [1/1], Step [2365/3504], Loss: 0.3331\n",
      "Epoch [1/1], Step [2366/3504], Loss: 0.4468\n",
      "Epoch [1/1], Step [2367/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [2368/3504], Loss: 0.3781\n",
      "Epoch [1/1], Step [2369/3504], Loss: 0.3959\n",
      "Epoch [1/1], Step [2370/3504], Loss: 0.3288\n",
      "Epoch [1/1], Step [2371/3504], Loss: 0.4198\n",
      "Epoch [1/1], Step [2372/3504], Loss: 0.2832\n",
      "Epoch [1/1], Step [2373/3504], Loss: 0.3159\n",
      "Epoch [1/1], Step [2374/3504], Loss: 0.3682\n",
      "Epoch [1/1], Step [2375/3504], Loss: 0.3222\n",
      "Epoch [1/1], Step [2376/3504], Loss: 0.4312\n",
      "Epoch [1/1], Step [2377/3504], Loss: 0.4406\n",
      "Epoch [1/1], Step [2378/3504], Loss: 0.4126\n",
      "Epoch [1/1], Step [2379/3504], Loss: 0.3253\n",
      "Epoch [1/1], Step [2380/3504], Loss: 0.3685\n",
      "Epoch [1/1], Step [2381/3504], Loss: 0.3680\n",
      "Epoch [1/1], Step [2382/3504], Loss: 0.3580\n",
      "Epoch [1/1], Step [2383/3504], Loss: 0.4160\n",
      "Epoch [1/1], Step [2384/3504], Loss: 0.4227\n",
      "Epoch [1/1], Step [2385/3504], Loss: 0.3461\n",
      "Epoch [1/1], Step [2386/3504], Loss: 0.4227\n",
      "Epoch [1/1], Step [2387/3504], Loss: 0.3959\n",
      "Epoch [1/1], Step [2388/3504], Loss: 0.3666\n",
      "Epoch [1/1], Step [2389/3504], Loss: 0.3223\n",
      "Epoch [1/1], Step [2390/3504], Loss: 0.3128\n",
      "Epoch [1/1], Step [2391/3504], Loss: 0.3578\n",
      "Epoch [1/1], Step [2392/3504], Loss: 0.2699\n",
      "Epoch [1/1], Step [2393/3504], Loss: 0.3077\n",
      "Epoch [1/1], Step [2394/3504], Loss: 0.3013\n",
      "Epoch [1/1], Step [2395/3504], Loss: 0.3601\n",
      "Epoch [1/1], Step [2396/3504], Loss: 0.2937\n",
      "Epoch [1/1], Step [2397/3504], Loss: 0.3304\n",
      "Epoch [1/1], Step [2398/3504], Loss: 0.3549\n",
      "Epoch [1/1], Step [2399/3504], Loss: 0.3276\n",
      "Epoch [1/1], Step [2400/3504], Loss: 0.3352\n",
      "Epoch [1/1], Step [2401/3504], Loss: 0.2533\n",
      "Epoch [1/1], Step [2402/3504], Loss: 0.4303\n",
      "Epoch [1/1], Step [2403/3504], Loss: 0.2594\n",
      "Epoch [1/1], Step [2404/3504], Loss: 0.3272\n",
      "Epoch [1/1], Step [2405/3504], Loss: 0.4194\n",
      "Epoch [1/1], Step [2406/3504], Loss: 0.4318\n",
      "Epoch [1/1], Step [2407/3504], Loss: 0.4068\n",
      "Epoch [1/1], Step [2408/3504], Loss: 0.4036\n",
      "Epoch [1/1], Step [2409/3504], Loss: 0.3618\n",
      "Epoch [1/1], Step [2410/3504], Loss: 0.3385\n",
      "Epoch [1/1], Step [2411/3504], Loss: 0.3654\n",
      "Epoch [1/1], Step [2412/3504], Loss: 0.3274\n",
      "Epoch [1/1], Step [2413/3504], Loss: 0.3526\n",
      "Epoch [1/1], Step [2414/3504], Loss: 0.4111\n",
      "Epoch [1/1], Step [2415/3504], Loss: 0.2852\n",
      "Epoch [1/1], Step [2416/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [2417/3504], Loss: 0.3007\n",
      "Epoch [1/1], Step [2418/3504], Loss: 0.3789\n",
      "Epoch [1/1], Step [2419/3504], Loss: 0.3525\n",
      "Epoch [1/1], Step [2420/3504], Loss: 0.3322\n",
      "Epoch [1/1], Step [2421/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [2422/3504], Loss: 0.3394\n",
      "Epoch [1/1], Step [2423/3504], Loss: 0.3744\n",
      "Epoch [1/1], Step [2424/3504], Loss: 0.3387\n",
      "Epoch [1/1], Step [2425/3504], Loss: 0.4201\n",
      "Epoch [1/1], Step [2426/3504], Loss: 0.3669\n",
      "Epoch [1/1], Step [2427/3504], Loss: 0.3465\n",
      "Epoch [1/1], Step [2428/3504], Loss: 0.3968\n",
      "Epoch [1/1], Step [2429/3504], Loss: 0.4093\n",
      "Epoch [1/1], Step [2430/3504], Loss: 0.2990\n",
      "Epoch [1/1], Step [2431/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [2432/3504], Loss: 0.3461\n",
      "Epoch [1/1], Step [2433/3504], Loss: 0.3931\n",
      "Epoch [1/1], Step [2434/3504], Loss: 0.3238\n",
      "Epoch [1/1], Step [2435/3504], Loss: 0.3262\n",
      "Epoch [1/1], Step [2436/3504], Loss: 0.4455\n",
      "Epoch [1/1], Step [2437/3504], Loss: 0.3481\n",
      "Epoch [1/1], Step [2438/3504], Loss: 0.3455\n",
      "Epoch [1/1], Step [2439/3504], Loss: 0.3318\n",
      "Epoch [1/1], Step [2440/3504], Loss: 0.3902\n",
      "Epoch [1/1], Step [2441/3504], Loss: 0.3366\n",
      "Epoch [1/1], Step [2442/3504], Loss: 0.3118\n",
      "Epoch [1/1], Step [2443/3504], Loss: 0.4502\n",
      "Epoch [1/1], Step [2444/3504], Loss: 0.2949\n",
      "Epoch [1/1], Step [2445/3504], Loss: 0.3932\n",
      "Epoch [1/1], Step [2446/3504], Loss: 0.3750\n",
      "Epoch [1/1], Step [2447/3504], Loss: 0.2979\n",
      "Epoch [1/1], Step [2448/3504], Loss: 0.3361\n",
      "Epoch [1/1], Step [2449/3504], Loss: 0.3587\n",
      "Epoch [1/1], Step [2450/3504], Loss: 0.2892\n",
      "Epoch [1/1], Step [2451/3504], Loss: 0.3043\n",
      "Epoch [1/1], Step [2452/3504], Loss: 0.3702\n",
      "Epoch [1/1], Step [2453/3504], Loss: 0.4327\n",
      "Epoch [1/1], Step [2454/3504], Loss: 0.4212\n",
      "Epoch [1/1], Step [2455/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [2456/3504], Loss: 0.3701\n",
      "Epoch [1/1], Step [2457/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [2458/3504], Loss: 0.3646\n",
      "Epoch [1/1], Step [2459/3504], Loss: 0.4110\n",
      "Epoch [1/1], Step [2460/3504], Loss: 0.2943\n",
      "Epoch [1/1], Step [2461/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [2462/3504], Loss: 0.4054\n",
      "Epoch [1/1], Step [2463/3504], Loss: 0.2823\n",
      "Epoch [1/1], Step [2464/3504], Loss: 0.4101\n",
      "Epoch [1/1], Step [2465/3504], Loss: 0.3154\n",
      "Epoch [1/1], Step [2466/3504], Loss: 0.3542\n",
      "Epoch [1/1], Step [2467/3504], Loss: 0.3856\n",
      "Epoch [1/1], Step [2468/3504], Loss: 0.3388\n",
      "Epoch [1/1], Step [2469/3504], Loss: 0.3405\n",
      "Epoch [1/1], Step [2470/3504], Loss: 0.3190\n",
      "Epoch [1/1], Step [2471/3504], Loss: 0.5301\n",
      "Epoch [1/1], Step [2472/3504], Loss: 0.3483\n",
      "Epoch [1/1], Step [2473/3504], Loss: 0.3353\n",
      "Epoch [1/1], Step [2474/3504], Loss: 0.3583\n",
      "Epoch [1/1], Step [2475/3504], Loss: 0.4255\n",
      "Epoch [1/1], Step [2476/3504], Loss: 0.3870\n",
      "Epoch [1/1], Step [2477/3504], Loss: 0.3783\n",
      "Epoch [1/1], Step [2478/3504], Loss: 0.3352\n",
      "Epoch [1/1], Step [2479/3504], Loss: 0.2913\n",
      "Epoch [1/1], Step [2480/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [2481/3504], Loss: 0.4120\n",
      "Epoch [1/1], Step [2482/3504], Loss: 0.3574\n",
      "Epoch [1/1], Step [2483/3504], Loss: 0.3954\n",
      "Epoch [1/1], Step [2484/3504], Loss: 0.3447\n",
      "Epoch [1/1], Step [2485/3504], Loss: 0.2930\n",
      "Epoch [1/1], Step [2486/3504], Loss: 0.3003\n",
      "Epoch [1/1], Step [2487/3504], Loss: 0.4622\n",
      "Epoch [1/1], Step [2488/3504], Loss: 0.3558\n",
      "Epoch [1/1], Step [2489/3504], Loss: 0.3288\n",
      "Epoch [1/1], Step [2490/3504], Loss: 0.2545\n",
      "Epoch [1/1], Step [2491/3504], Loss: 0.2748\n",
      "Epoch [1/1], Step [2492/3504], Loss: 0.2381\n",
      "Epoch [1/1], Step [2493/3504], Loss: 0.3225\n",
      "Epoch [1/1], Step [2494/3504], Loss: 0.3513\n",
      "Epoch [1/1], Step [2495/3504], Loss: 0.3572\n",
      "Epoch [1/1], Step [2496/3504], Loss: 0.3299\n",
      "Epoch [1/1], Step [2497/3504], Loss: 0.4845\n",
      "Epoch [1/1], Step [2498/3504], Loss: 0.4705\n",
      "Epoch [1/1], Step [2499/3504], Loss: 0.2867\n",
      "Epoch [1/1], Step [2500/3504], Loss: 0.3414\n",
      "Epoch [1/1], Step [2501/3504], Loss: 0.3461\n",
      "Epoch [1/1], Step [2502/3504], Loss: 0.2860\n",
      "Epoch [1/1], Step [2503/3504], Loss: 0.2914\n",
      "Epoch [1/1], Step [2504/3504], Loss: 0.2427\n",
      "Epoch [1/1], Step [2505/3504], Loss: 0.3535\n",
      "Epoch [1/1], Step [2506/3504], Loss: 0.2820\n",
      "Epoch [1/1], Step [2507/3504], Loss: 0.3866\n",
      "Epoch [1/1], Step [2508/3504], Loss: 0.2793\n",
      "Epoch [1/1], Step [2509/3504], Loss: 0.3239\n",
      "Epoch [1/1], Step [2510/3504], Loss: 0.3170\n",
      "Epoch [1/1], Step [2511/3504], Loss: 0.4042\n",
      "Epoch [1/1], Step [2512/3504], Loss: 0.3719\n",
      "Epoch [1/1], Step [2513/3504], Loss: 0.4250\n",
      "Epoch [1/1], Step [2514/3504], Loss: 0.3731\n",
      "Epoch [1/1], Step [2515/3504], Loss: 0.4124\n",
      "Epoch [1/1], Step [2516/3504], Loss: 0.4324\n",
      "Epoch [1/1], Step [2517/3504], Loss: 0.4369\n",
      "Epoch [1/1], Step [2518/3504], Loss: 0.3296\n",
      "Epoch [1/1], Step [2519/3504], Loss: 0.3146\n",
      "Epoch [1/1], Step [2520/3504], Loss: 0.4102\n",
      "Epoch [1/1], Step [2521/3504], Loss: 0.3207\n",
      "Epoch [1/1], Step [2522/3504], Loss: 0.4592\n",
      "Epoch [1/1], Step [2523/3504], Loss: 0.3813\n",
      "Epoch [1/1], Step [2524/3504], Loss: 0.3636\n",
      "Epoch [1/1], Step [2525/3504], Loss: 0.3090\n",
      "Epoch [1/1], Step [2526/3504], Loss: 0.3276\n",
      "Epoch [1/1], Step [2527/3504], Loss: 0.5579\n",
      "Epoch [1/1], Step [2528/3504], Loss: 0.3715\n",
      "Epoch [1/1], Step [2529/3504], Loss: 0.4370\n",
      "Epoch [1/1], Step [2530/3504], Loss: 0.2789\n",
      "Epoch [1/1], Step [2531/3504], Loss: 0.4216\n",
      "Epoch [1/1], Step [2532/3504], Loss: 0.3788\n",
      "Epoch [1/1], Step [2533/3504], Loss: 0.3318\n",
      "Epoch [1/1], Step [2534/3504], Loss: 0.3872\n",
      "Epoch [1/1], Step [2535/3504], Loss: 0.3990\n",
      "Epoch [1/1], Step [2536/3504], Loss: 0.3548\n",
      "Epoch [1/1], Step [2537/3504], Loss: 0.4247\n",
      "Epoch [1/1], Step [2538/3504], Loss: 0.3667\n",
      "Epoch [1/1], Step [2539/3504], Loss: 0.3582\n",
      "Epoch [1/1], Step [2540/3504], Loss: 0.3889\n",
      "Epoch [1/1], Step [2541/3504], Loss: 0.3585\n",
      "Epoch [1/1], Step [2542/3504], Loss: 0.2949\n",
      "Epoch [1/1], Step [2543/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [2544/3504], Loss: 0.4351\n",
      "Epoch [1/1], Step [2545/3504], Loss: 0.3600\n",
      "Epoch [1/1], Step [2546/3504], Loss: 0.4066\n",
      "Epoch [1/1], Step [2547/3504], Loss: 0.3117\n",
      "Epoch [1/1], Step [2548/3504], Loss: 0.3798\n",
      "Epoch [1/1], Step [2549/3504], Loss: 0.3676\n",
      "Epoch [1/1], Step [2550/3504], Loss: 0.3129\n",
      "Epoch [1/1], Step [2551/3504], Loss: 0.2768\n",
      "Epoch [1/1], Step [2552/3504], Loss: 0.3424\n",
      "Epoch [1/1], Step [2553/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [2554/3504], Loss: 0.2905\n",
      "Epoch [1/1], Step [2555/3504], Loss: 0.3552\n",
      "Epoch [1/1], Step [2556/3504], Loss: 0.3671\n",
      "Epoch [1/1], Step [2557/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [2558/3504], Loss: 0.3057\n",
      "Epoch [1/1], Step [2559/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [2560/3504], Loss: 0.3295\n",
      "Epoch [1/1], Step [2561/3504], Loss: 0.2916\n",
      "Epoch [1/1], Step [2562/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [2563/3504], Loss: 0.3609\n",
      "Epoch [1/1], Step [2564/3504], Loss: 0.3272\n",
      "Epoch [1/1], Step [2565/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [2566/3504], Loss: 0.4162\n",
      "Epoch [1/1], Step [2567/3504], Loss: 0.3465\n",
      "Epoch [1/1], Step [2568/3504], Loss: 0.2884\n",
      "Epoch [1/1], Step [2569/3504], Loss: 0.3528\n",
      "Epoch [1/1], Step [2570/3504], Loss: 0.3167\n",
      "Epoch [1/1], Step [2571/3504], Loss: 0.2542\n",
      "Epoch [1/1], Step [2572/3504], Loss: 0.4036\n",
      "Epoch [1/1], Step [2573/3504], Loss: 0.3631\n",
      "Epoch [1/1], Step [2574/3504], Loss: 0.4324\n",
      "Epoch [1/1], Step [2575/3504], Loss: 0.4241\n",
      "Epoch [1/1], Step [2576/3504], Loss: 0.3383\n",
      "Epoch [1/1], Step [2577/3504], Loss: 0.3976\n",
      "Epoch [1/1], Step [2578/3504], Loss: 0.4304\n",
      "Epoch [1/1], Step [2579/3504], Loss: 0.4601\n",
      "Epoch [1/1], Step [2580/3504], Loss: 0.3076\n",
      "Epoch [1/1], Step [2581/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [2582/3504], Loss: 0.4194\n",
      "Epoch [1/1], Step [2583/3504], Loss: 0.4307\n",
      "Epoch [1/1], Step [2584/3504], Loss: 0.3280\n",
      "Epoch [1/1], Step [2585/3504], Loss: 0.4332\n",
      "Epoch [1/1], Step [2586/3504], Loss: 0.3506\n",
      "Epoch [1/1], Step [2587/3504], Loss: 0.3336\n",
      "Epoch [1/1], Step [2588/3504], Loss: 0.4802\n",
      "Epoch [1/1], Step [2589/3504], Loss: 0.3364\n",
      "Epoch [1/1], Step [2590/3504], Loss: 0.4300\n",
      "Epoch [1/1], Step [2591/3504], Loss: 0.3870\n",
      "Epoch [1/1], Step [2592/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [2593/3504], Loss: 0.3323\n",
      "Epoch [1/1], Step [2594/3504], Loss: 0.3730\n",
      "Epoch [1/1], Step [2595/3504], Loss: 0.3140\n",
      "Epoch [1/1], Step [2596/3504], Loss: 0.3938\n",
      "Epoch [1/1], Step [2597/3504], Loss: 0.3961\n",
      "Epoch [1/1], Step [2598/3504], Loss: 0.3147\n",
      "Epoch [1/1], Step [2599/3504], Loss: 0.3705\n",
      "Epoch [1/1], Step [2600/3504], Loss: 0.3801\n",
      "Epoch [1/1], Step [2601/3504], Loss: 0.4554\n",
      "Epoch [1/1], Step [2602/3504], Loss: 0.4444\n",
      "Epoch [1/1], Step [2603/3504], Loss: 0.2984\n",
      "Epoch [1/1], Step [2604/3504], Loss: 0.3330\n",
      "Epoch [1/1], Step [2605/3504], Loss: 0.3294\n",
      "Epoch [1/1], Step [2606/3504], Loss: 0.3406\n",
      "Epoch [1/1], Step [2607/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [2608/3504], Loss: 0.4590\n",
      "Epoch [1/1], Step [2609/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [2610/3504], Loss: 0.2922\n",
      "Epoch [1/1], Step [2611/3504], Loss: 0.3364\n",
      "Epoch [1/1], Step [2612/3504], Loss: 0.3172\n",
      "Epoch [1/1], Step [2613/3504], Loss: 0.3577\n",
      "Epoch [1/1], Step [2614/3504], Loss: 0.3309\n",
      "Epoch [1/1], Step [2615/3504], Loss: 0.3321\n",
      "Epoch [1/1], Step [2616/3504], Loss: 0.3737\n",
      "Epoch [1/1], Step [2617/3504], Loss: 0.4028\n",
      "Epoch [1/1], Step [2618/3504], Loss: 0.3819\n",
      "Epoch [1/1], Step [2619/3504], Loss: 0.3109\n",
      "Epoch [1/1], Step [2620/3504], Loss: 0.3569\n",
      "Epoch [1/1], Step [2621/3504], Loss: 0.3766\n",
      "Epoch [1/1], Step [2622/3504], Loss: 0.3395\n",
      "Epoch [1/1], Step [2623/3504], Loss: 0.3655\n",
      "Epoch [1/1], Step [2624/3504], Loss: 0.3571\n",
      "Epoch [1/1], Step [2625/3504], Loss: 0.3913\n",
      "Epoch [1/1], Step [2626/3504], Loss: 0.2711\n",
      "Epoch [1/1], Step [2627/3504], Loss: 0.3888\n",
      "Epoch [1/1], Step [2628/3504], Loss: 0.3430\n",
      "Epoch [1/1], Step [2629/3504], Loss: 0.3025\n",
      "Epoch [1/1], Step [2630/3504], Loss: 0.3272\n",
      "Epoch [1/1], Step [2631/3504], Loss: 0.3312\n",
      "Epoch [1/1], Step [2632/3504], Loss: 0.3148\n",
      "Epoch [1/1], Step [2633/3504], Loss: 0.3529\n",
      "Epoch [1/1], Step [2634/3504], Loss: 0.3575\n",
      "Epoch [1/1], Step [2635/3504], Loss: 0.2982\n",
      "Epoch [1/1], Step [2636/3504], Loss: 0.3767\n",
      "Epoch [1/1], Step [2637/3504], Loss: 0.3692\n",
      "Epoch [1/1], Step [2638/3504], Loss: 0.2968\n",
      "Epoch [1/1], Step [2639/3504], Loss: 0.2221\n",
      "Epoch [1/1], Step [2640/3504], Loss: 0.3709\n",
      "Epoch [1/1], Step [2641/3504], Loss: 0.3264\n",
      "Epoch [1/1], Step [2642/3504], Loss: 0.3389\n",
      "Epoch [1/1], Step [2643/3504], Loss: 0.3711\n",
      "Epoch [1/1], Step [2644/3504], Loss: 0.3111\n",
      "Epoch [1/1], Step [2645/3504], Loss: 0.3649\n",
      "Epoch [1/1], Step [2646/3504], Loss: 0.4320\n",
      "Epoch [1/1], Step [2647/3504], Loss: 0.3151\n",
      "Epoch [1/1], Step [2648/3504], Loss: 0.3751\n",
      "Epoch [1/1], Step [2649/3504], Loss: 0.4461\n",
      "Epoch [1/1], Step [2650/3504], Loss: 0.4685\n",
      "Epoch [1/1], Step [2651/3504], Loss: 0.3239\n",
      "Epoch [1/1], Step [2652/3504], Loss: 0.3164\n",
      "Epoch [1/1], Step [2653/3504], Loss: 0.3152\n",
      "Epoch [1/1], Step [2654/3504], Loss: 0.4108\n",
      "Epoch [1/1], Step [2655/3504], Loss: 0.4105\n",
      "Epoch [1/1], Step [2656/3504], Loss: 0.4084\n",
      "Epoch [1/1], Step [2657/3504], Loss: 0.4078\n",
      "Epoch [1/1], Step [2658/3504], Loss: 0.2703\n",
      "Epoch [1/1], Step [2659/3504], Loss: 0.3763\n",
      "Epoch [1/1], Step [2660/3504], Loss: 0.3638\n",
      "Epoch [1/1], Step [2661/3504], Loss: 0.4452\n",
      "Epoch [1/1], Step [2662/3504], Loss: 0.3214\n",
      "Epoch [1/1], Step [2663/3504], Loss: 0.2837\n",
      "Epoch [1/1], Step [2664/3504], Loss: 0.3799\n",
      "Epoch [1/1], Step [2665/3504], Loss: 0.4152\n",
      "Epoch [1/1], Step [2666/3504], Loss: 0.3323\n",
      "Epoch [1/1], Step [2667/3504], Loss: 0.2856\n",
      "Epoch [1/1], Step [2668/3504], Loss: 0.3339\n",
      "Epoch [1/1], Step [2669/3504], Loss: 0.2930\n",
      "Epoch [1/1], Step [2670/3504], Loss: 0.3630\n",
      "Epoch [1/1], Step [2671/3504], Loss: 0.3507\n",
      "Epoch [1/1], Step [2672/3504], Loss: 0.4041\n",
      "Epoch [1/1], Step [2673/3504], Loss: 0.3644\n",
      "Epoch [1/1], Step [2674/3504], Loss: 0.4177\n",
      "Epoch [1/1], Step [2675/3504], Loss: 0.3018\n",
      "Epoch [1/1], Step [2676/3504], Loss: 0.3859\n",
      "Epoch [1/1], Step [2677/3504], Loss: 0.3453\n",
      "Epoch [1/1], Step [2678/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [2679/3504], Loss: 0.3698\n",
      "Epoch [1/1], Step [2680/3504], Loss: 0.3253\n",
      "Epoch [1/1], Step [2681/3504], Loss: 0.2146\n",
      "Epoch [1/1], Step [2682/3504], Loss: 0.4102\n",
      "Epoch [1/1], Step [2683/3504], Loss: 0.3264\n",
      "Epoch [1/1], Step [2684/3504], Loss: 0.3190\n",
      "Epoch [1/1], Step [2685/3504], Loss: 0.3264\n",
      "Epoch [1/1], Step [2686/3504], Loss: 0.3239\n",
      "Epoch [1/1], Step [2687/3504], Loss: 0.3372\n",
      "Epoch [1/1], Step [2688/3504], Loss: 0.4003\n",
      "Epoch [1/1], Step [2689/3504], Loss: 0.3554\n",
      "Epoch [1/1], Step [2690/3504], Loss: 0.3619\n",
      "Epoch [1/1], Step [2691/3504], Loss: 0.3136\n",
      "Epoch [1/1], Step [2692/3504], Loss: 0.3834\n",
      "Epoch [1/1], Step [2693/3504], Loss: 0.3867\n",
      "Epoch [1/1], Step [2694/3504], Loss: 0.3006\n",
      "Epoch [1/1], Step [2695/3504], Loss: 0.3453\n",
      "Epoch [1/1], Step [2696/3504], Loss: 0.3273\n",
      "Epoch [1/1], Step [2697/3504], Loss: 0.2668\n",
      "Epoch [1/1], Step [2698/3504], Loss: 0.3365\n",
      "Epoch [1/1], Step [2699/3504], Loss: 0.3140\n",
      "Epoch [1/1], Step [2700/3504], Loss: 0.4287\n",
      "Epoch [1/1], Step [2701/3504], Loss: 0.4748\n",
      "Epoch [1/1], Step [2702/3504], Loss: 0.2809\n",
      "Epoch [1/1], Step [2703/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [2704/3504], Loss: 0.3271\n",
      "Epoch [1/1], Step [2705/3504], Loss: 0.4295\n",
      "Epoch [1/1], Step [2706/3504], Loss: 0.2800\n",
      "Epoch [1/1], Step [2707/3504], Loss: 0.4253\n",
      "Epoch [1/1], Step [2708/3504], Loss: 0.3177\n",
      "Epoch [1/1], Step [2709/3504], Loss: 0.3015\n",
      "Epoch [1/1], Step [2710/3504], Loss: 0.4667\n",
      "Epoch [1/1], Step [2711/3504], Loss: 0.3919\n",
      "Epoch [1/1], Step [2712/3504], Loss: 0.4119\n",
      "Epoch [1/1], Step [2713/3504], Loss: 0.3100\n",
      "Epoch [1/1], Step [2714/3504], Loss: 0.3958\n",
      "Epoch [1/1], Step [2715/3504], Loss: 0.3648\n",
      "Epoch [1/1], Step [2716/3504], Loss: 0.3540\n",
      "Epoch [1/1], Step [2717/3504], Loss: 0.4175\n",
      "Epoch [1/1], Step [2718/3504], Loss: 0.3625\n",
      "Epoch [1/1], Step [2719/3504], Loss: 0.3609\n",
      "Epoch [1/1], Step [2720/3504], Loss: 0.3944\n",
      "Epoch [1/1], Step [2721/3504], Loss: 0.3517\n",
      "Epoch [1/1], Step [2722/3504], Loss: 0.3769\n",
      "Epoch [1/1], Step [2723/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [2724/3504], Loss: 0.4594\n",
      "Epoch [1/1], Step [2725/3504], Loss: 0.2879\n",
      "Epoch [1/1], Step [2726/3504], Loss: 0.2942\n",
      "Epoch [1/1], Step [2727/3504], Loss: 0.3365\n",
      "Epoch [1/1], Step [2728/3504], Loss: 0.4911\n",
      "Epoch [1/1], Step [2729/3504], Loss: 0.5162\n",
      "Epoch [1/1], Step [2730/3504], Loss: 0.3390\n",
      "Epoch [1/1], Step [2731/3504], Loss: 0.3818\n",
      "Epoch [1/1], Step [2732/3504], Loss: 0.3385\n",
      "Epoch [1/1], Step [2733/3504], Loss: 0.3938\n",
      "Epoch [1/1], Step [2734/3504], Loss: 0.2849\n",
      "Epoch [1/1], Step [2735/3504], Loss: 0.3873\n",
      "Epoch [1/1], Step [2736/3504], Loss: 0.3506\n",
      "Epoch [1/1], Step [2737/3504], Loss: 0.3334\n",
      "Epoch [1/1], Step [2738/3504], Loss: 0.3422\n",
      "Epoch [1/1], Step [2739/3504], Loss: 0.4863\n",
      "Epoch [1/1], Step [2740/3504], Loss: 0.2879\n",
      "Epoch [1/1], Step [2741/3504], Loss: 0.3260\n",
      "Epoch [1/1], Step [2742/3504], Loss: 0.3922\n",
      "Epoch [1/1], Step [2743/3504], Loss: 0.3316\n",
      "Epoch [1/1], Step [2744/3504], Loss: 0.3072\n",
      "Epoch [1/1], Step [2745/3504], Loss: 0.3242\n",
      "Epoch [1/1], Step [2746/3504], Loss: 0.3174\n",
      "Epoch [1/1], Step [2747/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [2748/3504], Loss: 0.2930\n",
      "Epoch [1/1], Step [2749/3504], Loss: 0.3211\n",
      "Epoch [1/1], Step [2750/3504], Loss: 0.2828\n",
      "Epoch [1/1], Step [2751/3504], Loss: 0.3879\n",
      "Epoch [1/1], Step [2752/3504], Loss: 0.4056\n",
      "Epoch [1/1], Step [2753/3504], Loss: 0.4252\n",
      "Epoch [1/1], Step [2754/3504], Loss: 0.3378\n",
      "Epoch [1/1], Step [2755/3504], Loss: 0.4079\n",
      "Epoch [1/1], Step [2756/3504], Loss: 0.4469\n",
      "Epoch [1/1], Step [2757/3504], Loss: 0.4129\n",
      "Epoch [1/1], Step [2758/3504], Loss: 0.4335\n",
      "Epoch [1/1], Step [2759/3504], Loss: 0.3489\n",
      "Epoch [1/1], Step [2760/3504], Loss: 0.3389\n",
      "Epoch [1/1], Step [2761/3504], Loss: 0.3763\n",
      "Epoch [1/1], Step [2762/3504], Loss: 0.3321\n",
      "Epoch [1/1], Step [2763/3504], Loss: 0.3542\n",
      "Epoch [1/1], Step [2764/3504], Loss: 0.3908\n",
      "Epoch [1/1], Step [2765/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [2766/3504], Loss: 0.2931\n",
      "Epoch [1/1], Step [2767/3504], Loss: 0.3250\n",
      "Epoch [1/1], Step [2768/3504], Loss: 0.1993\n",
      "Epoch [1/1], Step [2769/3504], Loss: 0.3705\n",
      "Epoch [1/1], Step [2770/3504], Loss: 0.2782\n",
      "Epoch [1/1], Step [2771/3504], Loss: 0.3569\n",
      "Epoch [1/1], Step [2772/3504], Loss: 0.4428\n",
      "Epoch [1/1], Step [2773/3504], Loss: 0.2489\n",
      "Epoch [1/1], Step [2774/3504], Loss: 0.4516\n",
      "Epoch [1/1], Step [2775/3504], Loss: 0.3306\n",
      "Epoch [1/1], Step [2776/3504], Loss: 0.2884\n",
      "Epoch [1/1], Step [2777/3504], Loss: 0.4331\n",
      "Epoch [1/1], Step [2778/3504], Loss: 0.3601\n",
      "Epoch [1/1], Step [2779/3504], Loss: 0.3493\n",
      "Epoch [1/1], Step [2780/3504], Loss: 0.2845\n",
      "Epoch [1/1], Step [2781/3504], Loss: 0.3059\n",
      "Epoch [1/1], Step [2782/3504], Loss: 0.4244\n",
      "Epoch [1/1], Step [2783/3504], Loss: 0.4336\n",
      "Epoch [1/1], Step [2784/3504], Loss: 0.2892\n",
      "Epoch [1/1], Step [2785/3504], Loss: 0.3292\n",
      "Epoch [1/1], Step [2786/3504], Loss: 0.3425\n",
      "Epoch [1/1], Step [2787/3504], Loss: 0.3726\n",
      "Epoch [1/1], Step [2788/3504], Loss: 0.3527\n",
      "Epoch [1/1], Step [2789/3504], Loss: 0.3071\n",
      "Epoch [1/1], Step [2790/3504], Loss: 0.3327\n",
      "Epoch [1/1], Step [2791/3504], Loss: 0.3446\n",
      "Epoch [1/1], Step [2792/3504], Loss: 0.4195\n",
      "Epoch [1/1], Step [2793/3504], Loss: 0.3185\n",
      "Epoch [1/1], Step [2794/3504], Loss: 0.3935\n",
      "Epoch [1/1], Step [2795/3504], Loss: 0.3090\n",
      "Epoch [1/1], Step [2796/3504], Loss: 0.4259\n",
      "Epoch [1/1], Step [2797/3504], Loss: 0.2995\n",
      "Epoch [1/1], Step [2798/3504], Loss: 0.2979\n",
      "Epoch [1/1], Step [2799/3504], Loss: 0.5272\n",
      "Epoch [1/1], Step [2800/3504], Loss: 0.4946\n",
      "Epoch [1/1], Step [2801/3504], Loss: 0.3051\n",
      "Epoch [1/1], Step [2802/3504], Loss: 0.3363\n",
      "Epoch [1/1], Step [2803/3504], Loss: 0.3365\n",
      "Epoch [1/1], Step [2804/3504], Loss: 0.3280\n",
      "Epoch [1/1], Step [2805/3504], Loss: 0.4642\n",
      "Epoch [1/1], Step [2806/3504], Loss: 0.3170\n",
      "Epoch [1/1], Step [2807/3504], Loss: 0.3274\n",
      "Epoch [1/1], Step [2808/3504], Loss: 0.3587\n",
      "Epoch [1/1], Step [2809/3504], Loss: 0.3176\n",
      "Epoch [1/1], Step [2810/3504], Loss: 0.4567\n",
      "Epoch [1/1], Step [2811/3504], Loss: 0.4029\n",
      "Epoch [1/1], Step [2812/3504], Loss: 0.3975\n",
      "Epoch [1/1], Step [2813/3504], Loss: 0.3610\n",
      "Epoch [1/1], Step [2814/3504], Loss: 0.4383\n",
      "Epoch [1/1], Step [2815/3504], Loss: 0.3675\n",
      "Epoch [1/1], Step [2816/3504], Loss: 0.3167\n",
      "Epoch [1/1], Step [2817/3504], Loss: 0.3748\n",
      "Epoch [1/1], Step [2818/3504], Loss: 0.3657\n",
      "Epoch [1/1], Step [2819/3504], Loss: 0.3226\n",
      "Epoch [1/1], Step [2820/3504], Loss: 0.4735\n",
      "Epoch [1/1], Step [2821/3504], Loss: 0.4730\n",
      "Epoch [1/1], Step [2822/3504], Loss: 0.3564\n",
      "Epoch [1/1], Step [2823/3504], Loss: 0.4150\n",
      "Epoch [1/1], Step [2824/3504], Loss: 0.4166\n",
      "Epoch [1/1], Step [2825/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [2826/3504], Loss: 0.4764\n",
      "Epoch [1/1], Step [2827/3504], Loss: 0.3644\n",
      "Epoch [1/1], Step [2828/3504], Loss: 0.3865\n",
      "Epoch [1/1], Step [2829/3504], Loss: 0.3745\n",
      "Epoch [1/1], Step [2830/3504], Loss: 0.3087\n",
      "Epoch [1/1], Step [2831/3504], Loss: 0.3369\n",
      "Epoch [1/1], Step [2832/3504], Loss: 0.3517\n",
      "Epoch [1/1], Step [2833/3504], Loss: 0.4404\n",
      "Epoch [1/1], Step [2834/3504], Loss: 0.2565\n",
      "Epoch [1/1], Step [2835/3504], Loss: 0.4563\n",
      "Epoch [1/1], Step [2836/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [2837/3504], Loss: 0.3151\n",
      "Epoch [1/1], Step [2838/3504], Loss: 0.3714\n",
      "Epoch [1/1], Step [2839/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [2840/3504], Loss: 0.3613\n",
      "Epoch [1/1], Step [2841/3504], Loss: 0.3501\n",
      "Epoch [1/1], Step [2842/3504], Loss: 0.3934\n",
      "Epoch [1/1], Step [2843/3504], Loss: 0.3773\n",
      "Epoch [1/1], Step [2844/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [2845/3504], Loss: 0.3440\n",
      "Epoch [1/1], Step [2846/3504], Loss: 0.4120\n",
      "Epoch [1/1], Step [2847/3504], Loss: 0.3425\n",
      "Epoch [1/1], Step [2848/3504], Loss: 0.3900\n",
      "Epoch [1/1], Step [2849/3504], Loss: 0.3638\n",
      "Epoch [1/1], Step [2850/3504], Loss: 0.3161\n",
      "Epoch [1/1], Step [2851/3504], Loss: 0.4052\n",
      "Epoch [1/1], Step [2852/3504], Loss: 0.3541\n",
      "Epoch [1/1], Step [2853/3504], Loss: 0.3566\n",
      "Epoch [1/1], Step [2854/3504], Loss: 0.4064\n",
      "Epoch [1/1], Step [2855/3504], Loss: 0.3436\n",
      "Epoch [1/1], Step [2856/3504], Loss: 0.3146\n",
      "Epoch [1/1], Step [2857/3504], Loss: 0.4183\n",
      "Epoch [1/1], Step [2858/3504], Loss: 0.2979\n",
      "Epoch [1/1], Step [2859/3504], Loss: 0.2671\n",
      "Epoch [1/1], Step [2860/3504], Loss: 0.3595\n",
      "Epoch [1/1], Step [2861/3504], Loss: 0.3202\n",
      "Epoch [1/1], Step [2862/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [2863/3504], Loss: 0.3086\n",
      "Epoch [1/1], Step [2864/3504], Loss: 0.4587\n",
      "Epoch [1/1], Step [2865/3504], Loss: 0.3423\n",
      "Epoch [1/1], Step [2866/3504], Loss: 0.3490\n",
      "Epoch [1/1], Step [2867/3504], Loss: 0.2750\n",
      "Epoch [1/1], Step [2868/3504], Loss: 0.2892\n",
      "Epoch [1/1], Step [2869/3504], Loss: 0.3623\n",
      "Epoch [1/1], Step [2870/3504], Loss: 0.4418\n",
      "Epoch [1/1], Step [2871/3504], Loss: 0.3810\n",
      "Epoch [1/1], Step [2872/3504], Loss: 0.3687\n",
      "Epoch [1/1], Step [2873/3504], Loss: 0.3300\n",
      "Epoch [1/1], Step [2874/3504], Loss: 0.3351\n",
      "Epoch [1/1], Step [2875/3504], Loss: 0.3110\n",
      "Epoch [1/1], Step [2876/3504], Loss: 0.4126\n",
      "Epoch [1/1], Step [2877/3504], Loss: 0.3141\n",
      "Epoch [1/1], Step [2878/3504], Loss: 0.2793\n",
      "Epoch [1/1], Step [2879/3504], Loss: 0.3458\n",
      "Epoch [1/1], Step [2880/3504], Loss: 0.3333\n",
      "Epoch [1/1], Step [2881/3504], Loss: 0.4071\n",
      "Epoch [1/1], Step [2882/3504], Loss: 0.3175\n",
      "Epoch [1/1], Step [2883/3504], Loss: 0.2967\n",
      "Epoch [1/1], Step [2884/3504], Loss: 0.4074\n",
      "Epoch [1/1], Step [2885/3504], Loss: 0.3574\n",
      "Epoch [1/1], Step [2886/3504], Loss: 0.3890\n",
      "Epoch [1/1], Step [2887/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [2888/3504], Loss: 0.3200\n",
      "Epoch [1/1], Step [2889/3504], Loss: 0.3132\n",
      "Epoch [1/1], Step [2890/3504], Loss: 0.3583\n",
      "Epoch [1/1], Step [2891/3504], Loss: 0.4145\n",
      "Epoch [1/1], Step [2892/3504], Loss: 0.3109\n",
      "Epoch [1/1], Step [2893/3504], Loss: 0.3178\n",
      "Epoch [1/1], Step [2894/3504], Loss: 0.3033\n",
      "Epoch [1/1], Step [2895/3504], Loss: 0.4312\n",
      "Epoch [1/1], Step [2896/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [2897/3504], Loss: 0.3615\n",
      "Epoch [1/1], Step [2898/3504], Loss: 0.3354\n",
      "Epoch [1/1], Step [2899/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [2900/3504], Loss: 0.4125\n",
      "Epoch [1/1], Step [2901/3504], Loss: 0.4588\n",
      "Epoch [1/1], Step [2902/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [2903/3504], Loss: 0.3612\n",
      "Epoch [1/1], Step [2904/3504], Loss: 0.2577\n",
      "Epoch [1/1], Step [2905/3504], Loss: 0.3270\n",
      "Epoch [1/1], Step [2906/3504], Loss: 0.4186\n",
      "Epoch [1/1], Step [2907/3504], Loss: 0.3763\n",
      "Epoch [1/1], Step [2908/3504], Loss: 0.3303\n",
      "Epoch [1/1], Step [2909/3504], Loss: 0.3326\n",
      "Epoch [1/1], Step [2910/3504], Loss: 0.3327\n",
      "Epoch [1/1], Step [2911/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [2912/3504], Loss: 0.3766\n",
      "Epoch [1/1], Step [2913/3504], Loss: 0.4441\n",
      "Epoch [1/1], Step [2914/3504], Loss: 0.3839\n",
      "Epoch [1/1], Step [2915/3504], Loss: 0.3311\n",
      "Epoch [1/1], Step [2916/3504], Loss: 0.3739\n",
      "Epoch [1/1], Step [2917/3504], Loss: 0.4318\n",
      "Epoch [1/1], Step [2918/3504], Loss: 0.3868\n",
      "Epoch [1/1], Step [2919/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [2920/3504], Loss: 0.3473\n",
      "Epoch [1/1], Step [2921/3504], Loss: 0.3371\n",
      "Epoch [1/1], Step [2922/3504], Loss: 0.4297\n",
      "Epoch [1/1], Step [2923/3504], Loss: 0.3106\n",
      "Epoch [1/1], Step [2924/3504], Loss: 0.3421\n",
      "Epoch [1/1], Step [2925/3504], Loss: 0.3729\n",
      "Epoch [1/1], Step [2926/3504], Loss: 0.3035\n",
      "Epoch [1/1], Step [2927/3504], Loss: 0.2804\n",
      "Epoch [1/1], Step [2928/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [2929/3504], Loss: 0.3897\n",
      "Epoch [1/1], Step [2930/3504], Loss: 0.3748\n",
      "Epoch [1/1], Step [2931/3504], Loss: 0.3301\n",
      "Epoch [1/1], Step [2932/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [2933/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [2934/3504], Loss: 0.3138\n",
      "Epoch [1/1], Step [2935/3504], Loss: 0.3497\n",
      "Epoch [1/1], Step [2936/3504], Loss: 0.3651\n",
      "Epoch [1/1], Step [2937/3504], Loss: 0.2300\n",
      "Epoch [1/1], Step [2938/3504], Loss: 0.3807\n",
      "Epoch [1/1], Step [2939/3504], Loss: 0.3563\n",
      "Epoch [1/1], Step [2940/3504], Loss: 0.3407\n",
      "Epoch [1/1], Step [2941/3504], Loss: 0.3033\n",
      "Epoch [1/1], Step [2942/3504], Loss: 0.3799\n",
      "Epoch [1/1], Step [2943/3504], Loss: 0.4151\n",
      "Epoch [1/1], Step [2944/3504], Loss: 0.2610\n",
      "Epoch [1/1], Step [2945/3504], Loss: 0.3227\n",
      "Epoch [1/1], Step [2946/3504], Loss: 0.2791\n",
      "Epoch [1/1], Step [2947/3504], Loss: 0.4117\n",
      "Epoch [1/1], Step [2948/3504], Loss: 0.3315\n",
      "Epoch [1/1], Step [2949/3504], Loss: 0.4177\n",
      "Epoch [1/1], Step [2950/3504], Loss: 0.4155\n",
      "Epoch [1/1], Step [2951/3504], Loss: 0.3482\n",
      "Epoch [1/1], Step [2952/3504], Loss: 0.3723\n",
      "Epoch [1/1], Step [2953/3504], Loss: 0.2797\n",
      "Epoch [1/1], Step [2954/3504], Loss: 0.4598\n",
      "Epoch [1/1], Step [2955/3504], Loss: 0.4486\n",
      "Epoch [1/1], Step [2956/3504], Loss: 0.2848\n",
      "Epoch [1/1], Step [2957/3504], Loss: 0.3646\n",
      "Epoch [1/1], Step [2958/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [2959/3504], Loss: 0.2717\n",
      "Epoch [1/1], Step [2960/3504], Loss: 0.4229\n",
      "Epoch [1/1], Step [2961/3504], Loss: 0.3659\n",
      "Epoch [1/1], Step [2962/3504], Loss: 0.4546\n",
      "Epoch [1/1], Step [2963/3504], Loss: 0.3931\n",
      "Epoch [1/1], Step [2964/3504], Loss: 0.3225\n",
      "Epoch [1/1], Step [2965/3504], Loss: 0.3137\n",
      "Epoch [1/1], Step [2966/3504], Loss: 0.3683\n",
      "Epoch [1/1], Step [2967/3504], Loss: 0.3658\n",
      "Epoch [1/1], Step [2968/3504], Loss: 0.3055\n",
      "Epoch [1/1], Step [2969/3504], Loss: 0.3530\n",
      "Epoch [1/1], Step [2970/3504], Loss: 0.3171\n",
      "Epoch [1/1], Step [2971/3504], Loss: 0.4428\n",
      "Epoch [1/1], Step [2972/3504], Loss: 0.2745\n",
      "Epoch [1/1], Step [2973/3504], Loss: 0.3760\n",
      "Epoch [1/1], Step [2974/3504], Loss: 0.3359\n",
      "Epoch [1/1], Step [2975/3504], Loss: 0.3654\n",
      "Epoch [1/1], Step [2976/3504], Loss: 0.3190\n",
      "Epoch [1/1], Step [2977/3504], Loss: 0.3419\n",
      "Epoch [1/1], Step [2978/3504], Loss: 0.3300\n",
      "Epoch [1/1], Step [2979/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [2980/3504], Loss: 0.3204\n",
      "Epoch [1/1], Step [2981/3504], Loss: 0.3532\n",
      "Epoch [1/1], Step [2982/3504], Loss: 0.3984\n",
      "Epoch [1/1], Step [2983/3504], Loss: 0.4821\n",
      "Epoch [1/1], Step [2984/3504], Loss: 0.3375\n",
      "Epoch [1/1], Step [2985/3504], Loss: 0.3472\n",
      "Epoch [1/1], Step [2986/3504], Loss: 0.4098\n",
      "Epoch [1/1], Step [2987/3504], Loss: 0.4164\n",
      "Epoch [1/1], Step [2988/3504], Loss: 0.3933\n",
      "Epoch [1/1], Step [2989/3504], Loss: 0.2630\n",
      "Epoch [1/1], Step [2990/3504], Loss: 0.4865\n",
      "Epoch [1/1], Step [2991/3504], Loss: 0.3863\n",
      "Epoch [1/1], Step [2992/3504], Loss: 0.3665\n",
      "Epoch [1/1], Step [2993/3504], Loss: 0.3991\n",
      "Epoch [1/1], Step [2994/3504], Loss: 0.3022\n",
      "Epoch [1/1], Step [2995/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [2996/3504], Loss: 0.3296\n",
      "Epoch [1/1], Step [2997/3504], Loss: 0.3243\n",
      "Epoch [1/1], Step [2998/3504], Loss: 0.3708\n",
      "Epoch [1/1], Step [2999/3504], Loss: 0.2879\n",
      "Epoch [1/1], Step [3000/3504], Loss: 0.4360\n",
      "Epoch [1/1], Step [3001/3504], Loss: 0.3743\n",
      "Epoch [1/1], Step [3002/3504], Loss: 0.3583\n",
      "Epoch [1/1], Step [3003/3504], Loss: 0.3208\n",
      "Epoch [1/1], Step [3004/3504], Loss: 0.5306\n",
      "Epoch [1/1], Step [3005/3504], Loss: 0.3135\n",
      "Epoch [1/1], Step [3006/3504], Loss: 0.3951\n",
      "Epoch [1/1], Step [3007/3504], Loss: 0.3416\n",
      "Epoch [1/1], Step [3008/3504], Loss: 0.3245\n",
      "Epoch [1/1], Step [3009/3504], Loss: 0.3652\n",
      "Epoch [1/1], Step [3010/3504], Loss: 0.2900\n",
      "Epoch [1/1], Step [3011/3504], Loss: 0.4648\n",
      "Epoch [1/1], Step [3012/3504], Loss: 0.2708\n",
      "Epoch [1/1], Step [3013/3504], Loss: 0.3394\n",
      "Epoch [1/1], Step [3014/3504], Loss: 0.2777\n",
      "Epoch [1/1], Step [3015/3504], Loss: 0.3323\n",
      "Epoch [1/1], Step [3016/3504], Loss: 0.4327\n",
      "Epoch [1/1], Step [3017/3504], Loss: 0.4124\n",
      "Epoch [1/1], Step [3018/3504], Loss: 0.3751\n",
      "Epoch [1/1], Step [3019/3504], Loss: 0.3794\n",
      "Epoch [1/1], Step [3020/3504], Loss: 0.3470\n",
      "Epoch [1/1], Step [3021/3504], Loss: 0.4009\n",
      "Epoch [1/1], Step [3022/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [3023/3504], Loss: 0.2921\n",
      "Epoch [1/1], Step [3024/3504], Loss: 0.3142\n",
      "Epoch [1/1], Step [3025/3504], Loss: 0.3665\n",
      "Epoch [1/1], Step [3026/3504], Loss: 0.3548\n",
      "Epoch [1/1], Step [3027/3504], Loss: 0.4096\n",
      "Epoch [1/1], Step [3028/3504], Loss: 0.3127\n",
      "Epoch [1/1], Step [3029/3504], Loss: 0.3433\n",
      "Epoch [1/1], Step [3030/3504], Loss: 0.2907\n",
      "Epoch [1/1], Step [3031/3504], Loss: 0.3031\n",
      "Epoch [1/1], Step [3032/3504], Loss: 0.3744\n",
      "Epoch [1/1], Step [3033/3504], Loss: 0.3313\n",
      "Epoch [1/1], Step [3034/3504], Loss: 0.3691\n",
      "Epoch [1/1], Step [3035/3504], Loss: 0.3371\n",
      "Epoch [1/1], Step [3036/3504], Loss: 0.3561\n",
      "Epoch [1/1], Step [3037/3504], Loss: 0.3161\n",
      "Epoch [1/1], Step [3038/3504], Loss: 0.2747\n",
      "Epoch [1/1], Step [3039/3504], Loss: 0.3490\n",
      "Epoch [1/1], Step [3040/3504], Loss: 0.3655\n",
      "Epoch [1/1], Step [3041/3504], Loss: 0.2161\n",
      "Epoch [1/1], Step [3042/3504], Loss: 0.3099\n",
      "Epoch [1/1], Step [3043/3504], Loss: 0.3847\n",
      "Epoch [1/1], Step [3044/3504], Loss: 0.3886\n",
      "Epoch [1/1], Step [3045/3504], Loss: 0.3138\n",
      "Epoch [1/1], Step [3046/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [3047/3504], Loss: 0.3306\n",
      "Epoch [1/1], Step [3048/3504], Loss: 0.3741\n",
      "Epoch [1/1], Step [3049/3504], Loss: 0.3154\n",
      "Epoch [1/1], Step [3050/3504], Loss: 0.3443\n",
      "Epoch [1/1], Step [3051/3504], Loss: 0.3900\n",
      "Epoch [1/1], Step [3052/3504], Loss: 0.4363\n",
      "Epoch [1/1], Step [3053/3504], Loss: 0.3623\n",
      "Epoch [1/1], Step [3054/3504], Loss: 0.4517\n",
      "Epoch [1/1], Step [3055/3504], Loss: 0.4146\n",
      "Epoch [1/1], Step [3056/3504], Loss: 0.3764\n",
      "Epoch [1/1], Step [3057/3504], Loss: 0.3793\n",
      "Epoch [1/1], Step [3058/3504], Loss: 0.3800\n",
      "Epoch [1/1], Step [3059/3504], Loss: 0.3779\n",
      "Epoch [1/1], Step [3060/3504], Loss: 0.3655\n",
      "Epoch [1/1], Step [3061/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [3062/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [3063/3504], Loss: 0.3063\n",
      "Epoch [1/1], Step [3064/3504], Loss: 0.2936\n",
      "Epoch [1/1], Step [3065/3504], Loss: 0.3458\n",
      "Epoch [1/1], Step [3066/3504], Loss: 0.3019\n",
      "Epoch [1/1], Step [3067/3504], Loss: 0.2804\n",
      "Epoch [1/1], Step [3068/3504], Loss: 0.3056\n",
      "Epoch [1/1], Step [3069/3504], Loss: 0.3159\n",
      "Epoch [1/1], Step [3070/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [3071/3504], Loss: 0.3792\n",
      "Epoch [1/1], Step [3072/3504], Loss: 0.4118\n",
      "Epoch [1/1], Step [3073/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [3074/3504], Loss: 0.3632\n",
      "Epoch [1/1], Step [3075/3504], Loss: 0.4139\n",
      "Epoch [1/1], Step [3076/3504], Loss: 0.3725\n",
      "Epoch [1/1], Step [3077/3504], Loss: 0.4837\n",
      "Epoch [1/1], Step [3078/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [3079/3504], Loss: 0.3269\n",
      "Epoch [1/1], Step [3080/3504], Loss: 0.4148\n",
      "Epoch [1/1], Step [3081/3504], Loss: 0.3149\n",
      "Epoch [1/1], Step [3082/3504], Loss: 0.4144\n",
      "Epoch [1/1], Step [3083/3504], Loss: 0.3081\n",
      "Epoch [1/1], Step [3084/3504], Loss: 0.3318\n",
      "Epoch [1/1], Step [3085/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [3086/3504], Loss: 0.3168\n",
      "Epoch [1/1], Step [3087/3504], Loss: 0.4472\n",
      "Epoch [1/1], Step [3088/3504], Loss: 0.5111\n",
      "Epoch [1/1], Step [3089/3504], Loss: 0.3518\n",
      "Epoch [1/1], Step [3090/3504], Loss: 0.4296\n",
      "Epoch [1/1], Step [3091/3504], Loss: 0.3284\n",
      "Epoch [1/1], Step [3092/3504], Loss: 0.3271\n",
      "Epoch [1/1], Step [3093/3504], Loss: 0.3089\n",
      "Epoch [1/1], Step [3094/3504], Loss: 0.3565\n",
      "Epoch [1/1], Step [3095/3504], Loss: 0.4658\n",
      "Epoch [1/1], Step [3096/3504], Loss: 0.2992\n",
      "Epoch [1/1], Step [3097/3504], Loss: 0.3023\n",
      "Epoch [1/1], Step [3098/3504], Loss: 0.3761\n",
      "Epoch [1/1], Step [3099/3504], Loss: 0.4717\n",
      "Epoch [1/1], Step [3100/3504], Loss: 0.4401\n",
      "Epoch [1/1], Step [3101/3504], Loss: 0.3341\n",
      "Epoch [1/1], Step [3102/3504], Loss: 0.4046\n",
      "Epoch [1/1], Step [3103/3504], Loss: 0.3497\n",
      "Epoch [1/1], Step [3104/3504], Loss: 0.3706\n",
      "Epoch [1/1], Step [3105/3504], Loss: 0.4153\n",
      "Epoch [1/1], Step [3106/3504], Loss: 0.4461\n",
      "Epoch [1/1], Step [3107/3504], Loss: 0.3731\n",
      "Epoch [1/1], Step [3108/3504], Loss: 0.3837\n",
      "Epoch [1/1], Step [3109/3504], Loss: 0.3733\n",
      "Epoch [1/1], Step [3110/3504], Loss: 0.3429\n",
      "Epoch [1/1], Step [3111/3504], Loss: 0.4776\n",
      "Epoch [1/1], Step [3112/3504], Loss: 0.3647\n",
      "Epoch [1/1], Step [3113/3504], Loss: 0.3632\n",
      "Epoch [1/1], Step [3114/3504], Loss: 0.4040\n",
      "Epoch [1/1], Step [3115/3504], Loss: 0.3844\n",
      "Epoch [1/1], Step [3116/3504], Loss: 0.3871\n",
      "Epoch [1/1], Step [3117/3504], Loss: 0.3122\n",
      "Epoch [1/1], Step [3118/3504], Loss: 0.4111\n",
      "Epoch [1/1], Step [3119/3504], Loss: 0.4103\n",
      "Epoch [1/1], Step [3120/3504], Loss: 0.3971\n",
      "Epoch [1/1], Step [3121/3504], Loss: 0.3730\n",
      "Epoch [1/1], Step [3122/3504], Loss: 0.4210\n",
      "Epoch [1/1], Step [3123/3504], Loss: 0.3806\n",
      "Epoch [1/1], Step [3124/3504], Loss: 0.3233\n",
      "Epoch [1/1], Step [3125/3504], Loss: 0.3659\n",
      "Epoch [1/1], Step [3126/3504], Loss: 0.3754\n",
      "Epoch [1/1], Step [3127/3504], Loss: 0.3323\n",
      "Epoch [1/1], Step [3128/3504], Loss: 0.3661\n",
      "Epoch [1/1], Step [3129/3504], Loss: 0.3521\n",
      "Epoch [1/1], Step [3130/3504], Loss: 0.3922\n",
      "Epoch [1/1], Step [3131/3504], Loss: 0.3853\n",
      "Epoch [1/1], Step [3132/3504], Loss: 0.3637\n",
      "Epoch [1/1], Step [3133/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [3134/3504], Loss: 0.2584\n",
      "Epoch [1/1], Step [3135/3504], Loss: 0.3318\n",
      "Epoch [1/1], Step [3136/3504], Loss: 0.3607\n",
      "Epoch [1/1], Step [3137/3504], Loss: 0.3873\n",
      "Epoch [1/1], Step [3138/3504], Loss: 0.3632\n",
      "Epoch [1/1], Step [3139/3504], Loss: 0.3477\n",
      "Epoch [1/1], Step [3140/3504], Loss: 0.3818\n",
      "Epoch [1/1], Step [3141/3504], Loss: 0.3790\n",
      "Epoch [1/1], Step [3142/3504], Loss: 0.3683\n",
      "Epoch [1/1], Step [3143/3504], Loss: 0.4218\n",
      "Epoch [1/1], Step [3144/3504], Loss: 0.3659\n",
      "Epoch [1/1], Step [3145/3504], Loss: 0.3575\n",
      "Epoch [1/1], Step [3146/3504], Loss: 0.3195\n",
      "Epoch [1/1], Step [3147/3504], Loss: 0.4135\n",
      "Epoch [1/1], Step [3148/3504], Loss: 0.4144\n",
      "Epoch [1/1], Step [3149/3504], Loss: 0.3190\n",
      "Epoch [1/1], Step [3150/3504], Loss: 0.3983\n",
      "Epoch [1/1], Step [3151/3504], Loss: 0.3619\n",
      "Epoch [1/1], Step [3152/3504], Loss: 0.4104\n",
      "Epoch [1/1], Step [3153/3504], Loss: 0.4141\n",
      "Epoch [1/1], Step [3154/3504], Loss: 0.3807\n",
      "Epoch [1/1], Step [3155/3504], Loss: 0.3776\n",
      "Epoch [1/1], Step [3156/3504], Loss: 0.3684\n",
      "Epoch [1/1], Step [3157/3504], Loss: 0.3758\n",
      "Epoch [1/1], Step [3158/3504], Loss: 0.3062\n",
      "Epoch [1/1], Step [3159/3504], Loss: 0.3777\n",
      "Epoch [1/1], Step [3160/3504], Loss: 0.3240\n",
      "Epoch [1/1], Step [3161/3504], Loss: 0.3761\n",
      "Epoch [1/1], Step [3162/3504], Loss: 0.3257\n",
      "Epoch [1/1], Step [3163/3504], Loss: 0.3238\n",
      "Epoch [1/1], Step [3164/3504], Loss: 0.3293\n",
      "Epoch [1/1], Step [3165/3504], Loss: 0.3427\n",
      "Epoch [1/1], Step [3166/3504], Loss: 0.2846\n",
      "Epoch [1/1], Step [3167/3504], Loss: 0.3479\n",
      "Epoch [1/1], Step [3168/3504], Loss: 0.4277\n",
      "Epoch [1/1], Step [3169/3504], Loss: 0.3033\n",
      "Epoch [1/1], Step [3170/3504], Loss: 0.3405\n",
      "Epoch [1/1], Step [3171/3504], Loss: 0.2510\n",
      "Epoch [1/1], Step [3172/3504], Loss: 0.4867\n",
      "Epoch [1/1], Step [3173/3504], Loss: 0.3857\n",
      "Epoch [1/1], Step [3174/3504], Loss: 0.4309\n",
      "Epoch [1/1], Step [3175/3504], Loss: 0.3355\n",
      "Epoch [1/1], Step [3176/3504], Loss: 0.2984\n",
      "Epoch [1/1], Step [3177/3504], Loss: 0.3179\n",
      "Epoch [1/1], Step [3178/3504], Loss: 0.4651\n",
      "Epoch [1/1], Step [3179/3504], Loss: 0.4598\n",
      "Epoch [1/1], Step [3180/3504], Loss: 0.4262\n",
      "Epoch [1/1], Step [3181/3504], Loss: 0.3542\n",
      "Epoch [1/1], Step [3182/3504], Loss: 0.3629\n",
      "Epoch [1/1], Step [3183/3504], Loss: 0.4253\n",
      "Epoch [1/1], Step [3184/3504], Loss: 0.3395\n",
      "Epoch [1/1], Step [3185/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [3186/3504], Loss: 0.4401\n",
      "Epoch [1/1], Step [3187/3504], Loss: 0.4239\n",
      "Epoch [1/1], Step [3188/3504], Loss: 0.3716\n",
      "Epoch [1/1], Step [3189/3504], Loss: 0.3681\n",
      "Epoch [1/1], Step [3190/3504], Loss: 0.3283\n",
      "Epoch [1/1], Step [3191/3504], Loss: 0.3520\n",
      "Epoch [1/1], Step [3192/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [3193/3504], Loss: 0.3372\n",
      "Epoch [1/1], Step [3194/3504], Loss: 0.3743\n",
      "Epoch [1/1], Step [3195/3504], Loss: 0.3212\n",
      "Epoch [1/1], Step [3196/3504], Loss: 0.3705\n",
      "Epoch [1/1], Step [3197/3504], Loss: 0.2955\n",
      "Epoch [1/1], Step [3198/3504], Loss: 0.3495\n",
      "Epoch [1/1], Step [3199/3504], Loss: 0.3086\n",
      "Epoch [1/1], Step [3200/3504], Loss: 0.3142\n",
      "Epoch [1/1], Step [3201/3504], Loss: 0.3501\n",
      "Epoch [1/1], Step [3202/3504], Loss: 0.3132\n",
      "Epoch [1/1], Step [3203/3504], Loss: 0.3259\n",
      "Epoch [1/1], Step [3204/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [3205/3504], Loss: 0.3886\n",
      "Epoch [1/1], Step [3206/3504], Loss: 0.3232\n",
      "Epoch [1/1], Step [3207/3504], Loss: 0.3991\n",
      "Epoch [1/1], Step [3208/3504], Loss: 0.3965\n",
      "Epoch [1/1], Step [3209/3504], Loss: 0.4189\n",
      "Epoch [1/1], Step [3210/3504], Loss: 0.3797\n",
      "Epoch [1/1], Step [3211/3504], Loss: 0.3334\n",
      "Epoch [1/1], Step [3212/3504], Loss: 0.3073\n",
      "Epoch [1/1], Step [3213/3504], Loss: 0.3325\n",
      "Epoch [1/1], Step [3214/3504], Loss: 0.3163\n",
      "Epoch [1/1], Step [3215/3504], Loss: 0.3243\n",
      "Epoch [1/1], Step [3216/3504], Loss: 0.3567\n",
      "Epoch [1/1], Step [3217/3504], Loss: 0.5089\n",
      "Epoch [1/1], Step [3218/3504], Loss: 0.3657\n",
      "Epoch [1/1], Step [3219/3504], Loss: 0.3045\n",
      "Epoch [1/1], Step [3220/3504], Loss: 0.3117\n",
      "Epoch [1/1], Step [3221/3504], Loss: 0.4343\n",
      "Epoch [1/1], Step [3222/3504], Loss: 0.3687\n",
      "Epoch [1/1], Step [3223/3504], Loss: 0.3904\n",
      "Epoch [1/1], Step [3224/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [3225/3504], Loss: 0.2896\n",
      "Epoch [1/1], Step [3226/3504], Loss: 0.3387\n",
      "Epoch [1/1], Step [3227/3504], Loss: 0.2959\n",
      "Epoch [1/1], Step [3228/3504], Loss: 0.3084\n",
      "Epoch [1/1], Step [3229/3504], Loss: 0.4210\n",
      "Epoch [1/1], Step [3230/3504], Loss: 0.3425\n",
      "Epoch [1/1], Step [3231/3504], Loss: 0.3843\n",
      "Epoch [1/1], Step [3232/3504], Loss: 0.3202\n",
      "Epoch [1/1], Step [3233/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [3234/3504], Loss: 0.3405\n",
      "Epoch [1/1], Step [3235/3504], Loss: 0.3671\n",
      "Epoch [1/1], Step [3236/3504], Loss: 0.3523\n",
      "Epoch [1/1], Step [3237/3504], Loss: 0.3968\n",
      "Epoch [1/1], Step [3238/3504], Loss: 0.3074\n",
      "Epoch [1/1], Step [3239/3504], Loss: 0.3631\n",
      "Epoch [1/1], Step [3240/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [3241/3504], Loss: 0.4116\n",
      "Epoch [1/1], Step [3242/3504], Loss: 0.3681\n",
      "Epoch [1/1], Step [3243/3504], Loss: 0.3988\n",
      "Epoch [1/1], Step [3244/3504], Loss: 0.2626\n",
      "Epoch [1/1], Step [3245/3504], Loss: 0.3701\n",
      "Epoch [1/1], Step [3246/3504], Loss: 0.2720\n",
      "Epoch [1/1], Step [3247/3504], Loss: 0.3603\n",
      "Epoch [1/1], Step [3248/3504], Loss: 0.4189\n",
      "Epoch [1/1], Step [3249/3504], Loss: 0.3362\n",
      "Epoch [1/1], Step [3250/3504], Loss: 0.3287\n",
      "Epoch [1/1], Step [3251/3504], Loss: 0.3697\n",
      "Epoch [1/1], Step [3252/3504], Loss: 0.3855\n",
      "Epoch [1/1], Step [3253/3504], Loss: 0.4402\n",
      "Epoch [1/1], Step [3254/3504], Loss: 0.3399\n",
      "Epoch [1/1], Step [3255/3504], Loss: 0.3676\n",
      "Epoch [1/1], Step [3256/3504], Loss: 0.3284\n",
      "Epoch [1/1], Step [3257/3504], Loss: 0.2942\n",
      "Epoch [1/1], Step [3258/3504], Loss: 0.3619\n",
      "Epoch [1/1], Step [3259/3504], Loss: 0.4632\n",
      "Epoch [1/1], Step [3260/3504], Loss: 0.3435\n",
      "Epoch [1/1], Step [3261/3504], Loss: 0.3229\n",
      "Epoch [1/1], Step [3262/3504], Loss: 0.3095\n",
      "Epoch [1/1], Step [3263/3504], Loss: 0.3038\n",
      "Epoch [1/1], Step [3264/3504], Loss: 0.2776\n",
      "Epoch [1/1], Step [3265/3504], Loss: 0.3449\n",
      "Epoch [1/1], Step [3266/3504], Loss: 0.3439\n",
      "Epoch [1/1], Step [3267/3504], Loss: 0.2874\n",
      "Epoch [1/1], Step [3268/3504], Loss: 0.3736\n",
      "Epoch [1/1], Step [3269/3504], Loss: 0.3727\n",
      "Epoch [1/1], Step [3270/3504], Loss: 0.3332\n",
      "Epoch [1/1], Step [3271/3504], Loss: 0.3617\n",
      "Epoch [1/1], Step [3272/3504], Loss: 0.3719\n",
      "Epoch [1/1], Step [3273/3504], Loss: 0.4050\n",
      "Epoch [1/1], Step [3274/3504], Loss: 0.3026\n",
      "Epoch [1/1], Step [3275/3504], Loss: 0.3161\n",
      "Epoch [1/1], Step [3276/3504], Loss: 0.3575\n",
      "Epoch [1/1], Step [3277/3504], Loss: 0.3296\n",
      "Epoch [1/1], Step [3278/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [3279/3504], Loss: 0.4414\n",
      "Epoch [1/1], Step [3280/3504], Loss: 0.3463\n",
      "Epoch [1/1], Step [3281/3504], Loss: 0.3781\n",
      "Epoch [1/1], Step [3282/3504], Loss: 0.3301\n",
      "Epoch [1/1], Step [3283/3504], Loss: 0.3602\n",
      "Epoch [1/1], Step [3284/3504], Loss: 0.3616\n",
      "Epoch [1/1], Step [3285/3504], Loss: 0.3973\n",
      "Epoch [1/1], Step [3286/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [3287/3504], Loss: 0.3822\n",
      "Epoch [1/1], Step [3288/3504], Loss: 0.3927\n",
      "Epoch [1/1], Step [3289/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [3290/3504], Loss: 0.3054\n",
      "Epoch [1/1], Step [3291/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [3292/3504], Loss: 0.3985\n",
      "Epoch [1/1], Step [3293/3504], Loss: 0.4092\n",
      "Epoch [1/1], Step [3294/3504], Loss: 0.4142\n",
      "Epoch [1/1], Step [3295/3504], Loss: 0.3798\n",
      "Epoch [1/1], Step [3296/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [3297/3504], Loss: 0.2840\n",
      "Epoch [1/1], Step [3298/3504], Loss: 0.3944\n",
      "Epoch [1/1], Step [3299/3504], Loss: 0.3220\n",
      "Epoch [1/1], Step [3300/3504], Loss: 0.3418\n",
      "Epoch [1/1], Step [3301/3504], Loss: 0.3302\n",
      "Epoch [1/1], Step [3302/3504], Loss: 0.3913\n",
      "Epoch [1/1], Step [3303/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [3304/3504], Loss: 0.3155\n",
      "Epoch [1/1], Step [3305/3504], Loss: 0.3527\n",
      "Epoch [1/1], Step [3306/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [3307/3504], Loss: 0.3360\n",
      "Epoch [1/1], Step [3308/3504], Loss: 0.3914\n",
      "Epoch [1/1], Step [3309/3504], Loss: 0.2783\n",
      "Epoch [1/1], Step [3310/3504], Loss: 0.3601\n",
      "Epoch [1/1], Step [3311/3504], Loss: 0.4562\n",
      "Epoch [1/1], Step [3312/3504], Loss: 0.4750\n",
      "Epoch [1/1], Step [3313/3504], Loss: 0.3559\n",
      "Epoch [1/1], Step [3314/3504], Loss: 0.3584\n",
      "Epoch [1/1], Step [3315/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [3316/3504], Loss: 0.3432\n",
      "Epoch [1/1], Step [3317/3504], Loss: 0.4169\n",
      "Epoch [1/1], Step [3318/3504], Loss: 0.4161\n",
      "Epoch [1/1], Step [3319/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [3320/3504], Loss: 0.3035\n",
      "Epoch [1/1], Step [3321/3504], Loss: 0.3455\n",
      "Epoch [1/1], Step [3322/3504], Loss: 0.3348\n",
      "Epoch [1/1], Step [3323/3504], Loss: 0.3444\n",
      "Epoch [1/1], Step [3324/3504], Loss: 0.4683\n",
      "Epoch [1/1], Step [3325/3504], Loss: 0.2945\n",
      "Epoch [1/1], Step [3326/3504], Loss: 0.3207\n",
      "Epoch [1/1], Step [3327/3504], Loss: 0.3835\n",
      "Epoch [1/1], Step [3328/3504], Loss: 0.3408\n",
      "Epoch [1/1], Step [3329/3504], Loss: 0.3275\n",
      "Epoch [1/1], Step [3330/3504], Loss: 0.3536\n",
      "Epoch [1/1], Step [3331/3504], Loss: 0.3330\n",
      "Epoch [1/1], Step [3332/3504], Loss: 0.4048\n",
      "Epoch [1/1], Step [3333/3504], Loss: 0.3228\n",
      "Epoch [1/1], Step [3334/3504], Loss: 0.4431\n",
      "Epoch [1/1], Step [3335/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [3336/3504], Loss: 0.3581\n",
      "Epoch [1/1], Step [3337/3504], Loss: 0.2883\n",
      "Epoch [1/1], Step [3338/3504], Loss: 0.3453\n",
      "Epoch [1/1], Step [3339/3504], Loss: 0.3442\n",
      "Epoch [1/1], Step [3340/3504], Loss: 0.4418\n",
      "Epoch [1/1], Step [3341/3504], Loss: 0.3187\n",
      "Epoch [1/1], Step [3342/3504], Loss: 0.4299\n",
      "Epoch [1/1], Step [3343/3504], Loss: 0.3281\n",
      "Epoch [1/1], Step [3344/3504], Loss: 0.4386\n",
      "Epoch [1/1], Step [3345/3504], Loss: 0.3136\n",
      "Epoch [1/1], Step [3346/3504], Loss: 0.4009\n",
      "Epoch [1/1], Step [3347/3504], Loss: 0.3728\n",
      "Epoch [1/1], Step [3348/3504], Loss: 0.3730\n",
      "Epoch [1/1], Step [3349/3504], Loss: 0.3990\n",
      "Epoch [1/1], Step [3350/3504], Loss: 0.2646\n",
      "Epoch [1/1], Step [3351/3504], Loss: 0.4908\n",
      "Epoch [1/1], Step [3352/3504], Loss: 0.3222\n",
      "Epoch [1/1], Step [3353/3504], Loss: 0.3670\n",
      "Epoch [1/1], Step [3354/3504], Loss: 0.4460\n",
      "Epoch [1/1], Step [3355/3504], Loss: 0.3504\n",
      "Epoch [1/1], Step [3356/3504], Loss: 0.3882\n",
      "Epoch [1/1], Step [3357/3504], Loss: 0.3875\n",
      "Epoch [1/1], Step [3358/3504], Loss: 0.4629\n",
      "Epoch [1/1], Step [3359/3504], Loss: 0.3234\n",
      "Epoch [1/1], Step [3360/3504], Loss: 0.3067\n",
      "Epoch [1/1], Step [3361/3504], Loss: 0.3178\n",
      "Epoch [1/1], Step [3362/3504], Loss: 0.3681\n",
      "Epoch [1/1], Step [3363/3504], Loss: 0.3905\n",
      "Epoch [1/1], Step [3364/3504], Loss: 0.3331\n",
      "Epoch [1/1], Step [3365/3504], Loss: 0.3444\n",
      "Epoch [1/1], Step [3366/3504], Loss: 0.3275\n",
      "Epoch [1/1], Step [3367/3504], Loss: 0.3881\n",
      "Epoch [1/1], Step [3368/3504], Loss: 0.3088\n",
      "Epoch [1/1], Step [3369/3504], Loss: 0.3390\n",
      "Epoch [1/1], Step [3370/3504], Loss: 0.3321\n",
      "Epoch [1/1], Step [3371/3504], Loss: 0.3414\n",
      "Epoch [1/1], Step [3372/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [3373/3504], Loss: 0.3105\n",
      "Epoch [1/1], Step [3374/3504], Loss: 0.4511\n",
      "Epoch [1/1], Step [3375/3504], Loss: 0.4164\n",
      "Epoch [1/1], Step [3376/3504], Loss: 0.3224\n",
      "Epoch [1/1], Step [3377/3504], Loss: 0.2801\n",
      "Epoch [1/1], Step [3378/3504], Loss: 0.3284\n",
      "Epoch [1/1], Step [3379/3504], Loss: 0.4333\n",
      "Epoch [1/1], Step [3380/3504], Loss: 0.3535\n",
      "Epoch [1/1], Step [3381/3504], Loss: 0.3508\n",
      "Epoch [1/1], Step [3382/3504], Loss: 0.3688\n",
      "Epoch [1/1], Step [3383/3504], Loss: 0.2687\n",
      "Epoch [1/1], Step [3384/3504], Loss: 0.3208\n",
      "Epoch [1/1], Step [3385/3504], Loss: 0.4704\n",
      "Epoch [1/1], Step [3386/3504], Loss: 0.3146\n",
      "Epoch [1/1], Step [3387/3504], Loss: 0.3250\n",
      "Epoch [1/1], Step [3388/3504], Loss: 0.2993\n",
      "Epoch [1/1], Step [3389/3504], Loss: 0.4338\n",
      "Epoch [1/1], Step [3390/3504], Loss: 0.4156\n",
      "Epoch [1/1], Step [3391/3504], Loss: 0.3045\n",
      "Epoch [1/1], Step [3392/3504], Loss: 0.4098\n",
      "Epoch [1/1], Step [3393/3504], Loss: 0.2980\n",
      "Epoch [1/1], Step [3394/3504], Loss: 0.4000\n",
      "Epoch [1/1], Step [3395/3504], Loss: 0.2995\n",
      "Epoch [1/1], Step [3396/3504], Loss: 0.4596\n",
      "Epoch [1/1], Step [3397/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [3398/3504], Loss: 0.2773\n",
      "Epoch [1/1], Step [3399/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [3400/3504], Loss: 0.4336\n",
      "Epoch [1/1], Step [3401/3504], Loss: 0.3044\n",
      "Epoch [1/1], Step [3402/3504], Loss: 0.4506\n",
      "Epoch [1/1], Step [3403/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [3404/3504], Loss: 0.3559\n",
      "Epoch [1/1], Step [3405/3504], Loss: 0.2633\n",
      "Epoch [1/1], Step [3406/3504], Loss: 0.4435\n",
      "Epoch [1/1], Step [3407/3504], Loss: 0.3475\n",
      "Epoch [1/1], Step [3408/3504], Loss: 0.3280\n",
      "Epoch [1/1], Step [3409/3504], Loss: 0.3407\n",
      "Epoch [1/1], Step [3410/3504], Loss: 0.3114\n",
      "Epoch [1/1], Step [3411/3504], Loss: 0.3846\n",
      "Epoch [1/1], Step [3412/3504], Loss: 0.3679\n",
      "Epoch [1/1], Step [3413/3504], Loss: 0.3122\n",
      "Epoch [1/1], Step [3414/3504], Loss: 0.3794\n",
      "Epoch [1/1], Step [3415/3504], Loss: 0.3758\n",
      "Epoch [1/1], Step [3416/3504], Loss: 0.3216\n",
      "Epoch [1/1], Step [3417/3504], Loss: 0.3760\n",
      "Epoch [1/1], Step [3418/3504], Loss: 0.3419\n",
      "Epoch [1/1], Step [3419/3504], Loss: 0.3774\n",
      "Epoch [1/1], Step [3420/3504], Loss: 0.3501\n",
      "Epoch [1/1], Step [3421/3504], Loss: 0.3560\n",
      "Epoch [1/1], Step [3422/3504], Loss: 0.3699\n",
      "Epoch [1/1], Step [3423/3504], Loss: 0.3689\n",
      "Epoch [1/1], Step [3424/3504], Loss: 0.3387\n",
      "Epoch [1/1], Step [3425/3504], Loss: 0.4178\n",
      "Epoch [1/1], Step [3426/3504], Loss: 0.3522\n",
      "Epoch [1/1], Step [3427/3504], Loss: 0.4223\n",
      "Epoch [1/1], Step [3428/3504], Loss: 0.3430\n",
      "Epoch [1/1], Step [3429/3504], Loss: 0.3374\n",
      "Epoch [1/1], Step [3430/3504], Loss: 0.2762\n",
      "Epoch [1/1], Step [3431/3504], Loss: 0.3541\n",
      "Epoch [1/1], Step [3432/3504], Loss: 0.2484\n",
      "Epoch [1/1], Step [3433/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [3434/3504], Loss: 0.2481\n",
      "Epoch [1/1], Step [3435/3504], Loss: 0.3792\n",
      "Epoch [1/1], Step [3436/3504], Loss: 0.4622\n",
      "Epoch [1/1], Step [3437/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [3438/3504], Loss: 0.3959\n",
      "Epoch [1/1], Step [3439/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [3440/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [3441/3504], Loss: 0.4017\n",
      "Epoch [1/1], Step [3442/3504], Loss: 0.3839\n",
      "Epoch [1/1], Step [3443/3504], Loss: 0.5406\n",
      "Epoch [1/1], Step [3444/3504], Loss: 0.3544\n",
      "Epoch [1/1], Step [3445/3504], Loss: 0.3529\n",
      "Epoch [1/1], Step [3446/3504], Loss: 0.3648\n",
      "Epoch [1/1], Step [3447/3504], Loss: 0.3498\n",
      "Epoch [1/1], Step [3448/3504], Loss: 0.3512\n",
      "Epoch [1/1], Step [3449/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [3450/3504], Loss: 0.3888\n",
      "Epoch [1/1], Step [3451/3504], Loss: 0.3723\n",
      "Epoch [1/1], Step [3452/3504], Loss: 0.3456\n",
      "Epoch [1/1], Step [3453/3504], Loss: 0.3754\n",
      "Epoch [1/1], Step [3454/3504], Loss: 0.3954\n",
      "Epoch [1/1], Step [3455/3504], Loss: 0.2960\n",
      "Epoch [1/1], Step [3456/3504], Loss: 0.4070\n",
      "Epoch [1/1], Step [3457/3504], Loss: 0.4868\n",
      "Epoch [1/1], Step [3458/3504], Loss: 0.3326\n",
      "Epoch [1/1], Step [3459/3504], Loss: 0.3941\n",
      "Epoch [1/1], Step [3460/3504], Loss: 0.3951\n",
      "Epoch [1/1], Step [3461/3504], Loss: 0.3495\n",
      "Epoch [1/1], Step [3462/3504], Loss: 0.3354\n",
      "Epoch [1/1], Step [3463/3504], Loss: 0.2780\n",
      "Epoch [1/1], Step [3464/3504], Loss: 0.2880\n",
      "Epoch [1/1], Step [3465/3504], Loss: 0.3767\n",
      "Epoch [1/1], Step [3466/3504], Loss: 0.3821\n",
      "Epoch [1/1], Step [3467/3504], Loss: 0.2751\n",
      "Epoch [1/1], Step [3468/3504], Loss: 0.3728\n",
      "Epoch [1/1], Step [3469/3504], Loss: 0.4410\n",
      "Epoch [1/1], Step [3470/3504], Loss: 0.2594\n",
      "Epoch [1/1], Step [3471/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [3472/3504], Loss: 0.2705\n",
      "Epoch [1/1], Step [3473/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [3474/3504], Loss: 0.3213\n",
      "Epoch [1/1], Step [3475/3504], Loss: 0.3372\n",
      "Epoch [1/1], Step [3476/3504], Loss: 0.3586\n",
      "Epoch [1/1], Step [3477/3504], Loss: 0.3670\n",
      "Epoch [1/1], Step [3478/3504], Loss: 0.3265\n",
      "Epoch [1/1], Step [3479/3504], Loss: 0.4057\n",
      "Epoch [1/1], Step [3480/3504], Loss: 0.3915\n",
      "Epoch [1/1], Step [3481/3504], Loss: 0.3501\n",
      "Epoch [1/1], Step [3482/3504], Loss: 0.2855\n",
      "Epoch [1/1], Step [3483/3504], Loss: 0.3070\n",
      "Epoch [1/1], Step [3484/3504], Loss: 0.2915\n",
      "Epoch [1/1], Step [3485/3504], Loss: 0.3278\n",
      "Epoch [1/1], Step [3486/3504], Loss: 0.3186\n",
      "Epoch [1/1], Step [3487/3504], Loss: 0.2746\n",
      "Epoch [1/1], Step [3488/3504], Loss: 0.2652\n",
      "Epoch [1/1], Step [3489/3504], Loss: 0.3132\n",
      "Epoch [1/1], Step [3490/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [3491/3504], Loss: 0.4964\n",
      "Epoch [1/1], Step [3492/3504], Loss: 0.3035\n",
      "Epoch [1/1], Step [3493/3504], Loss: 0.3461\n",
      "Epoch [1/1], Step [3494/3504], Loss: 0.2758\n",
      "Epoch [1/1], Step [3495/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [3496/3504], Loss: 0.3484\n",
      "Epoch [1/1], Step [3497/3504], Loss: 0.2926\n",
      "Epoch [1/1], Step [3498/3504], Loss: 0.2966\n",
      "Epoch [1/1], Step [3499/3504], Loss: 0.4122\n",
      "Epoch [1/1], Step [3500/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [3501/3504], Loss: 0.3607\n",
      "Epoch [1/1], Step [3502/3504], Loss: 0.2697\n",
      "Epoch [1/1], Step [3503/3504], Loss: 0.2757\n",
      "Epoch [1/1], Step [3504/3504], Loss: 0.1609\n",
      "Epoch [1/1], Train Accuracy: 0.8108\n",
      "Epoch [1/1], Test Accuracy: 0.8104\n"
     ]
    }
   ],
   "source": [
    "train_accs, test_accs = trainer(small_model, train_dataloader, test_dataloader, 1, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b71548a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PaperModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(970, 30) # 972 - 2 features\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(30, 30)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(30, 30)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.linear4 = torch.nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.linear4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba315b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_model = PaperModel().to(device)\n",
    "optimizer = torch.optim.RMSprop(paper_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5ef2cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/3504], Loss: 0.5836\n",
      "Epoch [1/1], Step [2/3504], Loss: 0.3663\n",
      "Epoch [1/1], Step [3/3504], Loss: 0.4439\n",
      "Epoch [1/1], Step [4/3504], Loss: 0.3537\n",
      "Epoch [1/1], Step [5/3504], Loss: 0.3597\n",
      "Epoch [1/1], Step [6/3504], Loss: 0.3501\n",
      "Epoch [1/1], Step [7/3504], Loss: 0.4184\n",
      "Epoch [1/1], Step [8/3504], Loss: 0.4446\n",
      "Epoch [1/1], Step [9/3504], Loss: 0.4917\n",
      "Epoch [1/1], Step [10/3504], Loss: 0.3680\n",
      "Epoch [1/1], Step [11/3504], Loss: 0.4641\n",
      "Epoch [1/1], Step [12/3504], Loss: 0.3038\n",
      "Epoch [1/1], Step [13/3504], Loss: 0.3472\n",
      "Epoch [1/1], Step [14/3504], Loss: 0.4634\n",
      "Epoch [1/1], Step [15/3504], Loss: 0.3717\n",
      "Epoch [1/1], Step [16/3504], Loss: 0.5184\n",
      "Epoch [1/1], Step [17/3504], Loss: 0.3817\n",
      "Epoch [1/1], Step [18/3504], Loss: 0.4183\n",
      "Epoch [1/1], Step [19/3504], Loss: 0.3171\n",
      "Epoch [1/1], Step [20/3504], Loss: 0.4870\n",
      "Epoch [1/1], Step [21/3504], Loss: 0.3834\n",
      "Epoch [1/1], Step [22/3504], Loss: 0.3174\n",
      "Epoch [1/1], Step [23/3504], Loss: 0.4284\n",
      "Epoch [1/1], Step [24/3504], Loss: 0.3705\n",
      "Epoch [1/1], Step [25/3504], Loss: 0.4710\n",
      "Epoch [1/1], Step [26/3504], Loss: 0.3475\n",
      "Epoch [1/1], Step [27/3504], Loss: 0.4230\n",
      "Epoch [1/1], Step [28/3504], Loss: 0.4423\n",
      "Epoch [1/1], Step [29/3504], Loss: 0.4101\n",
      "Epoch [1/1], Step [30/3504], Loss: 0.4566\n",
      "Epoch [1/1], Step [31/3504], Loss: 0.4474\n",
      "Epoch [1/1], Step [32/3504], Loss: 0.4784\n",
      "Epoch [1/1], Step [33/3504], Loss: 0.4867\n",
      "Epoch [1/1], Step [34/3504], Loss: 0.3549\n",
      "Epoch [1/1], Step [35/3504], Loss: 0.3704\n",
      "Epoch [1/1], Step [36/3504], Loss: 0.5183\n",
      "Epoch [1/1], Step [37/3504], Loss: 0.5086\n",
      "Epoch [1/1], Step [38/3504], Loss: 0.3340\n",
      "Epoch [1/1], Step [39/3504], Loss: 0.4391\n",
      "Epoch [1/1], Step [40/3504], Loss: 0.4767\n",
      "Epoch [1/1], Step [41/3504], Loss: 0.4091\n",
      "Epoch [1/1], Step [42/3504], Loss: 0.6215\n",
      "Epoch [1/1], Step [43/3504], Loss: 0.4035\n",
      "Epoch [1/1], Step [44/3504], Loss: 0.4381\n",
      "Epoch [1/1], Step [45/3504], Loss: 0.4229\n",
      "Epoch [1/1], Step [46/3504], Loss: 0.4818\n",
      "Epoch [1/1], Step [47/3504], Loss: 0.4193\n",
      "Epoch [1/1], Step [48/3504], Loss: 0.4982\n",
      "Epoch [1/1], Step [49/3504], Loss: 0.4917\n",
      "Epoch [1/1], Step [50/3504], Loss: 0.5594\n",
      "Epoch [1/1], Step [51/3504], Loss: 0.3198\n",
      "Epoch [1/1], Step [52/3504], Loss: 0.4378\n",
      "Epoch [1/1], Step [53/3504], Loss: 0.3692\n",
      "Epoch [1/1], Step [54/3504], Loss: 0.4715\n",
      "Epoch [1/1], Step [55/3504], Loss: 0.4691\n",
      "Epoch [1/1], Step [56/3504], Loss: 0.3881\n",
      "Epoch [1/1], Step [57/3504], Loss: 0.4569\n",
      "Epoch [1/1], Step [58/3504], Loss: 0.5104\n",
      "Epoch [1/1], Step [59/3504], Loss: 0.3791\n",
      "Epoch [1/1], Step [60/3504], Loss: 0.3672\n",
      "Epoch [1/1], Step [61/3504], Loss: 0.5077\n",
      "Epoch [1/1], Step [62/3504], Loss: 0.4427\n",
      "Epoch [1/1], Step [63/3504], Loss: 0.4742\n",
      "Epoch [1/1], Step [64/3504], Loss: 0.3154\n",
      "Epoch [1/1], Step [65/3504], Loss: 0.3733\n",
      "Epoch [1/1], Step [66/3504], Loss: 0.4544\n",
      "Epoch [1/1], Step [67/3504], Loss: 0.4053\n",
      "Epoch [1/1], Step [68/3504], Loss: 0.5075\n",
      "Epoch [1/1], Step [69/3504], Loss: 0.4366\n",
      "Epoch [1/1], Step [70/3504], Loss: 0.4746\n",
      "Epoch [1/1], Step [71/3504], Loss: 0.5086\n",
      "Epoch [1/1], Step [72/3504], Loss: 0.4441\n",
      "Epoch [1/1], Step [73/3504], Loss: 0.5432\n",
      "Epoch [1/1], Step [74/3504], Loss: 0.3830\n",
      "Epoch [1/1], Step [75/3504], Loss: 0.4359\n",
      "Epoch [1/1], Step [76/3504], Loss: 0.4936\n",
      "Epoch [1/1], Step [77/3504], Loss: 0.2939\n",
      "Epoch [1/1], Step [78/3504], Loss: 0.4602\n",
      "Epoch [1/1], Step [79/3504], Loss: 0.6470\n",
      "Epoch [1/1], Step [80/3504], Loss: 0.3421\n",
      "Epoch [1/1], Step [81/3504], Loss: 0.4595\n",
      "Epoch [1/1], Step [82/3504], Loss: 0.3238\n",
      "Epoch [1/1], Step [83/3504], Loss: 0.4082\n",
      "Epoch [1/1], Step [84/3504], Loss: 0.5260\n",
      "Epoch [1/1], Step [85/3504], Loss: 0.2886\n",
      "Epoch [1/1], Step [86/3504], Loss: 0.2971\n",
      "Epoch [1/1], Step [87/3504], Loss: 0.3611\n",
      "Epoch [1/1], Step [88/3504], Loss: 0.3300\n",
      "Epoch [1/1], Step [89/3504], Loss: 0.4957\n",
      "Epoch [1/1], Step [90/3504], Loss: 0.4517\n",
      "Epoch [1/1], Step [91/3504], Loss: 0.4610\n",
      "Epoch [1/1], Step [92/3504], Loss: 0.4370\n",
      "Epoch [1/1], Step [93/3504], Loss: 0.4125\n",
      "Epoch [1/1], Step [94/3504], Loss: 0.4318\n",
      "Epoch [1/1], Step [95/3504], Loss: 0.3824\n",
      "Epoch [1/1], Step [96/3504], Loss: 0.3572\n",
      "Epoch [1/1], Step [97/3504], Loss: 0.5500\n",
      "Epoch [1/1], Step [98/3504], Loss: 0.3191\n",
      "Epoch [1/1], Step [99/3504], Loss: 0.3906\n",
      "Epoch [1/1], Step [100/3504], Loss: 0.4088\n",
      "Epoch [1/1], Step [101/3504], Loss: 0.4328\n",
      "Epoch [1/1], Step [102/3504], Loss: 0.4083\n",
      "Epoch [1/1], Step [103/3504], Loss: 0.3485\n",
      "Epoch [1/1], Step [104/3504], Loss: 0.3315\n",
      "Epoch [1/1], Step [105/3504], Loss: 0.4693\n",
      "Epoch [1/1], Step [106/3504], Loss: 0.4726\n",
      "Epoch [1/1], Step [107/3504], Loss: 0.5311\n",
      "Epoch [1/1], Step [108/3504], Loss: 0.4311\n",
      "Epoch [1/1], Step [109/3504], Loss: 0.4096\n",
      "Epoch [1/1], Step [110/3504], Loss: 0.4754\n",
      "Epoch [1/1], Step [111/3504], Loss: 0.4389\n",
      "Epoch [1/1], Step [112/3504], Loss: 0.4822\n",
      "Epoch [1/1], Step [113/3504], Loss: 0.5745\n",
      "Epoch [1/1], Step [114/3504], Loss: 0.4698\n",
      "Epoch [1/1], Step [115/3504], Loss: 0.3818\n",
      "Epoch [1/1], Step [116/3504], Loss: 0.4383\n",
      "Epoch [1/1], Step [117/3504], Loss: 0.3781\n",
      "Epoch [1/1], Step [118/3504], Loss: 0.4529\n",
      "Epoch [1/1], Step [119/3504], Loss: 0.3998\n",
      "Epoch [1/1], Step [120/3504], Loss: 0.5650\n",
      "Epoch [1/1], Step [121/3504], Loss: 0.4423\n",
      "Epoch [1/1], Step [122/3504], Loss: 0.6479\n",
      "Epoch [1/1], Step [123/3504], Loss: 0.3279\n",
      "Epoch [1/1], Step [124/3504], Loss: 0.5170\n",
      "Epoch [1/1], Step [125/3504], Loss: 0.4571\n",
      "Epoch [1/1], Step [126/3504], Loss: 0.4710\n",
      "Epoch [1/1], Step [127/3504], Loss: 0.4734\n",
      "Epoch [1/1], Step [128/3504], Loss: 0.4571\n",
      "Epoch [1/1], Step [129/3504], Loss: 0.4687\n",
      "Epoch [1/1], Step [130/3504], Loss: 0.4535\n",
      "Epoch [1/1], Step [131/3504], Loss: 0.3890\n",
      "Epoch [1/1], Step [132/3504], Loss: 0.4487\n",
      "Epoch [1/1], Step [133/3504], Loss: 0.4245\n",
      "Epoch [1/1], Step [134/3504], Loss: 0.4596\n",
      "Epoch [1/1], Step [135/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [136/3504], Loss: 0.4498\n",
      "Epoch [1/1], Step [137/3504], Loss: 0.4573\n",
      "Epoch [1/1], Step [138/3504], Loss: 0.4469\n",
      "Epoch [1/1], Step [139/3504], Loss: 0.3844\n",
      "Epoch [1/1], Step [140/3504], Loss: 0.3982\n",
      "Epoch [1/1], Step [141/3504], Loss: 0.4460\n",
      "Epoch [1/1], Step [142/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [143/3504], Loss: 0.4639\n",
      "Epoch [1/1], Step [144/3504], Loss: 0.4952\n",
      "Epoch [1/1], Step [145/3504], Loss: 0.5140\n",
      "Epoch [1/1], Step [146/3504], Loss: 0.5113\n",
      "Epoch [1/1], Step [147/3504], Loss: 0.4222\n",
      "Epoch [1/1], Step [148/3504], Loss: 0.3341\n",
      "Epoch [1/1], Step [149/3504], Loss: 0.3453\n",
      "Epoch [1/1], Step [150/3504], Loss: 0.5590\n",
      "Epoch [1/1], Step [151/3504], Loss: 0.4291\n",
      "Epoch [1/1], Step [152/3504], Loss: 0.3911\n",
      "Epoch [1/1], Step [153/3504], Loss: 0.5258\n",
      "Epoch [1/1], Step [154/3504], Loss: 0.5005\n",
      "Epoch [1/1], Step [155/3504], Loss: 0.3505\n",
      "Epoch [1/1], Step [156/3504], Loss: 0.5748\n",
      "Epoch [1/1], Step [157/3504], Loss: 0.5189\n",
      "Epoch [1/1], Step [158/3504], Loss: 0.3345\n",
      "Epoch [1/1], Step [159/3504], Loss: 0.3755\n",
      "Epoch [1/1], Step [160/3504], Loss: 0.3955\n",
      "Epoch [1/1], Step [161/3504], Loss: 0.5983\n",
      "Epoch [1/1], Step [162/3504], Loss: 0.3568\n",
      "Epoch [1/1], Step [163/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [164/3504], Loss: 0.3148\n",
      "Epoch [1/1], Step [165/3504], Loss: 0.4008\n",
      "Epoch [1/1], Step [166/3504], Loss: 0.4720\n",
      "Epoch [1/1], Step [167/3504], Loss: 0.4419\n",
      "Epoch [1/1], Step [168/3504], Loss: 0.3611\n",
      "Epoch [1/1], Step [169/3504], Loss: 0.4087\n",
      "Epoch [1/1], Step [170/3504], Loss: 0.3528\n",
      "Epoch [1/1], Step [171/3504], Loss: 0.3054\n",
      "Epoch [1/1], Step [172/3504], Loss: 0.4646\n",
      "Epoch [1/1], Step [173/3504], Loss: 0.3663\n",
      "Epoch [1/1], Step [174/3504], Loss: 0.7070\n",
      "Epoch [1/1], Step [175/3504], Loss: 0.3866\n",
      "Epoch [1/1], Step [176/3504], Loss: 0.4435\n",
      "Epoch [1/1], Step [177/3504], Loss: 0.4479\n",
      "Epoch [1/1], Step [178/3504], Loss: 0.4673\n",
      "Epoch [1/1], Step [179/3504], Loss: 0.4135\n",
      "Epoch [1/1], Step [180/3504], Loss: 0.4656\n",
      "Epoch [1/1], Step [181/3504], Loss: 0.4181\n",
      "Epoch [1/1], Step [182/3504], Loss: 0.3883\n",
      "Epoch [1/1], Step [183/3504], Loss: 0.4762\n",
      "Epoch [1/1], Step [184/3504], Loss: 0.5248\n",
      "Epoch [1/1], Step [185/3504], Loss: 0.4875\n",
      "Epoch [1/1], Step [186/3504], Loss: 0.4490\n",
      "Epoch [1/1], Step [187/3504], Loss: 0.4225\n",
      "Epoch [1/1], Step [188/3504], Loss: 0.4589\n",
      "Epoch [1/1], Step [189/3504], Loss: 0.4433\n",
      "Epoch [1/1], Step [190/3504], Loss: 0.2747\n",
      "Epoch [1/1], Step [191/3504], Loss: 0.4827\n",
      "Epoch [1/1], Step [192/3504], Loss: 0.3339\n",
      "Epoch [1/1], Step [193/3504], Loss: 0.5138\n",
      "Epoch [1/1], Step [194/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [195/3504], Loss: 0.4470\n",
      "Epoch [1/1], Step [196/3504], Loss: 0.3887\n",
      "Epoch [1/1], Step [197/3504], Loss: 0.3585\n",
      "Epoch [1/1], Step [198/3504], Loss: 0.3754\n",
      "Epoch [1/1], Step [199/3504], Loss: 0.5682\n",
      "Epoch [1/1], Step [200/3504], Loss: 0.5868\n",
      "Epoch [1/1], Step [201/3504], Loss: 0.4375\n",
      "Epoch [1/1], Step [202/3504], Loss: 0.4264\n",
      "Epoch [1/1], Step [203/3504], Loss: 0.4743\n",
      "Epoch [1/1], Step [204/3504], Loss: 0.3989\n",
      "Epoch [1/1], Step [205/3504], Loss: 0.3903\n",
      "Epoch [1/1], Step [206/3504], Loss: 0.4313\n",
      "Epoch [1/1], Step [207/3504], Loss: 0.4699\n",
      "Epoch [1/1], Step [208/3504], Loss: 0.3354\n",
      "Epoch [1/1], Step [209/3504], Loss: 0.3694\n",
      "Epoch [1/1], Step [210/3504], Loss: 0.4091\n",
      "Epoch [1/1], Step [211/3504], Loss: 0.4324\n",
      "Epoch [1/1], Step [212/3504], Loss: 0.5402\n",
      "Epoch [1/1], Step [213/3504], Loss: 0.4601\n",
      "Epoch [1/1], Step [214/3504], Loss: 0.4374\n",
      "Epoch [1/1], Step [215/3504], Loss: 0.4576\n",
      "Epoch [1/1], Step [216/3504], Loss: 0.5238\n",
      "Epoch [1/1], Step [217/3504], Loss: 0.4197\n",
      "Epoch [1/1], Step [218/3504], Loss: 0.4778\n",
      "Epoch [1/1], Step [219/3504], Loss: 0.3787\n",
      "Epoch [1/1], Step [220/3504], Loss: 0.4063\n",
      "Epoch [1/1], Step [221/3504], Loss: 0.5357\n",
      "Epoch [1/1], Step [222/3504], Loss: 0.4246\n",
      "Epoch [1/1], Step [223/3504], Loss: 0.3902\n",
      "Epoch [1/1], Step [224/3504], Loss: 0.3842\n",
      "Epoch [1/1], Step [225/3504], Loss: 0.3857\n",
      "Epoch [1/1], Step [226/3504], Loss: 0.3701\n",
      "Epoch [1/1], Step [227/3504], Loss: 0.3786\n",
      "Epoch [1/1], Step [228/3504], Loss: 0.3671\n",
      "Epoch [1/1], Step [229/3504], Loss: 0.4004\n",
      "Epoch [1/1], Step [230/3504], Loss: 0.4705\n",
      "Epoch [1/1], Step [231/3504], Loss: 0.3079\n",
      "Epoch [1/1], Step [232/3504], Loss: 0.4003\n",
      "Epoch [1/1], Step [233/3504], Loss: 0.4340\n",
      "Epoch [1/1], Step [234/3504], Loss: 0.4787\n",
      "Epoch [1/1], Step [235/3504], Loss: 0.4466\n",
      "Epoch [1/1], Step [236/3504], Loss: 0.5352\n",
      "Epoch [1/1], Step [237/3504], Loss: 0.4497\n",
      "Epoch [1/1], Step [238/3504], Loss: 0.3805\n",
      "Epoch [1/1], Step [239/3504], Loss: 0.3829\n",
      "Epoch [1/1], Step [240/3504], Loss: 0.4044\n",
      "Epoch [1/1], Step [241/3504], Loss: 0.3996\n",
      "Epoch [1/1], Step [242/3504], Loss: 0.3643\n",
      "Epoch [1/1], Step [243/3504], Loss: 0.3965\n",
      "Epoch [1/1], Step [244/3504], Loss: 0.3799\n",
      "Epoch [1/1], Step [245/3504], Loss: 0.3826\n",
      "Epoch [1/1], Step [246/3504], Loss: 0.3957\n",
      "Epoch [1/1], Step [247/3504], Loss: 0.3439\n",
      "Epoch [1/1], Step [248/3504], Loss: 0.4105\n",
      "Epoch [1/1], Step [249/3504], Loss: 0.5553\n",
      "Epoch [1/1], Step [250/3504], Loss: 0.3901\n",
      "Epoch [1/1], Step [251/3504], Loss: 0.3337\n",
      "Epoch [1/1], Step [252/3504], Loss: 0.4430\n",
      "Epoch [1/1], Step [253/3504], Loss: 0.5304\n",
      "Epoch [1/1], Step [254/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [255/3504], Loss: 0.3540\n",
      "Epoch [1/1], Step [256/3504], Loss: 0.4229\n",
      "Epoch [1/1], Step [257/3504], Loss: 0.4127\n",
      "Epoch [1/1], Step [258/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [259/3504], Loss: 0.4513\n",
      "Epoch [1/1], Step [260/3504], Loss: 0.4155\n",
      "Epoch [1/1], Step [261/3504], Loss: 0.3114\n",
      "Epoch [1/1], Step [262/3504], Loss: 0.3812\n",
      "Epoch [1/1], Step [263/3504], Loss: 0.4959\n",
      "Epoch [1/1], Step [264/3504], Loss: 0.3787\n",
      "Epoch [1/1], Step [265/3504], Loss: 0.3859\n",
      "Epoch [1/1], Step [266/3504], Loss: 0.4161\n",
      "Epoch [1/1], Step [267/3504], Loss: 0.4708\n",
      "Epoch [1/1], Step [268/3504], Loss: 0.4800\n",
      "Epoch [1/1], Step [269/3504], Loss: 0.4662\n",
      "Epoch [1/1], Step [270/3504], Loss: 0.4362\n",
      "Epoch [1/1], Step [271/3504], Loss: 0.4439\n",
      "Epoch [1/1], Step [272/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [273/3504], Loss: 0.3916\n",
      "Epoch [1/1], Step [274/3504], Loss: 0.4230\n",
      "Epoch [1/1], Step [275/3504], Loss: 0.3928\n",
      "Epoch [1/1], Step [276/3504], Loss: 0.4286\n",
      "Epoch [1/1], Step [277/3504], Loss: 0.4053\n",
      "Epoch [1/1], Step [278/3504], Loss: 0.5369\n",
      "Epoch [1/1], Step [279/3504], Loss: 0.4540\n",
      "Epoch [1/1], Step [280/3504], Loss: 0.4814\n",
      "Epoch [1/1], Step [281/3504], Loss: 0.4584\n",
      "Epoch [1/1], Step [282/3504], Loss: 0.4567\n",
      "Epoch [1/1], Step [283/3504], Loss: 0.4078\n",
      "Epoch [1/1], Step [284/3504], Loss: 0.5049\n",
      "Epoch [1/1], Step [285/3504], Loss: 0.5348\n",
      "Epoch [1/1], Step [286/3504], Loss: 0.4332\n",
      "Epoch [1/1], Step [287/3504], Loss: 0.5102\n",
      "Epoch [1/1], Step [288/3504], Loss: 0.4574\n",
      "Epoch [1/1], Step [289/3504], Loss: 0.3967\n",
      "Epoch [1/1], Step [290/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [291/3504], Loss: 0.3570\n",
      "Epoch [1/1], Step [292/3504], Loss: 0.3919\n",
      "Epoch [1/1], Step [293/3504], Loss: 0.4357\n",
      "Epoch [1/1], Step [294/3504], Loss: 0.4723\n",
      "Epoch [1/1], Step [295/3504], Loss: 0.2979\n",
      "Epoch [1/1], Step [296/3504], Loss: 0.5550\n",
      "Epoch [1/1], Step [297/3504], Loss: 0.3402\n",
      "Epoch [1/1], Step [298/3504], Loss: 0.3995\n",
      "Epoch [1/1], Step [299/3504], Loss: 0.4628\n",
      "Epoch [1/1], Step [300/3504], Loss: 0.4451\n",
      "Epoch [1/1], Step [301/3504], Loss: 0.4555\n",
      "Epoch [1/1], Step [302/3504], Loss: 0.3877\n",
      "Epoch [1/1], Step [303/3504], Loss: 0.4269\n",
      "Epoch [1/1], Step [304/3504], Loss: 0.3817\n",
      "Epoch [1/1], Step [305/3504], Loss: 0.5032\n",
      "Epoch [1/1], Step [306/3504], Loss: 0.4698\n",
      "Epoch [1/1], Step [307/3504], Loss: 0.4545\n",
      "Epoch [1/1], Step [308/3504], Loss: 0.3972\n",
      "Epoch [1/1], Step [309/3504], Loss: 0.5060\n",
      "Epoch [1/1], Step [310/3504], Loss: 0.4248\n",
      "Epoch [1/1], Step [311/3504], Loss: 0.4712\n",
      "Epoch [1/1], Step [312/3504], Loss: 0.4832\n",
      "Epoch [1/1], Step [313/3504], Loss: 0.4989\n",
      "Epoch [1/1], Step [314/3504], Loss: 0.3829\n",
      "Epoch [1/1], Step [315/3504], Loss: 0.3798\n",
      "Epoch [1/1], Step [316/3504], Loss: 0.4146\n",
      "Epoch [1/1], Step [317/3504], Loss: 0.4475\n",
      "Epoch [1/1], Step [318/3504], Loss: 0.3243\n",
      "Epoch [1/1], Step [319/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [320/3504], Loss: 0.5535\n",
      "Epoch [1/1], Step [321/3504], Loss: 0.3158\n",
      "Epoch [1/1], Step [322/3504], Loss: 0.2922\n",
      "Epoch [1/1], Step [323/3504], Loss: 0.3213\n",
      "Epoch [1/1], Step [324/3504], Loss: 0.3911\n",
      "Epoch [1/1], Step [325/3504], Loss: 0.5395\n",
      "Epoch [1/1], Step [326/3504], Loss: 0.4284\n",
      "Epoch [1/1], Step [327/3504], Loss: 0.4685\n",
      "Epoch [1/1], Step [328/3504], Loss: 0.5131\n",
      "Epoch [1/1], Step [329/3504], Loss: 0.4109\n",
      "Epoch [1/1], Step [330/3504], Loss: 0.4269\n",
      "Epoch [1/1], Step [331/3504], Loss: 0.4829\n",
      "Epoch [1/1], Step [332/3504], Loss: 0.3685\n",
      "Epoch [1/1], Step [333/3504], Loss: 0.2712\n",
      "Epoch [1/1], Step [334/3504], Loss: 0.5419\n",
      "Epoch [1/1], Step [335/3504], Loss: 0.4882\n",
      "Epoch [1/1], Step [336/3504], Loss: 0.4805\n",
      "Epoch [1/1], Step [337/3504], Loss: 0.5214\n",
      "Epoch [1/1], Step [338/3504], Loss: 0.3984\n",
      "Epoch [1/1], Step [339/3504], Loss: 0.4139\n",
      "Epoch [1/1], Step [340/3504], Loss: 0.5017\n",
      "Epoch [1/1], Step [341/3504], Loss: 0.3690\n",
      "Epoch [1/1], Step [342/3504], Loss: 0.4290\n",
      "Epoch [1/1], Step [343/3504], Loss: 0.3919\n",
      "Epoch [1/1], Step [344/3504], Loss: 0.4344\n",
      "Epoch [1/1], Step [345/3504], Loss: 0.4177\n",
      "Epoch [1/1], Step [346/3504], Loss: 0.4329\n",
      "Epoch [1/1], Step [347/3504], Loss: 0.5115\n",
      "Epoch [1/1], Step [348/3504], Loss: 0.4143\n",
      "Epoch [1/1], Step [349/3504], Loss: 0.4956\n",
      "Epoch [1/1], Step [350/3504], Loss: 0.4787\n",
      "Epoch [1/1], Step [351/3504], Loss: 0.5289\n",
      "Epoch [1/1], Step [352/3504], Loss: 0.4227\n",
      "Epoch [1/1], Step [353/3504], Loss: 0.3235\n",
      "Epoch [1/1], Step [354/3504], Loss: 0.4091\n",
      "Epoch [1/1], Step [355/3504], Loss: 0.5861\n",
      "Epoch [1/1], Step [356/3504], Loss: 0.4257\n",
      "Epoch [1/1], Step [357/3504], Loss: 0.4536\n",
      "Epoch [1/1], Step [358/3504], Loss: 0.4550\n",
      "Epoch [1/1], Step [359/3504], Loss: 0.4391\n",
      "Epoch [1/1], Step [360/3504], Loss: 0.3673\n",
      "Epoch [1/1], Step [361/3504], Loss: 0.3400\n",
      "Epoch [1/1], Step [362/3504], Loss: 0.3428\n",
      "Epoch [1/1], Step [363/3504], Loss: 0.4982\n",
      "Epoch [1/1], Step [364/3504], Loss: 0.4201\n",
      "Epoch [1/1], Step [365/3504], Loss: 0.4787\n",
      "Epoch [1/1], Step [366/3504], Loss: 0.3111\n",
      "Epoch [1/1], Step [367/3504], Loss: 0.4158\n",
      "Epoch [1/1], Step [368/3504], Loss: 0.4512\n",
      "Epoch [1/1], Step [369/3504], Loss: 0.4041\n",
      "Epoch [1/1], Step [370/3504], Loss: 0.3415\n",
      "Epoch [1/1], Step [371/3504], Loss: 0.4434\n",
      "Epoch [1/1], Step [372/3504], Loss: 0.4252\n",
      "Epoch [1/1], Step [373/3504], Loss: 0.4338\n",
      "Epoch [1/1], Step [374/3504], Loss: 0.4396\n",
      "Epoch [1/1], Step [375/3504], Loss: 0.3801\n",
      "Epoch [1/1], Step [376/3504], Loss: 0.4065\n",
      "Epoch [1/1], Step [377/3504], Loss: 0.4159\n",
      "Epoch [1/1], Step [378/3504], Loss: 0.3873\n",
      "Epoch [1/1], Step [379/3504], Loss: 0.4390\n",
      "Epoch [1/1], Step [380/3504], Loss: 0.3999\n",
      "Epoch [1/1], Step [381/3504], Loss: 0.5050\n",
      "Epoch [1/1], Step [382/3504], Loss: 0.4093\n",
      "Epoch [1/1], Step [383/3504], Loss: 0.5238\n",
      "Epoch [1/1], Step [384/3504], Loss: 0.4826\n",
      "Epoch [1/1], Step [385/3504], Loss: 0.4887\n",
      "Epoch [1/1], Step [386/3504], Loss: 0.3384\n",
      "Epoch [1/1], Step [387/3504], Loss: 0.3506\n",
      "Epoch [1/1], Step [388/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [389/3504], Loss: 0.3500\n",
      "Epoch [1/1], Step [390/3504], Loss: 0.5736\n",
      "Epoch [1/1], Step [391/3504], Loss: 0.4204\n",
      "Epoch [1/1], Step [392/3504], Loss: 0.4451\n",
      "Epoch [1/1], Step [393/3504], Loss: 0.4506\n",
      "Epoch [1/1], Step [394/3504], Loss: 0.3513\n",
      "Epoch [1/1], Step [395/3504], Loss: 0.3315\n",
      "Epoch [1/1], Step [396/3504], Loss: 0.3546\n",
      "Epoch [1/1], Step [397/3504], Loss: 0.4383\n",
      "Epoch [1/1], Step [398/3504], Loss: 0.4487\n",
      "Epoch [1/1], Step [399/3504], Loss: 0.3413\n",
      "Epoch [1/1], Step [400/3504], Loss: 0.3727\n",
      "Epoch [1/1], Step [401/3504], Loss: 0.4328\n",
      "Epoch [1/1], Step [402/3504], Loss: 0.4488\n",
      "Epoch [1/1], Step [403/3504], Loss: 0.5966\n",
      "Epoch [1/1], Step [404/3504], Loss: 0.4626\n",
      "Epoch [1/1], Step [405/3504], Loss: 0.3454\n",
      "Epoch [1/1], Step [406/3504], Loss: 0.4191\n",
      "Epoch [1/1], Step [407/3504], Loss: 0.4259\n",
      "Epoch [1/1], Step [408/3504], Loss: 0.3327\n",
      "Epoch [1/1], Step [409/3504], Loss: 0.3847\n",
      "Epoch [1/1], Step [410/3504], Loss: 0.5006\n",
      "Epoch [1/1], Step [411/3504], Loss: 0.4683\n",
      "Epoch [1/1], Step [412/3504], Loss: 0.4308\n",
      "Epoch [1/1], Step [413/3504], Loss: 0.4802\n",
      "Epoch [1/1], Step [414/3504], Loss: 0.4542\n",
      "Epoch [1/1], Step [415/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [416/3504], Loss: 0.4162\n",
      "Epoch [1/1], Step [417/3504], Loss: 0.3735\n",
      "Epoch [1/1], Step [418/3504], Loss: 0.4634\n",
      "Epoch [1/1], Step [419/3504], Loss: 0.4737\n",
      "Epoch [1/1], Step [420/3504], Loss: 0.4404\n",
      "Epoch [1/1], Step [421/3504], Loss: 0.3851\n",
      "Epoch [1/1], Step [422/3504], Loss: 0.4935\n",
      "Epoch [1/1], Step [423/3504], Loss: 0.4159\n",
      "Epoch [1/1], Step [424/3504], Loss: 0.3666\n",
      "Epoch [1/1], Step [425/3504], Loss: 0.5386\n",
      "Epoch [1/1], Step [426/3504], Loss: 0.5322\n",
      "Epoch [1/1], Step [427/3504], Loss: 0.5679\n",
      "Epoch [1/1], Step [428/3504], Loss: 0.4385\n",
      "Epoch [1/1], Step [429/3504], Loss: 0.3533\n",
      "Epoch [1/1], Step [430/3504], Loss: 0.2657\n",
      "Epoch [1/1], Step [431/3504], Loss: 0.4608\n",
      "Epoch [1/1], Step [432/3504], Loss: 0.3308\n",
      "Epoch [1/1], Step [433/3504], Loss: 0.4461\n",
      "Epoch [1/1], Step [434/3504], Loss: 0.6197\n",
      "Epoch [1/1], Step [435/3504], Loss: 0.4713\n",
      "Epoch [1/1], Step [436/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [437/3504], Loss: 0.4923\n",
      "Epoch [1/1], Step [438/3504], Loss: 0.3616\n",
      "Epoch [1/1], Step [439/3504], Loss: 0.4487\n",
      "Epoch [1/1], Step [440/3504], Loss: 0.3324\n",
      "Epoch [1/1], Step [441/3504], Loss: 0.3784\n",
      "Epoch [1/1], Step [442/3504], Loss: 0.4225\n",
      "Epoch [1/1], Step [443/3504], Loss: 0.4241\n",
      "Epoch [1/1], Step [444/3504], Loss: 0.4204\n",
      "Epoch [1/1], Step [445/3504], Loss: 0.4263\n",
      "Epoch [1/1], Step [446/3504], Loss: 0.4637\n",
      "Epoch [1/1], Step [447/3504], Loss: 0.4232\n",
      "Epoch [1/1], Step [448/3504], Loss: 0.4883\n",
      "Epoch [1/1], Step [449/3504], Loss: 0.3861\n",
      "Epoch [1/1], Step [450/3504], Loss: 0.4840\n",
      "Epoch [1/1], Step [451/3504], Loss: 0.3501\n",
      "Epoch [1/1], Step [452/3504], Loss: 0.3788\n",
      "Epoch [1/1], Step [453/3504], Loss: 0.4770\n",
      "Epoch [1/1], Step [454/3504], Loss: 0.3196\n",
      "Epoch [1/1], Step [455/3504], Loss: 0.4544\n",
      "Epoch [1/1], Step [456/3504], Loss: 0.3385\n",
      "Epoch [1/1], Step [457/3504], Loss: 0.3890\n",
      "Epoch [1/1], Step [458/3504], Loss: 0.4902\n",
      "Epoch [1/1], Step [459/3504], Loss: 0.3119\n",
      "Epoch [1/1], Step [460/3504], Loss: 0.3108\n",
      "Epoch [1/1], Step [461/3504], Loss: 0.4446\n",
      "Epoch [1/1], Step [462/3504], Loss: 0.5015\n",
      "Epoch [1/1], Step [463/3504], Loss: 0.4710\n",
      "Epoch [1/1], Step [464/3504], Loss: 0.5280\n",
      "Epoch [1/1], Step [465/3504], Loss: 0.5461\n",
      "Epoch [1/1], Step [466/3504], Loss: 0.4639\n",
      "Epoch [1/1], Step [467/3504], Loss: 0.4251\n",
      "Epoch [1/1], Step [468/3504], Loss: 0.4906\n",
      "Epoch [1/1], Step [469/3504], Loss: 0.4810\n",
      "Epoch [1/1], Step [470/3504], Loss: 0.3551\n",
      "Epoch [1/1], Step [471/3504], Loss: 0.5054\n",
      "Epoch [1/1], Step [472/3504], Loss: 0.3928\n",
      "Epoch [1/1], Step [473/3504], Loss: 0.5514\n",
      "Epoch [1/1], Step [474/3504], Loss: 0.4583\n",
      "Epoch [1/1], Step [475/3504], Loss: 0.4355\n",
      "Epoch [1/1], Step [476/3504], Loss: 0.3300\n",
      "Epoch [1/1], Step [477/3504], Loss: 0.5550\n",
      "Epoch [1/1], Step [478/3504], Loss: 0.4191\n",
      "Epoch [1/1], Step [479/3504], Loss: 0.5493\n",
      "Epoch [1/1], Step [480/3504], Loss: 0.5014\n",
      "Epoch [1/1], Step [481/3504], Loss: 0.4385\n",
      "Epoch [1/1], Step [482/3504], Loss: 0.5326\n",
      "Epoch [1/1], Step [483/3504], Loss: 0.3969\n",
      "Epoch [1/1], Step [484/3504], Loss: 0.5098\n",
      "Epoch [1/1], Step [485/3504], Loss: 0.3776\n",
      "Epoch [1/1], Step [486/3504], Loss: 0.3322\n",
      "Epoch [1/1], Step [487/3504], Loss: 0.4766\n",
      "Epoch [1/1], Step [488/3504], Loss: 0.4583\n",
      "Epoch [1/1], Step [489/3504], Loss: 0.5740\n",
      "Epoch [1/1], Step [490/3504], Loss: 0.4245\n",
      "Epoch [1/1], Step [491/3504], Loss: 0.3620\n",
      "Epoch [1/1], Step [492/3504], Loss: 0.4962\n",
      "Epoch [1/1], Step [493/3504], Loss: 0.4432\n",
      "Epoch [1/1], Step [494/3504], Loss: 0.3925\n",
      "Epoch [1/1], Step [495/3504], Loss: 0.3971\n",
      "Epoch [1/1], Step [496/3504], Loss: 0.4976\n",
      "Epoch [1/1], Step [497/3504], Loss: 0.5557\n",
      "Epoch [1/1], Step [498/3504], Loss: 0.3676\n",
      "Epoch [1/1], Step [499/3504], Loss: 0.3476\n",
      "Epoch [1/1], Step [500/3504], Loss: 0.2686\n",
      "Epoch [1/1], Step [501/3504], Loss: 0.5238\n",
      "Epoch [1/1], Step [502/3504], Loss: 0.4349\n",
      "Epoch [1/1], Step [503/3504], Loss: 0.4743\n",
      "Epoch [1/1], Step [504/3504], Loss: 0.4300\n",
      "Epoch [1/1], Step [505/3504], Loss: 0.4783\n",
      "Epoch [1/1], Step [506/3504], Loss: 0.3695\n",
      "Epoch [1/1], Step [507/3504], Loss: 0.2965\n",
      "Epoch [1/1], Step [508/3504], Loss: 0.3975\n",
      "Epoch [1/1], Step [509/3504], Loss: 0.3630\n",
      "Epoch [1/1], Step [510/3504], Loss: 0.4191\n",
      "Epoch [1/1], Step [511/3504], Loss: 0.4438\n",
      "Epoch [1/1], Step [512/3504], Loss: 0.4275\n",
      "Epoch [1/1], Step [513/3504], Loss: 0.5420\n",
      "Epoch [1/1], Step [514/3504], Loss: 0.5483\n",
      "Epoch [1/1], Step [515/3504], Loss: 0.5451\n",
      "Epoch [1/1], Step [516/3504], Loss: 0.4496\n",
      "Epoch [1/1], Step [517/3504], Loss: 0.4069\n",
      "Epoch [1/1], Step [518/3504], Loss: 0.4408\n",
      "Epoch [1/1], Step [519/3504], Loss: 0.4252\n",
      "Epoch [1/1], Step [520/3504], Loss: 0.5978\n",
      "Epoch [1/1], Step [521/3504], Loss: 0.5552\n",
      "Epoch [1/1], Step [522/3504], Loss: 0.4424\n",
      "Epoch [1/1], Step [523/3504], Loss: 0.4192\n",
      "Epoch [1/1], Step [524/3504], Loss: 0.4651\n",
      "Epoch [1/1], Step [525/3504], Loss: 0.4255\n",
      "Epoch [1/1], Step [526/3504], Loss: 0.4418\n",
      "Epoch [1/1], Step [527/3504], Loss: 0.5194\n",
      "Epoch [1/1], Step [528/3504], Loss: 0.3131\n",
      "Epoch [1/1], Step [529/3504], Loss: 0.4743\n",
      "Epoch [1/1], Step [530/3504], Loss: 0.4787\n",
      "Epoch [1/1], Step [531/3504], Loss: 0.4436\n",
      "Epoch [1/1], Step [532/3504], Loss: 0.4615\n",
      "Epoch [1/1], Step [533/3504], Loss: 0.5645\n",
      "Epoch [1/1], Step [534/3504], Loss: 0.4786\n",
      "Epoch [1/1], Step [535/3504], Loss: 0.5527\n",
      "Epoch [1/1], Step [536/3504], Loss: 0.4376\n",
      "Epoch [1/1], Step [537/3504], Loss: 0.5293\n",
      "Epoch [1/1], Step [538/3504], Loss: 0.4205\n",
      "Epoch [1/1], Step [539/3504], Loss: 0.3538\n",
      "Epoch [1/1], Step [540/3504], Loss: 0.4270\n",
      "Epoch [1/1], Step [541/3504], Loss: 0.3499\n",
      "Epoch [1/1], Step [542/3504], Loss: 0.4623\n",
      "Epoch [1/1], Step [543/3504], Loss: 0.5930\n",
      "Epoch [1/1], Step [544/3504], Loss: 0.3700\n",
      "Epoch [1/1], Step [545/3504], Loss: 0.3810\n",
      "Epoch [1/1], Step [546/3504], Loss: 0.4340\n",
      "Epoch [1/1], Step [547/3504], Loss: 0.4437\n",
      "Epoch [1/1], Step [548/3504], Loss: 0.4823\n",
      "Epoch [1/1], Step [549/3504], Loss: 0.4881\n",
      "Epoch [1/1], Step [550/3504], Loss: 0.4048\n",
      "Epoch [1/1], Step [551/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [552/3504], Loss: 0.4320\n",
      "Epoch [1/1], Step [553/3504], Loss: 0.3907\n",
      "Epoch [1/1], Step [554/3504], Loss: 0.3917\n",
      "Epoch [1/1], Step [555/3504], Loss: 0.3916\n",
      "Epoch [1/1], Step [556/3504], Loss: 0.4252\n",
      "Epoch [1/1], Step [557/3504], Loss: 0.6240\n",
      "Epoch [1/1], Step [558/3504], Loss: 0.5358\n",
      "Epoch [1/1], Step [559/3504], Loss: 0.5356\n",
      "Epoch [1/1], Step [560/3504], Loss: 0.4858\n",
      "Epoch [1/1], Step [561/3504], Loss: 0.5273\n",
      "Epoch [1/1], Step [562/3504], Loss: 0.3707\n",
      "Epoch [1/1], Step [563/3504], Loss: 0.5233\n",
      "Epoch [1/1], Step [564/3504], Loss: 0.4169\n",
      "Epoch [1/1], Step [565/3504], Loss: 0.4278\n",
      "Epoch [1/1], Step [566/3504], Loss: 0.4493\n",
      "Epoch [1/1], Step [567/3504], Loss: 0.5760\n",
      "Epoch [1/1], Step [568/3504], Loss: 0.6000\n",
      "Epoch [1/1], Step [569/3504], Loss: 0.4391\n",
      "Epoch [1/1], Step [570/3504], Loss: 0.5095\n",
      "Epoch [1/1], Step [571/3504], Loss: 0.4066\n",
      "Epoch [1/1], Step [572/3504], Loss: 0.4889\n",
      "Epoch [1/1], Step [573/3504], Loss: 0.4644\n",
      "Epoch [1/1], Step [574/3504], Loss: 0.3546\n",
      "Epoch [1/1], Step [575/3504], Loss: 0.3619\n",
      "Epoch [1/1], Step [576/3504], Loss: 0.4507\n",
      "Epoch [1/1], Step [577/3504], Loss: 0.5156\n",
      "Epoch [1/1], Step [578/3504], Loss: 0.4159\n",
      "Epoch [1/1], Step [579/3504], Loss: 0.3809\n",
      "Epoch [1/1], Step [580/3504], Loss: 0.3645\n",
      "Epoch [1/1], Step [581/3504], Loss: 0.3570\n",
      "Epoch [1/1], Step [582/3504], Loss: 0.4132\n",
      "Epoch [1/1], Step [583/3504], Loss: 0.3003\n",
      "Epoch [1/1], Step [584/3504], Loss: 0.3395\n",
      "Epoch [1/1], Step [585/3504], Loss: 0.4244\n",
      "Epoch [1/1], Step [586/3504], Loss: 0.4912\n",
      "Epoch [1/1], Step [587/3504], Loss: 0.4565\n",
      "Epoch [1/1], Step [588/3504], Loss: 0.5064\n",
      "Epoch [1/1], Step [589/3504], Loss: 0.3274\n",
      "Epoch [1/1], Step [590/3504], Loss: 0.4144\n",
      "Epoch [1/1], Step [591/3504], Loss: 0.3925\n",
      "Epoch [1/1], Step [592/3504], Loss: 0.3995\n",
      "Epoch [1/1], Step [593/3504], Loss: 0.3610\n",
      "Epoch [1/1], Step [594/3504], Loss: 0.4897\n",
      "Epoch [1/1], Step [595/3504], Loss: 0.4571\n",
      "Epoch [1/1], Step [596/3504], Loss: 0.4785\n",
      "Epoch [1/1], Step [597/3504], Loss: 0.3423\n",
      "Epoch [1/1], Step [598/3504], Loss: 0.4774\n",
      "Epoch [1/1], Step [599/3504], Loss: 0.6525\n",
      "Epoch [1/1], Step [600/3504], Loss: 0.4777\n",
      "Epoch [1/1], Step [601/3504], Loss: 0.5354\n",
      "Epoch [1/1], Step [602/3504], Loss: 0.3204\n",
      "Epoch [1/1], Step [603/3504], Loss: 0.4762\n",
      "Epoch [1/1], Step [604/3504], Loss: 0.4494\n",
      "Epoch [1/1], Step [605/3504], Loss: 0.3384\n",
      "Epoch [1/1], Step [606/3504], Loss: 0.3854\n",
      "Epoch [1/1], Step [607/3504], Loss: 0.4625\n",
      "Epoch [1/1], Step [608/3504], Loss: 0.4341\n",
      "Epoch [1/1], Step [609/3504], Loss: 0.4533\n",
      "Epoch [1/1], Step [610/3504], Loss: 0.4197\n",
      "Epoch [1/1], Step [611/3504], Loss: 0.3105\n",
      "Epoch [1/1], Step [612/3504], Loss: 0.3618\n",
      "Epoch [1/1], Step [613/3504], Loss: 0.5132\n",
      "Epoch [1/1], Step [614/3504], Loss: 0.5103\n",
      "Epoch [1/1], Step [615/3504], Loss: 0.4763\n",
      "Epoch [1/1], Step [616/3504], Loss: 0.5082\n",
      "Epoch [1/1], Step [617/3504], Loss: 0.3263\n",
      "Epoch [1/1], Step [618/3504], Loss: 0.3603\n",
      "Epoch [1/1], Step [619/3504], Loss: 0.4233\n",
      "Epoch [1/1], Step [620/3504], Loss: 0.4551\n",
      "Epoch [1/1], Step [621/3504], Loss: 0.5012\n",
      "Epoch [1/1], Step [622/3504], Loss: 0.5709\n",
      "Epoch [1/1], Step [623/3504], Loss: 0.4459\n",
      "Epoch [1/1], Step [624/3504], Loss: 0.4200\n",
      "Epoch [1/1], Step [625/3504], Loss: 0.3733\n",
      "Epoch [1/1], Step [626/3504], Loss: 0.3151\n",
      "Epoch [1/1], Step [627/3504], Loss: 0.4241\n",
      "Epoch [1/1], Step [628/3504], Loss: 0.4423\n",
      "Epoch [1/1], Step [629/3504], Loss: 0.4304\n",
      "Epoch [1/1], Step [630/3504], Loss: 0.3530\n",
      "Epoch [1/1], Step [631/3504], Loss: 0.4482\n",
      "Epoch [1/1], Step [632/3504], Loss: 0.3386\n",
      "Epoch [1/1], Step [633/3504], Loss: 0.4428\n",
      "Epoch [1/1], Step [634/3504], Loss: 0.4588\n",
      "Epoch [1/1], Step [635/3504], Loss: 0.3344\n",
      "Epoch [1/1], Step [636/3504], Loss: 0.3990\n",
      "Epoch [1/1], Step [637/3504], Loss: 0.3442\n",
      "Epoch [1/1], Step [638/3504], Loss: 0.4821\n",
      "Epoch [1/1], Step [639/3504], Loss: 0.4360\n",
      "Epoch [1/1], Step [640/3504], Loss: 0.4647\n",
      "Epoch [1/1], Step [641/3504], Loss: 0.3885\n",
      "Epoch [1/1], Step [642/3504], Loss: 0.3897\n",
      "Epoch [1/1], Step [643/3504], Loss: 0.4841\n",
      "Epoch [1/1], Step [644/3504], Loss: 0.4399\n",
      "Epoch [1/1], Step [645/3504], Loss: 0.3692\n",
      "Epoch [1/1], Step [646/3504], Loss: 0.4718\n",
      "Epoch [1/1], Step [647/3504], Loss: 0.4306\n",
      "Epoch [1/1], Step [648/3504], Loss: 0.3253\n",
      "Epoch [1/1], Step [649/3504], Loss: 0.4205\n",
      "Epoch [1/1], Step [650/3504], Loss: 0.3739\n",
      "Epoch [1/1], Step [651/3504], Loss: 0.3349\n",
      "Epoch [1/1], Step [652/3504], Loss: 0.3697\n",
      "Epoch [1/1], Step [653/3504], Loss: 0.3894\n",
      "Epoch [1/1], Step [654/3504], Loss: 0.4208\n",
      "Epoch [1/1], Step [655/3504], Loss: 0.3082\n",
      "Epoch [1/1], Step [656/3504], Loss: 0.3674\n",
      "Epoch [1/1], Step [657/3504], Loss: 0.4694\n",
      "Epoch [1/1], Step [658/3504], Loss: 0.4079\n",
      "Epoch [1/1], Step [659/3504], Loss: 0.3273\n",
      "Epoch [1/1], Step [660/3504], Loss: 0.4108\n",
      "Epoch [1/1], Step [661/3504], Loss: 0.3795\n",
      "Epoch [1/1], Step [662/3504], Loss: 0.4398\n",
      "Epoch [1/1], Step [663/3504], Loss: 0.3726\n",
      "Epoch [1/1], Step [664/3504], Loss: 0.4748\n",
      "Epoch [1/1], Step [665/3504], Loss: 0.4638\n",
      "Epoch [1/1], Step [666/3504], Loss: 0.4407\n",
      "Epoch [1/1], Step [667/3504], Loss: 0.3422\n",
      "Epoch [1/1], Step [668/3504], Loss: 0.3844\n",
      "Epoch [1/1], Step [669/3504], Loss: 0.5026\n",
      "Epoch [1/1], Step [670/3504], Loss: 0.4749\n",
      "Epoch [1/1], Step [671/3504], Loss: 0.3853\n",
      "Epoch [1/1], Step [672/3504], Loss: 0.4491\n",
      "Epoch [1/1], Step [673/3504], Loss: 0.3580\n",
      "Epoch [1/1], Step [674/3504], Loss: 0.4579\n",
      "Epoch [1/1], Step [675/3504], Loss: 0.4777\n",
      "Epoch [1/1], Step [676/3504], Loss: 0.4517\n",
      "Epoch [1/1], Step [677/3504], Loss: 0.3934\n",
      "Epoch [1/1], Step [678/3504], Loss: 0.4340\n",
      "Epoch [1/1], Step [679/3504], Loss: 0.3603\n",
      "Epoch [1/1], Step [680/3504], Loss: 0.3040\n",
      "Epoch [1/1], Step [681/3504], Loss: 0.4109\n",
      "Epoch [1/1], Step [682/3504], Loss: 0.3849\n",
      "Epoch [1/1], Step [683/3504], Loss: 0.4004\n",
      "Epoch [1/1], Step [684/3504], Loss: 0.3458\n",
      "Epoch [1/1], Step [685/3504], Loss: 0.4841\n",
      "Epoch [1/1], Step [686/3504], Loss: 0.4885\n",
      "Epoch [1/1], Step [687/3504], Loss: 0.4283\n",
      "Epoch [1/1], Step [688/3504], Loss: 0.4102\n",
      "Epoch [1/1], Step [689/3504], Loss: 0.4655\n",
      "Epoch [1/1], Step [690/3504], Loss: 0.3550\n",
      "Epoch [1/1], Step [691/3504], Loss: 0.4551\n",
      "Epoch [1/1], Step [692/3504], Loss: 0.3562\n",
      "Epoch [1/1], Step [693/3504], Loss: 0.5141\n",
      "Epoch [1/1], Step [694/3504], Loss: 0.3841\n",
      "Epoch [1/1], Step [695/3504], Loss: 0.4069\n",
      "Epoch [1/1], Step [696/3504], Loss: 0.5321\n",
      "Epoch [1/1], Step [697/3504], Loss: 0.4472\n",
      "Epoch [1/1], Step [698/3504], Loss: 0.4944\n",
      "Epoch [1/1], Step [699/3504], Loss: 0.4036\n",
      "Epoch [1/1], Step [700/3504], Loss: 0.4557\n",
      "Epoch [1/1], Step [701/3504], Loss: 0.3838\n",
      "Epoch [1/1], Step [702/3504], Loss: 0.4677\n",
      "Epoch [1/1], Step [703/3504], Loss: 0.4584\n",
      "Epoch [1/1], Step [704/3504], Loss: 0.4255\n",
      "Epoch [1/1], Step [705/3504], Loss: 0.4828\n",
      "Epoch [1/1], Step [706/3504], Loss: 0.6143\n",
      "Epoch [1/1], Step [707/3504], Loss: 0.3882\n",
      "Epoch [1/1], Step [708/3504], Loss: 0.5089\n",
      "Epoch [1/1], Step [709/3504], Loss: 0.5084\n",
      "Epoch [1/1], Step [710/3504], Loss: 0.3507\n",
      "Epoch [1/1], Step [711/3504], Loss: 0.4275\n",
      "Epoch [1/1], Step [712/3504], Loss: 0.4737\n",
      "Epoch [1/1], Step [713/3504], Loss: 0.4165\n",
      "Epoch [1/1], Step [714/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [715/3504], Loss: 0.3938\n",
      "Epoch [1/1], Step [716/3504], Loss: 0.4095\n",
      "Epoch [1/1], Step [717/3504], Loss: 0.3613\n",
      "Epoch [1/1], Step [718/3504], Loss: 0.3517\n",
      "Epoch [1/1], Step [719/3504], Loss: 0.4212\n",
      "Epoch [1/1], Step [720/3504], Loss: 0.6050\n",
      "Epoch [1/1], Step [721/3504], Loss: 0.3945\n",
      "Epoch [1/1], Step [722/3504], Loss: 0.4645\n",
      "Epoch [1/1], Step [723/3504], Loss: 0.5546\n",
      "Epoch [1/1], Step [724/3504], Loss: 0.3319\n",
      "Epoch [1/1], Step [725/3504], Loss: 0.4325\n",
      "Epoch [1/1], Step [726/3504], Loss: 0.4321\n",
      "Epoch [1/1], Step [727/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [728/3504], Loss: 0.4982\n",
      "Epoch [1/1], Step [729/3504], Loss: 0.4546\n",
      "Epoch [1/1], Step [730/3504], Loss: 0.4394\n",
      "Epoch [1/1], Step [731/3504], Loss: 0.4414\n",
      "Epoch [1/1], Step [732/3504], Loss: 0.3823\n",
      "Epoch [1/1], Step [733/3504], Loss: 0.4814\n",
      "Epoch [1/1], Step [734/3504], Loss: 0.4082\n",
      "Epoch [1/1], Step [735/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [736/3504], Loss: 0.5206\n",
      "Epoch [1/1], Step [737/3504], Loss: 0.3777\n",
      "Epoch [1/1], Step [738/3504], Loss: 0.3577\n",
      "Epoch [1/1], Step [739/3504], Loss: 0.4787\n",
      "Epoch [1/1], Step [740/3504], Loss: 0.5195\n",
      "Epoch [1/1], Step [741/3504], Loss: 0.4249\n",
      "Epoch [1/1], Step [742/3504], Loss: 0.4264\n",
      "Epoch [1/1], Step [743/3504], Loss: 0.4606\n",
      "Epoch [1/1], Step [744/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [745/3504], Loss: 0.4056\n",
      "Epoch [1/1], Step [746/3504], Loss: 0.4304\n",
      "Epoch [1/1], Step [747/3504], Loss: 0.3861\n",
      "Epoch [1/1], Step [748/3504], Loss: 0.4641\n",
      "Epoch [1/1], Step [749/3504], Loss: 0.5074\n",
      "Epoch [1/1], Step [750/3504], Loss: 0.4763\n",
      "Epoch [1/1], Step [751/3504], Loss: 0.3481\n",
      "Epoch [1/1], Step [752/3504], Loss: 0.4245\n",
      "Epoch [1/1], Step [753/3504], Loss: 0.3355\n",
      "Epoch [1/1], Step [754/3504], Loss: 0.3712\n",
      "Epoch [1/1], Step [755/3504], Loss: 0.3836\n",
      "Epoch [1/1], Step [756/3504], Loss: 0.5022\n",
      "Epoch [1/1], Step [757/3504], Loss: 0.5277\n",
      "Epoch [1/1], Step [758/3504], Loss: 0.4516\n",
      "Epoch [1/1], Step [759/3504], Loss: 0.4205\n",
      "Epoch [1/1], Step [760/3504], Loss: 0.3932\n",
      "Epoch [1/1], Step [761/3504], Loss: 0.3907\n",
      "Epoch [1/1], Step [762/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [763/3504], Loss: 0.4793\n",
      "Epoch [1/1], Step [764/3504], Loss: 0.3419\n",
      "Epoch [1/1], Step [765/3504], Loss: 0.4098\n",
      "Epoch [1/1], Step [766/3504], Loss: 0.4025\n",
      "Epoch [1/1], Step [767/3504], Loss: 0.4489\n",
      "Epoch [1/1], Step [768/3504], Loss: 0.4956\n",
      "Epoch [1/1], Step [769/3504], Loss: 0.4079\n",
      "Epoch [1/1], Step [770/3504], Loss: 0.3690\n",
      "Epoch [1/1], Step [771/3504], Loss: 0.4346\n",
      "Epoch [1/1], Step [772/3504], Loss: 0.4756\n",
      "Epoch [1/1], Step [773/3504], Loss: 0.4229\n",
      "Epoch [1/1], Step [774/3504], Loss: 0.3906\n",
      "Epoch [1/1], Step [775/3504], Loss: 0.5044\n",
      "Epoch [1/1], Step [776/3504], Loss: 0.4038\n",
      "Epoch [1/1], Step [777/3504], Loss: 0.3489\n",
      "Epoch [1/1], Step [778/3504], Loss: 0.3936\n",
      "Epoch [1/1], Step [779/3504], Loss: 0.4336\n",
      "Epoch [1/1], Step [780/3504], Loss: 0.3797\n",
      "Epoch [1/1], Step [781/3504], Loss: 0.5671\n",
      "Epoch [1/1], Step [782/3504], Loss: 0.5431\n",
      "Epoch [1/1], Step [783/3504], Loss: 0.4511\n",
      "Epoch [1/1], Step [784/3504], Loss: 0.5221\n",
      "Epoch [1/1], Step [785/3504], Loss: 0.3903\n",
      "Epoch [1/1], Step [786/3504], Loss: 0.4329\n",
      "Epoch [1/1], Step [787/3504], Loss: 0.3468\n",
      "Epoch [1/1], Step [788/3504], Loss: 0.3801\n",
      "Epoch [1/1], Step [789/3504], Loss: 0.3945\n",
      "Epoch [1/1], Step [790/3504], Loss: 0.4060\n",
      "Epoch [1/1], Step [791/3504], Loss: 0.5111\n",
      "Epoch [1/1], Step [792/3504], Loss: 0.4454\n",
      "Epoch [1/1], Step [793/3504], Loss: 0.6007\n",
      "Epoch [1/1], Step [794/3504], Loss: 0.4694\n",
      "Epoch [1/1], Step [795/3504], Loss: 0.4237\n",
      "Epoch [1/1], Step [796/3504], Loss: 0.4132\n",
      "Epoch [1/1], Step [797/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [798/3504], Loss: 0.3511\n",
      "Epoch [1/1], Step [799/3504], Loss: 0.3219\n",
      "Epoch [1/1], Step [800/3504], Loss: 0.3754\n",
      "Epoch [1/1], Step [801/3504], Loss: 0.3699\n",
      "Epoch [1/1], Step [802/3504], Loss: 0.2962\n",
      "Epoch [1/1], Step [803/3504], Loss: 0.5587\n",
      "Epoch [1/1], Step [804/3504], Loss: 0.5067\n",
      "Epoch [1/1], Step [805/3504], Loss: 0.4004\n",
      "Epoch [1/1], Step [806/3504], Loss: 0.4748\n",
      "Epoch [1/1], Step [807/3504], Loss: 0.4015\n",
      "Epoch [1/1], Step [808/3504], Loss: 0.4665\n",
      "Epoch [1/1], Step [809/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [810/3504], Loss: 0.4758\n",
      "Epoch [1/1], Step [811/3504], Loss: 0.6111\n",
      "Epoch [1/1], Step [812/3504], Loss: 0.4519\n",
      "Epoch [1/1], Step [813/3504], Loss: 0.4626\n",
      "Epoch [1/1], Step [814/3504], Loss: 0.2985\n",
      "Epoch [1/1], Step [815/3504], Loss: 0.4435\n",
      "Epoch [1/1], Step [816/3504], Loss: 0.4128\n",
      "Epoch [1/1], Step [817/3504], Loss: 0.4098\n",
      "Epoch [1/1], Step [818/3504], Loss: 0.4350\n",
      "Epoch [1/1], Step [819/3504], Loss: 0.3876\n",
      "Epoch [1/1], Step [820/3504], Loss: 0.6030\n",
      "Epoch [1/1], Step [821/3504], Loss: 0.4185\n",
      "Epoch [1/1], Step [822/3504], Loss: 0.3870\n",
      "Epoch [1/1], Step [823/3504], Loss: 0.3721\n",
      "Epoch [1/1], Step [824/3504], Loss: 0.3585\n",
      "Epoch [1/1], Step [825/3504], Loss: 0.4212\n",
      "Epoch [1/1], Step [826/3504], Loss: 0.4318\n",
      "Epoch [1/1], Step [827/3504], Loss: 0.5503\n",
      "Epoch [1/1], Step [828/3504], Loss: 0.4570\n",
      "Epoch [1/1], Step [829/3504], Loss: 0.3834\n",
      "Epoch [1/1], Step [830/3504], Loss: 0.3929\n",
      "Epoch [1/1], Step [831/3504], Loss: 0.4178\n",
      "Epoch [1/1], Step [832/3504], Loss: 0.4293\n",
      "Epoch [1/1], Step [833/3504], Loss: 0.4597\n",
      "Epoch [1/1], Step [834/3504], Loss: 0.4266\n",
      "Epoch [1/1], Step [835/3504], Loss: 0.4436\n",
      "Epoch [1/1], Step [836/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [837/3504], Loss: 0.3976\n",
      "Epoch [1/1], Step [838/3504], Loss: 0.3977\n",
      "Epoch [1/1], Step [839/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [840/3504], Loss: 0.4750\n",
      "Epoch [1/1], Step [841/3504], Loss: 0.3609\n",
      "Epoch [1/1], Step [842/3504], Loss: 0.4639\n",
      "Epoch [1/1], Step [843/3504], Loss: 0.4127\n",
      "Epoch [1/1], Step [844/3504], Loss: 0.3070\n",
      "Epoch [1/1], Step [845/3504], Loss: 0.3756\n",
      "Epoch [1/1], Step [846/3504], Loss: 0.5024\n",
      "Epoch [1/1], Step [847/3504], Loss: 0.4936\n",
      "Epoch [1/1], Step [848/3504], Loss: 0.3774\n",
      "Epoch [1/1], Step [849/3504], Loss: 0.4462\n",
      "Epoch [1/1], Step [850/3504], Loss: 0.4644\n",
      "Epoch [1/1], Step [851/3504], Loss: 0.3428\n",
      "Epoch [1/1], Step [852/3504], Loss: 0.4047\n",
      "Epoch [1/1], Step [853/3504], Loss: 0.4383\n",
      "Epoch [1/1], Step [854/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [855/3504], Loss: 0.4965\n",
      "Epoch [1/1], Step [856/3504], Loss: 0.3048\n",
      "Epoch [1/1], Step [857/3504], Loss: 0.3190\n",
      "Epoch [1/1], Step [858/3504], Loss: 0.4235\n",
      "Epoch [1/1], Step [859/3504], Loss: 0.4072\n",
      "Epoch [1/1], Step [860/3504], Loss: 0.3714\n",
      "Epoch [1/1], Step [861/3504], Loss: 0.4402\n",
      "Epoch [1/1], Step [862/3504], Loss: 0.6181\n",
      "Epoch [1/1], Step [863/3504], Loss: 0.3239\n",
      "Epoch [1/1], Step [864/3504], Loss: 0.4322\n",
      "Epoch [1/1], Step [865/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [866/3504], Loss: 0.3674\n",
      "Epoch [1/1], Step [867/3504], Loss: 0.4417\n",
      "Epoch [1/1], Step [868/3504], Loss: 0.4327\n",
      "Epoch [1/1], Step [869/3504], Loss: 0.4110\n",
      "Epoch [1/1], Step [870/3504], Loss: 0.4222\n",
      "Epoch [1/1], Step [871/3504], Loss: 0.4897\n",
      "Epoch [1/1], Step [872/3504], Loss: 0.4159\n",
      "Epoch [1/1], Step [873/3504], Loss: 0.4708\n",
      "Epoch [1/1], Step [874/3504], Loss: 0.4387\n",
      "Epoch [1/1], Step [875/3504], Loss: 0.5122\n",
      "Epoch [1/1], Step [876/3504], Loss: 0.4203\n",
      "Epoch [1/1], Step [877/3504], Loss: 0.5364\n",
      "Epoch [1/1], Step [878/3504], Loss: 0.4100\n",
      "Epoch [1/1], Step [879/3504], Loss: 0.5103\n",
      "Epoch [1/1], Step [880/3504], Loss: 0.3995\n",
      "Epoch [1/1], Step [881/3504], Loss: 0.4271\n",
      "Epoch [1/1], Step [882/3504], Loss: 0.4130\n",
      "Epoch [1/1], Step [883/3504], Loss: 0.3211\n",
      "Epoch [1/1], Step [884/3504], Loss: 0.4467\n",
      "Epoch [1/1], Step [885/3504], Loss: 0.5283\n",
      "Epoch [1/1], Step [886/3504], Loss: 0.3755\n",
      "Epoch [1/1], Step [887/3504], Loss: 0.4074\n",
      "Epoch [1/1], Step [888/3504], Loss: 0.3087\n",
      "Epoch [1/1], Step [889/3504], Loss: 0.4250\n",
      "Epoch [1/1], Step [890/3504], Loss: 0.5526\n",
      "Epoch [1/1], Step [891/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [892/3504], Loss: 0.4280\n",
      "Epoch [1/1], Step [893/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [894/3504], Loss: 0.3939\n",
      "Epoch [1/1], Step [895/3504], Loss: 0.4377\n",
      "Epoch [1/1], Step [896/3504], Loss: 0.4015\n",
      "Epoch [1/1], Step [897/3504], Loss: 0.6057\n",
      "Epoch [1/1], Step [898/3504], Loss: 0.4365\n",
      "Epoch [1/1], Step [899/3504], Loss: 0.4054\n",
      "Epoch [1/1], Step [900/3504], Loss: 0.4096\n",
      "Epoch [1/1], Step [901/3504], Loss: 0.5528\n",
      "Epoch [1/1], Step [902/3504], Loss: 0.3643\n",
      "Epoch [1/1], Step [903/3504], Loss: 0.3345\n",
      "Epoch [1/1], Step [904/3504], Loss: 0.4318\n",
      "Epoch [1/1], Step [905/3504], Loss: 0.4705\n",
      "Epoch [1/1], Step [906/3504], Loss: 0.3215\n",
      "Epoch [1/1], Step [907/3504], Loss: 0.4567\n",
      "Epoch [1/1], Step [908/3504], Loss: 0.3164\n",
      "Epoch [1/1], Step [909/3504], Loss: 0.4433\n",
      "Epoch [1/1], Step [910/3504], Loss: 0.4202\n",
      "Epoch [1/1], Step [911/3504], Loss: 0.3760\n",
      "Epoch [1/1], Step [912/3504], Loss: 0.3773\n",
      "Epoch [1/1], Step [913/3504], Loss: 0.4538\n",
      "Epoch [1/1], Step [914/3504], Loss: 0.4538\n",
      "Epoch [1/1], Step [915/3504], Loss: 0.3763\n",
      "Epoch [1/1], Step [916/3504], Loss: 0.4267\n",
      "Epoch [1/1], Step [917/3504], Loss: 0.4732\n",
      "Epoch [1/1], Step [918/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [919/3504], Loss: 0.5202\n",
      "Epoch [1/1], Step [920/3504], Loss: 0.3698\n",
      "Epoch [1/1], Step [921/3504], Loss: 0.4325\n",
      "Epoch [1/1], Step [922/3504], Loss: 0.6592\n",
      "Epoch [1/1], Step [923/3504], Loss: 0.3670\n",
      "Epoch [1/1], Step [924/3504], Loss: 0.4530\n",
      "Epoch [1/1], Step [925/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [926/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [927/3504], Loss: 0.4029\n",
      "Epoch [1/1], Step [928/3504], Loss: 0.3892\n",
      "Epoch [1/1], Step [929/3504], Loss: 0.4346\n",
      "Epoch [1/1], Step [930/3504], Loss: 0.5688\n",
      "Epoch [1/1], Step [931/3504], Loss: 0.5075\n",
      "Epoch [1/1], Step [932/3504], Loss: 0.4002\n",
      "Epoch [1/1], Step [933/3504], Loss: 0.4450\n",
      "Epoch [1/1], Step [934/3504], Loss: 0.5140\n",
      "Epoch [1/1], Step [935/3504], Loss: 0.4131\n",
      "Epoch [1/1], Step [936/3504], Loss: 0.3925\n",
      "Epoch [1/1], Step [937/3504], Loss: 0.5953\n",
      "Epoch [1/1], Step [938/3504], Loss: 0.3450\n",
      "Epoch [1/1], Step [939/3504], Loss: 0.3596\n",
      "Epoch [1/1], Step [940/3504], Loss: 0.4113\n",
      "Epoch [1/1], Step [941/3504], Loss: 0.4577\n",
      "Epoch [1/1], Step [942/3504], Loss: 0.4030\n",
      "Epoch [1/1], Step [943/3504], Loss: 0.3894\n",
      "Epoch [1/1], Step [944/3504], Loss: 0.3451\n",
      "Epoch [1/1], Step [945/3504], Loss: 0.4060\n",
      "Epoch [1/1], Step [946/3504], Loss: 0.4692\n",
      "Epoch [1/1], Step [947/3504], Loss: 0.4566\n",
      "Epoch [1/1], Step [948/3504], Loss: 0.3847\n",
      "Epoch [1/1], Step [949/3504], Loss: 0.4703\n",
      "Epoch [1/1], Step [950/3504], Loss: 0.4019\n",
      "Epoch [1/1], Step [951/3504], Loss: 0.4241\n",
      "Epoch [1/1], Step [952/3504], Loss: 0.5013\n",
      "Epoch [1/1], Step [953/3504], Loss: 0.5438\n",
      "Epoch [1/1], Step [954/3504], Loss: 0.4604\n",
      "Epoch [1/1], Step [955/3504], Loss: 0.4395\n",
      "Epoch [1/1], Step [956/3504], Loss: 0.3180\n",
      "Epoch [1/1], Step [957/3504], Loss: 0.3219\n",
      "Epoch [1/1], Step [958/3504], Loss: 0.3643\n",
      "Epoch [1/1], Step [959/3504], Loss: 0.4982\n",
      "Epoch [1/1], Step [960/3504], Loss: 0.4283\n",
      "Epoch [1/1], Step [961/3504], Loss: 0.4503\n",
      "Epoch [1/1], Step [962/3504], Loss: 0.4182\n",
      "Epoch [1/1], Step [963/3504], Loss: 0.3413\n",
      "Epoch [1/1], Step [964/3504], Loss: 0.5112\n",
      "Epoch [1/1], Step [965/3504], Loss: 0.4624\n",
      "Epoch [1/1], Step [966/3504], Loss: 0.3541\n",
      "Epoch [1/1], Step [967/3504], Loss: 0.3881\n",
      "Epoch [1/1], Step [968/3504], Loss: 0.3819\n",
      "Epoch [1/1], Step [969/3504], Loss: 0.4122\n",
      "Epoch [1/1], Step [970/3504], Loss: 0.4241\n",
      "Epoch [1/1], Step [971/3504], Loss: 0.4799\n",
      "Epoch [1/1], Step [972/3504], Loss: 0.4934\n",
      "Epoch [1/1], Step [973/3504], Loss: 0.3070\n",
      "Epoch [1/1], Step [974/3504], Loss: 0.4120\n",
      "Epoch [1/1], Step [975/3504], Loss: 0.3977\n",
      "Epoch [1/1], Step [976/3504], Loss: 0.4336\n",
      "Epoch [1/1], Step [977/3504], Loss: 0.4313\n",
      "Epoch [1/1], Step [978/3504], Loss: 0.3502\n",
      "Epoch [1/1], Step [979/3504], Loss: 0.4653\n",
      "Epoch [1/1], Step [980/3504], Loss: 0.4473\n",
      "Epoch [1/1], Step [981/3504], Loss: 0.5540\n",
      "Epoch [1/1], Step [982/3504], Loss: 0.3363\n",
      "Epoch [1/1], Step [983/3504], Loss: 0.3708\n",
      "Epoch [1/1], Step [984/3504], Loss: 0.4330\n",
      "Epoch [1/1], Step [985/3504], Loss: 0.4910\n",
      "Epoch [1/1], Step [986/3504], Loss: 0.5040\n",
      "Epoch [1/1], Step [987/3504], Loss: 0.4091\n",
      "Epoch [1/1], Step [988/3504], Loss: 0.5305\n",
      "Epoch [1/1], Step [989/3504], Loss: 0.4695\n",
      "Epoch [1/1], Step [990/3504], Loss: 0.4201\n",
      "Epoch [1/1], Step [991/3504], Loss: 0.3386\n",
      "Epoch [1/1], Step [992/3504], Loss: 0.4062\n",
      "Epoch [1/1], Step [993/3504], Loss: 0.3698\n",
      "Epoch [1/1], Step [994/3504], Loss: 0.4091\n",
      "Epoch [1/1], Step [995/3504], Loss: 0.3393\n",
      "Epoch [1/1], Step [996/3504], Loss: 0.3873\n",
      "Epoch [1/1], Step [997/3504], Loss: 0.2752\n",
      "Epoch [1/1], Step [998/3504], Loss: 0.2899\n",
      "Epoch [1/1], Step [999/3504], Loss: 0.3374\n",
      "Epoch [1/1], Step [1000/3504], Loss: 0.5559\n",
      "Epoch [1/1], Step [1001/3504], Loss: 0.3430\n",
      "Epoch [1/1], Step [1002/3504], Loss: 0.4331\n",
      "Epoch [1/1], Step [1003/3504], Loss: 0.4394\n",
      "Epoch [1/1], Step [1004/3504], Loss: 0.3912\n",
      "Epoch [1/1], Step [1005/3504], Loss: 0.3875\n",
      "Epoch [1/1], Step [1006/3504], Loss: 0.4061\n",
      "Epoch [1/1], Step [1007/3504], Loss: 0.4754\n",
      "Epoch [1/1], Step [1008/3504], Loss: 0.3229\n",
      "Epoch [1/1], Step [1009/3504], Loss: 0.3633\n",
      "Epoch [1/1], Step [1010/3504], Loss: 0.5028\n",
      "Epoch [1/1], Step [1011/3504], Loss: 0.4100\n",
      "Epoch [1/1], Step [1012/3504], Loss: 0.4018\n",
      "Epoch [1/1], Step [1013/3504], Loss: 0.2662\n",
      "Epoch [1/1], Step [1014/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [1015/3504], Loss: 0.4299\n",
      "Epoch [1/1], Step [1016/3504], Loss: 0.4478\n",
      "Epoch [1/1], Step [1017/3504], Loss: 0.3624\n",
      "Epoch [1/1], Step [1018/3504], Loss: 0.4018\n",
      "Epoch [1/1], Step [1019/3504], Loss: 0.4541\n",
      "Epoch [1/1], Step [1020/3504], Loss: 0.4022\n",
      "Epoch [1/1], Step [1021/3504], Loss: 0.4888\n",
      "Epoch [1/1], Step [1022/3504], Loss: 0.6233\n",
      "Epoch [1/1], Step [1023/3504], Loss: 0.4208\n",
      "Epoch [1/1], Step [1024/3504], Loss: 0.5117\n",
      "Epoch [1/1], Step [1025/3504], Loss: 0.3878\n",
      "Epoch [1/1], Step [1026/3504], Loss: 0.3186\n",
      "Epoch [1/1], Step [1027/3504], Loss: 0.4056\n",
      "Epoch [1/1], Step [1028/3504], Loss: 0.3905\n",
      "Epoch [1/1], Step [1029/3504], Loss: 0.4452\n",
      "Epoch [1/1], Step [1030/3504], Loss: 0.4550\n",
      "Epoch [1/1], Step [1031/3504], Loss: 0.4457\n",
      "Epoch [1/1], Step [1032/3504], Loss: 0.4233\n",
      "Epoch [1/1], Step [1033/3504], Loss: 0.3638\n",
      "Epoch [1/1], Step [1034/3504], Loss: 0.4210\n",
      "Epoch [1/1], Step [1035/3504], Loss: 0.4405\n",
      "Epoch [1/1], Step [1036/3504], Loss: 0.4283\n",
      "Epoch [1/1], Step [1037/3504], Loss: 0.4803\n",
      "Epoch [1/1], Step [1038/3504], Loss: 0.5294\n",
      "Epoch [1/1], Step [1039/3504], Loss: 0.3434\n",
      "Epoch [1/1], Step [1040/3504], Loss: 0.3509\n",
      "Epoch [1/1], Step [1041/3504], Loss: 0.2980\n",
      "Epoch [1/1], Step [1042/3504], Loss: 0.3383\n",
      "Epoch [1/1], Step [1043/3504], Loss: 0.4947\n",
      "Epoch [1/1], Step [1044/3504], Loss: 0.3846\n",
      "Epoch [1/1], Step [1045/3504], Loss: 0.4331\n",
      "Epoch [1/1], Step [1046/3504], Loss: 0.4056\n",
      "Epoch [1/1], Step [1047/3504], Loss: 0.4220\n",
      "Epoch [1/1], Step [1048/3504], Loss: 0.3717\n",
      "Epoch [1/1], Step [1049/3504], Loss: 0.4689\n",
      "Epoch [1/1], Step [1050/3504], Loss: 0.4789\n",
      "Epoch [1/1], Step [1051/3504], Loss: 0.4392\n",
      "Epoch [1/1], Step [1052/3504], Loss: 0.5026\n",
      "Epoch [1/1], Step [1053/3504], Loss: 0.3517\n",
      "Epoch [1/1], Step [1054/3504], Loss: 0.5286\n",
      "Epoch [1/1], Step [1055/3504], Loss: 0.3473\n",
      "Epoch [1/1], Step [1056/3504], Loss: 0.4750\n",
      "Epoch [1/1], Step [1057/3504], Loss: 0.4311\n",
      "Epoch [1/1], Step [1058/3504], Loss: 0.5399\n",
      "Epoch [1/1], Step [1059/3504], Loss: 0.4631\n",
      "Epoch [1/1], Step [1060/3504], Loss: 0.3295\n",
      "Epoch [1/1], Step [1061/3504], Loss: 0.4484\n",
      "Epoch [1/1], Step [1062/3504], Loss: 0.3853\n",
      "Epoch [1/1], Step [1063/3504], Loss: 0.5861\n",
      "Epoch [1/1], Step [1064/3504], Loss: 0.4121\n",
      "Epoch [1/1], Step [1065/3504], Loss: 0.3629\n",
      "Epoch [1/1], Step [1066/3504], Loss: 0.4100\n",
      "Epoch [1/1], Step [1067/3504], Loss: 0.4298\n",
      "Epoch [1/1], Step [1068/3504], Loss: 0.2606\n",
      "Epoch [1/1], Step [1069/3504], Loss: 0.3965\n",
      "Epoch [1/1], Step [1070/3504], Loss: 0.3509\n",
      "Epoch [1/1], Step [1071/3504], Loss: 0.3863\n",
      "Epoch [1/1], Step [1072/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [1073/3504], Loss: 0.4198\n",
      "Epoch [1/1], Step [1074/3504], Loss: 0.3868\n",
      "Epoch [1/1], Step [1075/3504], Loss: 0.4818\n",
      "Epoch [1/1], Step [1076/3504], Loss: 0.5768\n",
      "Epoch [1/1], Step [1077/3504], Loss: 0.3277\n",
      "Epoch [1/1], Step [1078/3504], Loss: 0.4098\n",
      "Epoch [1/1], Step [1079/3504], Loss: 0.3940\n",
      "Epoch [1/1], Step [1080/3504], Loss: 0.4944\n",
      "Epoch [1/1], Step [1081/3504], Loss: 0.4629\n",
      "Epoch [1/1], Step [1082/3504], Loss: 0.3512\n",
      "Epoch [1/1], Step [1083/3504], Loss: 0.3833\n",
      "Epoch [1/1], Step [1084/3504], Loss: 0.3456\n",
      "Epoch [1/1], Step [1085/3504], Loss: 0.5142\n",
      "Epoch [1/1], Step [1086/3504], Loss: 0.3754\n",
      "Epoch [1/1], Step [1087/3504], Loss: 0.4267\n",
      "Epoch [1/1], Step [1088/3504], Loss: 0.4109\n",
      "Epoch [1/1], Step [1089/3504], Loss: 0.3872\n",
      "Epoch [1/1], Step [1090/3504], Loss: 0.3601\n",
      "Epoch [1/1], Step [1091/3504], Loss: 0.4201\n",
      "Epoch [1/1], Step [1092/3504], Loss: 0.4243\n",
      "Epoch [1/1], Step [1093/3504], Loss: 0.5811\n",
      "Epoch [1/1], Step [1094/3504], Loss: 0.4199\n",
      "Epoch [1/1], Step [1095/3504], Loss: 0.5263\n",
      "Epoch [1/1], Step [1096/3504], Loss: 0.4738\n",
      "Epoch [1/1], Step [1097/3504], Loss: 0.4773\n",
      "Epoch [1/1], Step [1098/3504], Loss: 0.4175\n",
      "Epoch [1/1], Step [1099/3504], Loss: 0.4252\n",
      "Epoch [1/1], Step [1100/3504], Loss: 0.4031\n",
      "Epoch [1/1], Step [1101/3504], Loss: 0.4587\n",
      "Epoch [1/1], Step [1102/3504], Loss: 0.3924\n",
      "Epoch [1/1], Step [1103/3504], Loss: 0.3880\n",
      "Epoch [1/1], Step [1104/3504], Loss: 0.3792\n",
      "Epoch [1/1], Step [1105/3504], Loss: 0.5009\n",
      "Epoch [1/1], Step [1106/3504], Loss: 0.4113\n",
      "Epoch [1/1], Step [1107/3504], Loss: 0.3616\n",
      "Epoch [1/1], Step [1108/3504], Loss: 0.4570\n",
      "Epoch [1/1], Step [1109/3504], Loss: 0.3659\n",
      "Epoch [1/1], Step [1110/3504], Loss: 0.4045\n",
      "Epoch [1/1], Step [1111/3504], Loss: 0.3928\n",
      "Epoch [1/1], Step [1112/3504], Loss: 0.4171\n",
      "Epoch [1/1], Step [1113/3504], Loss: 0.3879\n",
      "Epoch [1/1], Step [1114/3504], Loss: 0.4173\n",
      "Epoch [1/1], Step [1115/3504], Loss: 0.4968\n",
      "Epoch [1/1], Step [1116/3504], Loss: 0.4059\n",
      "Epoch [1/1], Step [1117/3504], Loss: 0.5631\n",
      "Epoch [1/1], Step [1118/3504], Loss: 0.3156\n",
      "Epoch [1/1], Step [1119/3504], Loss: 0.4418\n",
      "Epoch [1/1], Step [1120/3504], Loss: 0.3778\n",
      "Epoch [1/1], Step [1121/3504], Loss: 0.4439\n",
      "Epoch [1/1], Step [1122/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [1123/3504], Loss: 0.4432\n",
      "Epoch [1/1], Step [1124/3504], Loss: 0.5838\n",
      "Epoch [1/1], Step [1125/3504], Loss: 0.5000\n",
      "Epoch [1/1], Step [1126/3504], Loss: 0.3944\n",
      "Epoch [1/1], Step [1127/3504], Loss: 0.5371\n",
      "Epoch [1/1], Step [1128/3504], Loss: 0.4754\n",
      "Epoch [1/1], Step [1129/3504], Loss: 0.5033\n",
      "Epoch [1/1], Step [1130/3504], Loss: 0.5015\n",
      "Epoch [1/1], Step [1131/3504], Loss: 0.4514\n",
      "Epoch [1/1], Step [1132/3504], Loss: 0.5149\n",
      "Epoch [1/1], Step [1133/3504], Loss: 0.4589\n",
      "Epoch [1/1], Step [1134/3504], Loss: 0.4377\n",
      "Epoch [1/1], Step [1135/3504], Loss: 0.4260\n",
      "Epoch [1/1], Step [1136/3504], Loss: 0.4341\n",
      "Epoch [1/1], Step [1137/3504], Loss: 0.3869\n",
      "Epoch [1/1], Step [1138/3504], Loss: 0.4054\n",
      "Epoch [1/1], Step [1139/3504], Loss: 0.3921\n",
      "Epoch [1/1], Step [1140/3504], Loss: 0.3377\n",
      "Epoch [1/1], Step [1141/3504], Loss: 0.4396\n",
      "Epoch [1/1], Step [1142/3504], Loss: 0.3644\n",
      "Epoch [1/1], Step [1143/3504], Loss: 0.4148\n",
      "Epoch [1/1], Step [1144/3504], Loss: 0.4643\n",
      "Epoch [1/1], Step [1145/3504], Loss: 0.3927\n",
      "Epoch [1/1], Step [1146/3504], Loss: 0.4826\n",
      "Epoch [1/1], Step [1147/3504], Loss: 0.3539\n",
      "Epoch [1/1], Step [1148/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [1149/3504], Loss: 0.3870\n",
      "Epoch [1/1], Step [1150/3504], Loss: 0.4427\n",
      "Epoch [1/1], Step [1151/3504], Loss: 0.3567\n",
      "Epoch [1/1], Step [1152/3504], Loss: 0.3850\n",
      "Epoch [1/1], Step [1153/3504], Loss: 0.3500\n",
      "Epoch [1/1], Step [1154/3504], Loss: 0.4233\n",
      "Epoch [1/1], Step [1155/3504], Loss: 0.3688\n",
      "Epoch [1/1], Step [1156/3504], Loss: 0.4721\n",
      "Epoch [1/1], Step [1157/3504], Loss: 0.3137\n",
      "Epoch [1/1], Step [1158/3504], Loss: 0.3719\n",
      "Epoch [1/1], Step [1159/3504], Loss: 0.4098\n",
      "Epoch [1/1], Step [1160/3504], Loss: 0.4116\n",
      "Epoch [1/1], Step [1161/3504], Loss: 0.4337\n",
      "Epoch [1/1], Step [1162/3504], Loss: 0.5317\n",
      "Epoch [1/1], Step [1163/3504], Loss: 0.4414\n",
      "Epoch [1/1], Step [1164/3504], Loss: 0.3969\n",
      "Epoch [1/1], Step [1165/3504], Loss: 0.3848\n",
      "Epoch [1/1], Step [1166/3504], Loss: 0.3442\n",
      "Epoch [1/1], Step [1167/3504], Loss: 0.5805\n",
      "Epoch [1/1], Step [1168/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [1169/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [1170/3504], Loss: 0.3836\n",
      "Epoch [1/1], Step [1171/3504], Loss: 0.5468\n",
      "Epoch [1/1], Step [1172/3504], Loss: 0.5776\n",
      "Epoch [1/1], Step [1173/3504], Loss: 0.3979\n",
      "Epoch [1/1], Step [1174/3504], Loss: 0.4827\n",
      "Epoch [1/1], Step [1175/3504], Loss: 0.3813\n",
      "Epoch [1/1], Step [1176/3504], Loss: 0.4620\n",
      "Epoch [1/1], Step [1177/3504], Loss: 0.4654\n",
      "Epoch [1/1], Step [1178/3504], Loss: 0.3291\n",
      "Epoch [1/1], Step [1179/3504], Loss: 0.4100\n",
      "Epoch [1/1], Step [1180/3504], Loss: 0.3076\n",
      "Epoch [1/1], Step [1181/3504], Loss: 0.4015\n",
      "Epoch [1/1], Step [1182/3504], Loss: 0.4429\n",
      "Epoch [1/1], Step [1183/3504], Loss: 0.3878\n",
      "Epoch [1/1], Step [1184/3504], Loss: 0.4055\n",
      "Epoch [1/1], Step [1185/3504], Loss: 0.5242\n",
      "Epoch [1/1], Step [1186/3504], Loss: 0.3202\n",
      "Epoch [1/1], Step [1187/3504], Loss: 0.3024\n",
      "Epoch [1/1], Step [1188/3504], Loss: 0.3192\n",
      "Epoch [1/1], Step [1189/3504], Loss: 0.4688\n",
      "Epoch [1/1], Step [1190/3504], Loss: 0.4387\n",
      "Epoch [1/1], Step [1191/3504], Loss: 0.2915\n",
      "Epoch [1/1], Step [1192/3504], Loss: 0.5076\n",
      "Epoch [1/1], Step [1193/3504], Loss: 0.3091\n",
      "Epoch [1/1], Step [1194/3504], Loss: 0.3548\n",
      "Epoch [1/1], Step [1195/3504], Loss: 0.6114\n",
      "Epoch [1/1], Step [1196/3504], Loss: 0.3663\n",
      "Epoch [1/1], Step [1197/3504], Loss: 0.3157\n",
      "Epoch [1/1], Step [1198/3504], Loss: 0.3595\n",
      "Epoch [1/1], Step [1199/3504], Loss: 0.4472\n",
      "Epoch [1/1], Step [1200/3504], Loss: 0.3875\n",
      "Epoch [1/1], Step [1201/3504], Loss: 0.5115\n",
      "Epoch [1/1], Step [1202/3504], Loss: 0.4213\n",
      "Epoch [1/1], Step [1203/3504], Loss: 0.4622\n",
      "Epoch [1/1], Step [1204/3504], Loss: 0.4900\n",
      "Epoch [1/1], Step [1205/3504], Loss: 0.4849\n",
      "Epoch [1/1], Step [1206/3504], Loss: 0.3273\n",
      "Epoch [1/1], Step [1207/3504], Loss: 0.3805\n",
      "Epoch [1/1], Step [1208/3504], Loss: 0.3793\n",
      "Epoch [1/1], Step [1209/3504], Loss: 0.5774\n",
      "Epoch [1/1], Step [1210/3504], Loss: 0.4212\n",
      "Epoch [1/1], Step [1211/3504], Loss: 0.4749\n",
      "Epoch [1/1], Step [1212/3504], Loss: 0.4716\n",
      "Epoch [1/1], Step [1213/3504], Loss: 0.5874\n",
      "Epoch [1/1], Step [1214/3504], Loss: 0.2647\n",
      "Epoch [1/1], Step [1215/3504], Loss: 0.4286\n",
      "Epoch [1/1], Step [1216/3504], Loss: 0.4024\n",
      "Epoch [1/1], Step [1217/3504], Loss: 0.5186\n",
      "Epoch [1/1], Step [1218/3504], Loss: 0.4015\n",
      "Epoch [1/1], Step [1219/3504], Loss: 0.4271\n",
      "Epoch [1/1], Step [1220/3504], Loss: 0.5496\n",
      "Epoch [1/1], Step [1221/3504], Loss: 0.5246\n",
      "Epoch [1/1], Step [1222/3504], Loss: 0.4095\n",
      "Epoch [1/1], Step [1223/3504], Loss: 0.4694\n",
      "Epoch [1/1], Step [1224/3504], Loss: 0.4040\n",
      "Epoch [1/1], Step [1225/3504], Loss: 0.3501\n",
      "Epoch [1/1], Step [1226/3504], Loss: 0.3576\n",
      "Epoch [1/1], Step [1227/3504], Loss: 0.4582\n",
      "Epoch [1/1], Step [1228/3504], Loss: 0.3925\n",
      "Epoch [1/1], Step [1229/3504], Loss: 0.4413\n",
      "Epoch [1/1], Step [1230/3504], Loss: 0.4121\n",
      "Epoch [1/1], Step [1231/3504], Loss: 0.3730\n",
      "Epoch [1/1], Step [1232/3504], Loss: 0.3475\n",
      "Epoch [1/1], Step [1233/3504], Loss: 0.3241\n",
      "Epoch [1/1], Step [1234/3504], Loss: 0.3730\n",
      "Epoch [1/1], Step [1235/3504], Loss: 0.3430\n",
      "Epoch [1/1], Step [1236/3504], Loss: 0.4823\n",
      "Epoch [1/1], Step [1237/3504], Loss: 0.4016\n",
      "Epoch [1/1], Step [1238/3504], Loss: 0.4642\n",
      "Epoch [1/1], Step [1239/3504], Loss: 0.3997\n",
      "Epoch [1/1], Step [1240/3504], Loss: 0.4068\n",
      "Epoch [1/1], Step [1241/3504], Loss: 0.3571\n",
      "Epoch [1/1], Step [1242/3504], Loss: 0.4790\n",
      "Epoch [1/1], Step [1243/3504], Loss: 0.3725\n",
      "Epoch [1/1], Step [1244/3504], Loss: 0.3719\n",
      "Epoch [1/1], Step [1245/3504], Loss: 0.3972\n",
      "Epoch [1/1], Step [1246/3504], Loss: 0.3812\n",
      "Epoch [1/1], Step [1247/3504], Loss: 0.3523\n",
      "Epoch [1/1], Step [1248/3504], Loss: 0.3833\n",
      "Epoch [1/1], Step [1249/3504], Loss: 0.4125\n",
      "Epoch [1/1], Step [1250/3504], Loss: 0.4152\n",
      "Epoch [1/1], Step [1251/3504], Loss: 0.4937\n",
      "Epoch [1/1], Step [1252/3504], Loss: 0.2623\n",
      "Epoch [1/1], Step [1253/3504], Loss: 0.4663\n",
      "Epoch [1/1], Step [1254/3504], Loss: 0.2825\n",
      "Epoch [1/1], Step [1255/3504], Loss: 0.4922\n",
      "Epoch [1/1], Step [1256/3504], Loss: 0.4343\n",
      "Epoch [1/1], Step [1257/3504], Loss: 0.5011\n",
      "Epoch [1/1], Step [1258/3504], Loss: 0.5269\n",
      "Epoch [1/1], Step [1259/3504], Loss: 0.4424\n",
      "Epoch [1/1], Step [1260/3504], Loss: 0.3637\n",
      "Epoch [1/1], Step [1261/3504], Loss: 0.4697\n",
      "Epoch [1/1], Step [1262/3504], Loss: 0.5561\n",
      "Epoch [1/1], Step [1263/3504], Loss: 0.3579\n",
      "Epoch [1/1], Step [1264/3504], Loss: 0.4871\n",
      "Epoch [1/1], Step [1265/3504], Loss: 0.4924\n",
      "Epoch [1/1], Step [1266/3504], Loss: 0.2914\n",
      "Epoch [1/1], Step [1267/3504], Loss: 0.4730\n",
      "Epoch [1/1], Step [1268/3504], Loss: 0.5578\n",
      "Epoch [1/1], Step [1269/3504], Loss: 0.4450\n",
      "Epoch [1/1], Step [1270/3504], Loss: 0.4160\n",
      "Epoch [1/1], Step [1271/3504], Loss: 0.3877\n",
      "Epoch [1/1], Step [1272/3504], Loss: 0.3796\n",
      "Epoch [1/1], Step [1273/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [1274/3504], Loss: 0.4337\n",
      "Epoch [1/1], Step [1275/3504], Loss: 0.4582\n",
      "Epoch [1/1], Step [1276/3504], Loss: 0.5000\n",
      "Epoch [1/1], Step [1277/3504], Loss: 0.4956\n",
      "Epoch [1/1], Step [1278/3504], Loss: 0.4963\n",
      "Epoch [1/1], Step [1279/3504], Loss: 0.5208\n",
      "Epoch [1/1], Step [1280/3504], Loss: 0.3277\n",
      "Epoch [1/1], Step [1281/3504], Loss: 0.4719\n",
      "Epoch [1/1], Step [1282/3504], Loss: 0.4349\n",
      "Epoch [1/1], Step [1283/3504], Loss: 0.4684\n",
      "Epoch [1/1], Step [1284/3504], Loss: 0.4162\n",
      "Epoch [1/1], Step [1285/3504], Loss: 0.5610\n",
      "Epoch [1/1], Step [1286/3504], Loss: 0.4466\n",
      "Epoch [1/1], Step [1287/3504], Loss: 0.4930\n",
      "Epoch [1/1], Step [1288/3504], Loss: 0.5237\n",
      "Epoch [1/1], Step [1289/3504], Loss: 0.5533\n",
      "Epoch [1/1], Step [1290/3504], Loss: 0.5512\n",
      "Epoch [1/1], Step [1291/3504], Loss: 0.4034\n",
      "Epoch [1/1], Step [1292/3504], Loss: 0.3927\n",
      "Epoch [1/1], Step [1293/3504], Loss: 0.4439\n",
      "Epoch [1/1], Step [1294/3504], Loss: 0.3605\n",
      "Epoch [1/1], Step [1295/3504], Loss: 0.4434\n",
      "Epoch [1/1], Step [1296/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [1297/3504], Loss: 0.5789\n",
      "Epoch [1/1], Step [1298/3504], Loss: 0.4234\n",
      "Epoch [1/1], Step [1299/3504], Loss: 0.4628\n",
      "Epoch [1/1], Step [1300/3504], Loss: 0.5552\n",
      "Epoch [1/1], Step [1301/3504], Loss: 0.4447\n",
      "Epoch [1/1], Step [1302/3504], Loss: 0.4046\n",
      "Epoch [1/1], Step [1303/3504], Loss: 0.4339\n",
      "Epoch [1/1], Step [1304/3504], Loss: 0.3780\n",
      "Epoch [1/1], Step [1305/3504], Loss: 0.4388\n",
      "Epoch [1/1], Step [1306/3504], Loss: 0.3606\n",
      "Epoch [1/1], Step [1307/3504], Loss: 0.4757\n",
      "Epoch [1/1], Step [1308/3504], Loss: 0.4324\n",
      "Epoch [1/1], Step [1309/3504], Loss: 0.3714\n",
      "Epoch [1/1], Step [1310/3504], Loss: 0.5742\n",
      "Epoch [1/1], Step [1311/3504], Loss: 0.4263\n",
      "Epoch [1/1], Step [1312/3504], Loss: 0.3928\n",
      "Epoch [1/1], Step [1313/3504], Loss: 0.4096\n",
      "Epoch [1/1], Step [1314/3504], Loss: 0.4997\n",
      "Epoch [1/1], Step [1315/3504], Loss: 0.4046\n",
      "Epoch [1/1], Step [1316/3504], Loss: 0.4633\n",
      "Epoch [1/1], Step [1317/3504], Loss: 0.4044\n",
      "Epoch [1/1], Step [1318/3504], Loss: 0.4710\n",
      "Epoch [1/1], Step [1319/3504], Loss: 0.3898\n",
      "Epoch [1/1], Step [1320/3504], Loss: 0.4311\n",
      "Epoch [1/1], Step [1321/3504], Loss: 0.3620\n",
      "Epoch [1/1], Step [1322/3504], Loss: 0.3901\n",
      "Epoch [1/1], Step [1323/3504], Loss: 0.3893\n",
      "Epoch [1/1], Step [1324/3504], Loss: 0.4134\n",
      "Epoch [1/1], Step [1325/3504], Loss: 0.4679\n",
      "Epoch [1/1], Step [1326/3504], Loss: 0.3673\n",
      "Epoch [1/1], Step [1327/3504], Loss: 0.5187\n",
      "Epoch [1/1], Step [1328/3504], Loss: 0.4311\n",
      "Epoch [1/1], Step [1329/3504], Loss: 0.4725\n",
      "Epoch [1/1], Step [1330/3504], Loss: 0.4144\n",
      "Epoch [1/1], Step [1331/3504], Loss: 0.4057\n",
      "Epoch [1/1], Step [1332/3504], Loss: 0.4817\n",
      "Epoch [1/1], Step [1333/3504], Loss: 0.3703\n",
      "Epoch [1/1], Step [1334/3504], Loss: 0.4376\n",
      "Epoch [1/1], Step [1335/3504], Loss: 0.4124\n",
      "Epoch [1/1], Step [1336/3504], Loss: 0.3733\n",
      "Epoch [1/1], Step [1337/3504], Loss: 0.3597\n",
      "Epoch [1/1], Step [1338/3504], Loss: 0.5828\n",
      "Epoch [1/1], Step [1339/3504], Loss: 0.5041\n",
      "Epoch [1/1], Step [1340/3504], Loss: 0.3694\n",
      "Epoch [1/1], Step [1341/3504], Loss: 0.4543\n",
      "Epoch [1/1], Step [1342/3504], Loss: 0.4661\n",
      "Epoch [1/1], Step [1343/3504], Loss: 0.3937\n",
      "Epoch [1/1], Step [1344/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [1345/3504], Loss: 0.5206\n",
      "Epoch [1/1], Step [1346/3504], Loss: 0.3329\n",
      "Epoch [1/1], Step [1347/3504], Loss: 0.4285\n",
      "Epoch [1/1], Step [1348/3504], Loss: 0.4567\n",
      "Epoch [1/1], Step [1349/3504], Loss: 0.4225\n",
      "Epoch [1/1], Step [1350/3504], Loss: 0.5289\n",
      "Epoch [1/1], Step [1351/3504], Loss: 0.3936\n",
      "Epoch [1/1], Step [1352/3504], Loss: 0.4104\n",
      "Epoch [1/1], Step [1353/3504], Loss: 0.4923\n",
      "Epoch [1/1], Step [1354/3504], Loss: 0.3087\n",
      "Epoch [1/1], Step [1355/3504], Loss: 0.5450\n",
      "Epoch [1/1], Step [1356/3504], Loss: 0.3999\n",
      "Epoch [1/1], Step [1357/3504], Loss: 0.5146\n",
      "Epoch [1/1], Step [1358/3504], Loss: 0.5869\n",
      "Epoch [1/1], Step [1359/3504], Loss: 0.3763\n",
      "Epoch [1/1], Step [1360/3504], Loss: 0.3897\n",
      "Epoch [1/1], Step [1361/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [1362/3504], Loss: 0.3701\n",
      "Epoch [1/1], Step [1363/3504], Loss: 0.4288\n",
      "Epoch [1/1], Step [1364/3504], Loss: 0.4617\n",
      "Epoch [1/1], Step [1365/3504], Loss: 0.3248\n",
      "Epoch [1/1], Step [1366/3504], Loss: 0.3699\n",
      "Epoch [1/1], Step [1367/3504], Loss: 0.4659\n",
      "Epoch [1/1], Step [1368/3504], Loss: 0.4958\n",
      "Epoch [1/1], Step [1369/3504], Loss: 0.4355\n",
      "Epoch [1/1], Step [1370/3504], Loss: 0.5016\n",
      "Epoch [1/1], Step [1371/3504], Loss: 0.4780\n",
      "Epoch [1/1], Step [1372/3504], Loss: 0.3987\n",
      "Epoch [1/1], Step [1373/3504], Loss: 0.4182\n",
      "Epoch [1/1], Step [1374/3504], Loss: 0.3538\n",
      "Epoch [1/1], Step [1375/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [1376/3504], Loss: 0.3016\n",
      "Epoch [1/1], Step [1377/3504], Loss: 0.3819\n",
      "Epoch [1/1], Step [1378/3504], Loss: 0.3610\n",
      "Epoch [1/1], Step [1379/3504], Loss: 0.5568\n",
      "Epoch [1/1], Step [1380/3504], Loss: 0.5007\n",
      "Epoch [1/1], Step [1381/3504], Loss: 0.4656\n",
      "Epoch [1/1], Step [1382/3504], Loss: 0.3745\n",
      "Epoch [1/1], Step [1383/3504], Loss: 0.3287\n",
      "Epoch [1/1], Step [1384/3504], Loss: 0.3976\n",
      "Epoch [1/1], Step [1385/3504], Loss: 0.3529\n",
      "Epoch [1/1], Step [1386/3504], Loss: 0.4465\n",
      "Epoch [1/1], Step [1387/3504], Loss: 0.3194\n",
      "Epoch [1/1], Step [1388/3504], Loss: 0.3826\n",
      "Epoch [1/1], Step [1389/3504], Loss: 0.4361\n",
      "Epoch [1/1], Step [1390/3504], Loss: 0.4868\n",
      "Epoch [1/1], Step [1391/3504], Loss: 0.5019\n",
      "Epoch [1/1], Step [1392/3504], Loss: 0.4946\n",
      "Epoch [1/1], Step [1393/3504], Loss: 0.4708\n",
      "Epoch [1/1], Step [1394/3504], Loss: 0.5714\n",
      "Epoch [1/1], Step [1395/3504], Loss: 0.5709\n",
      "Epoch [1/1], Step [1396/3504], Loss: 0.5079\n",
      "Epoch [1/1], Step [1397/3504], Loss: 0.5429\n",
      "Epoch [1/1], Step [1398/3504], Loss: 0.4544\n",
      "Epoch [1/1], Step [1399/3504], Loss: 0.4820\n",
      "Epoch [1/1], Step [1400/3504], Loss: 0.3412\n",
      "Epoch [1/1], Step [1401/3504], Loss: 0.4968\n",
      "Epoch [1/1], Step [1402/3504], Loss: 0.4813\n",
      "Epoch [1/1], Step [1403/3504], Loss: 0.5800\n",
      "Epoch [1/1], Step [1404/3504], Loss: 0.3424\n",
      "Epoch [1/1], Step [1405/3504], Loss: 0.3287\n",
      "Epoch [1/1], Step [1406/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [1407/3504], Loss: 0.4076\n",
      "Epoch [1/1], Step [1408/3504], Loss: 0.5654\n",
      "Epoch [1/1], Step [1409/3504], Loss: 0.4880\n",
      "Epoch [1/1], Step [1410/3504], Loss: 0.4412\n",
      "Epoch [1/1], Step [1411/3504], Loss: 0.3490\n",
      "Epoch [1/1], Step [1412/3504], Loss: 0.4201\n",
      "Epoch [1/1], Step [1413/3504], Loss: 0.4361\n",
      "Epoch [1/1], Step [1414/3504], Loss: 0.4968\n",
      "Epoch [1/1], Step [1415/3504], Loss: 0.4151\n",
      "Epoch [1/1], Step [1416/3504], Loss: 0.3837\n",
      "Epoch [1/1], Step [1417/3504], Loss: 0.3935\n",
      "Epoch [1/1], Step [1418/3504], Loss: 0.4120\n",
      "Epoch [1/1], Step [1419/3504], Loss: 0.4821\n",
      "Epoch [1/1], Step [1420/3504], Loss: 0.5242\n",
      "Epoch [1/1], Step [1421/3504], Loss: 0.4090\n",
      "Epoch [1/1], Step [1422/3504], Loss: 0.3517\n",
      "Epoch [1/1], Step [1423/3504], Loss: 0.3968\n",
      "Epoch [1/1], Step [1424/3504], Loss: 0.4432\n",
      "Epoch [1/1], Step [1425/3504], Loss: 0.4750\n",
      "Epoch [1/1], Step [1426/3504], Loss: 0.4230\n",
      "Epoch [1/1], Step [1427/3504], Loss: 0.4828\n",
      "Epoch [1/1], Step [1428/3504], Loss: 0.4931\n",
      "Epoch [1/1], Step [1429/3504], Loss: 0.3240\n",
      "Epoch [1/1], Step [1430/3504], Loss: 0.5570\n",
      "Epoch [1/1], Step [1431/3504], Loss: 0.3606\n",
      "Epoch [1/1], Step [1432/3504], Loss: 0.5384\n",
      "Epoch [1/1], Step [1433/3504], Loss: 0.5797\n",
      "Epoch [1/1], Step [1434/3504], Loss: 0.3634\n",
      "Epoch [1/1], Step [1435/3504], Loss: 0.4589\n",
      "Epoch [1/1], Step [1436/3504], Loss: 0.4480\n",
      "Epoch [1/1], Step [1437/3504], Loss: 0.4280\n",
      "Epoch [1/1], Step [1438/3504], Loss: 0.3540\n",
      "Epoch [1/1], Step [1439/3504], Loss: 0.3295\n",
      "Epoch [1/1], Step [1440/3504], Loss: 0.4575\n",
      "Epoch [1/1], Step [1441/3504], Loss: 0.5332\n",
      "Epoch [1/1], Step [1442/3504], Loss: 0.5258\n",
      "Epoch [1/1], Step [1443/3504], Loss: 0.3354\n",
      "Epoch [1/1], Step [1444/3504], Loss: 0.4143\n",
      "Epoch [1/1], Step [1445/3504], Loss: 0.6082\n",
      "Epoch [1/1], Step [1446/3504], Loss: 0.4415\n",
      "Epoch [1/1], Step [1447/3504], Loss: 0.3648\n",
      "Epoch [1/1], Step [1448/3504], Loss: 0.3714\n",
      "Epoch [1/1], Step [1449/3504], Loss: 0.4103\n",
      "Epoch [1/1], Step [1450/3504], Loss: 0.3810\n",
      "Epoch [1/1], Step [1451/3504], Loss: 0.4971\n",
      "Epoch [1/1], Step [1452/3504], Loss: 0.4058\n",
      "Epoch [1/1], Step [1453/3504], Loss: 0.4447\n",
      "Epoch [1/1], Step [1454/3504], Loss: 0.4904\n",
      "Epoch [1/1], Step [1455/3504], Loss: 0.5306\n",
      "Epoch [1/1], Step [1456/3504], Loss: 0.3676\n",
      "Epoch [1/1], Step [1457/3504], Loss: 0.4760\n",
      "Epoch [1/1], Step [1458/3504], Loss: 0.4415\n",
      "Epoch [1/1], Step [1459/3504], Loss: 0.3982\n",
      "Epoch [1/1], Step [1460/3504], Loss: 0.3103\n",
      "Epoch [1/1], Step [1461/3504], Loss: 0.4760\n",
      "Epoch [1/1], Step [1462/3504], Loss: 0.3675\n",
      "Epoch [1/1], Step [1463/3504], Loss: 0.3394\n",
      "Epoch [1/1], Step [1464/3504], Loss: 0.4846\n",
      "Epoch [1/1], Step [1465/3504], Loss: 0.4806\n",
      "Epoch [1/1], Step [1466/3504], Loss: 0.5096\n",
      "Epoch [1/1], Step [1467/3504], Loss: 0.5610\n",
      "Epoch [1/1], Step [1468/3504], Loss: 0.4949\n",
      "Epoch [1/1], Step [1469/3504], Loss: 0.3871\n",
      "Epoch [1/1], Step [1470/3504], Loss: 0.4115\n",
      "Epoch [1/1], Step [1471/3504], Loss: 0.5064\n",
      "Epoch [1/1], Step [1472/3504], Loss: 0.4453\n",
      "Epoch [1/1], Step [1473/3504], Loss: 0.4860\n",
      "Epoch [1/1], Step [1474/3504], Loss: 0.4134\n",
      "Epoch [1/1], Step [1475/3504], Loss: 0.4516\n",
      "Epoch [1/1], Step [1476/3504], Loss: 0.5035\n",
      "Epoch [1/1], Step [1477/3504], Loss: 0.3551\n",
      "Epoch [1/1], Step [1478/3504], Loss: 0.4092\n",
      "Epoch [1/1], Step [1479/3504], Loss: 0.4330\n",
      "Epoch [1/1], Step [1480/3504], Loss: 0.3315\n",
      "Epoch [1/1], Step [1481/3504], Loss: 0.4471\n",
      "Epoch [1/1], Step [1482/3504], Loss: 0.3615\n",
      "Epoch [1/1], Step [1483/3504], Loss: 0.4817\n",
      "Epoch [1/1], Step [1484/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [1485/3504], Loss: 0.4345\n",
      "Epoch [1/1], Step [1486/3504], Loss: 0.5171\n",
      "Epoch [1/1], Step [1487/3504], Loss: 0.3461\n",
      "Epoch [1/1], Step [1488/3504], Loss: 0.4813\n",
      "Epoch [1/1], Step [1489/3504], Loss: 0.4155\n",
      "Epoch [1/1], Step [1490/3504], Loss: 0.3510\n",
      "Epoch [1/1], Step [1491/3504], Loss: 0.5768\n",
      "Epoch [1/1], Step [1492/3504], Loss: 0.5222\n",
      "Epoch [1/1], Step [1493/3504], Loss: 0.3733\n",
      "Epoch [1/1], Step [1494/3504], Loss: 0.4886\n",
      "Epoch [1/1], Step [1495/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [1496/3504], Loss: 0.2947\n",
      "Epoch [1/1], Step [1497/3504], Loss: 0.4320\n",
      "Epoch [1/1], Step [1498/3504], Loss: 0.3607\n",
      "Epoch [1/1], Step [1499/3504], Loss: 0.5333\n",
      "Epoch [1/1], Step [1500/3504], Loss: 0.4612\n",
      "Epoch [1/1], Step [1501/3504], Loss: 0.4441\n",
      "Epoch [1/1], Step [1502/3504], Loss: 0.4252\n",
      "Epoch [1/1], Step [1503/3504], Loss: 0.5065\n",
      "Epoch [1/1], Step [1504/3504], Loss: 0.5427\n",
      "Epoch [1/1], Step [1505/3504], Loss: 0.3747\n",
      "Epoch [1/1], Step [1506/3504], Loss: 0.3374\n",
      "Epoch [1/1], Step [1507/3504], Loss: 0.3568\n",
      "Epoch [1/1], Step [1508/3504], Loss: 0.5106\n",
      "Epoch [1/1], Step [1509/3504], Loss: 0.3652\n",
      "Epoch [1/1], Step [1510/3504], Loss: 0.4839\n",
      "Epoch [1/1], Step [1511/3504], Loss: 0.4182\n",
      "Epoch [1/1], Step [1512/3504], Loss: 0.3671\n",
      "Epoch [1/1], Step [1513/3504], Loss: 0.3597\n",
      "Epoch [1/1], Step [1514/3504], Loss: 0.4158\n",
      "Epoch [1/1], Step [1515/3504], Loss: 0.3568\n",
      "Epoch [1/1], Step [1516/3504], Loss: 0.5195\n",
      "Epoch [1/1], Step [1517/3504], Loss: 0.3932\n",
      "Epoch [1/1], Step [1518/3504], Loss: 0.2994\n",
      "Epoch [1/1], Step [1519/3504], Loss: 0.3707\n",
      "Epoch [1/1], Step [1520/3504], Loss: 0.5302\n",
      "Epoch [1/1], Step [1521/3504], Loss: 0.3112\n",
      "Epoch [1/1], Step [1522/3504], Loss: 0.3863\n",
      "Epoch [1/1], Step [1523/3504], Loss: 0.4226\n",
      "Epoch [1/1], Step [1524/3504], Loss: 0.4501\n",
      "Epoch [1/1], Step [1525/3504], Loss: 0.4180\n",
      "Epoch [1/1], Step [1526/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [1527/3504], Loss: 0.4626\n",
      "Epoch [1/1], Step [1528/3504], Loss: 0.3473\n",
      "Epoch [1/1], Step [1529/3504], Loss: 0.5456\n",
      "Epoch [1/1], Step [1530/3504], Loss: 0.3753\n",
      "Epoch [1/1], Step [1531/3504], Loss: 0.3781\n",
      "Epoch [1/1], Step [1532/3504], Loss: 0.4779\n",
      "Epoch [1/1], Step [1533/3504], Loss: 0.3835\n",
      "Epoch [1/1], Step [1534/3504], Loss: 0.4269\n",
      "Epoch [1/1], Step [1535/3504], Loss: 0.5305\n",
      "Epoch [1/1], Step [1536/3504], Loss: 0.4020\n",
      "Epoch [1/1], Step [1537/3504], Loss: 0.3607\n",
      "Epoch [1/1], Step [1538/3504], Loss: 0.4317\n",
      "Epoch [1/1], Step [1539/3504], Loss: 0.3310\n",
      "Epoch [1/1], Step [1540/3504], Loss: 0.3289\n",
      "Epoch [1/1], Step [1541/3504], Loss: 0.3624\n",
      "Epoch [1/1], Step [1542/3504], Loss: 0.4105\n",
      "Epoch [1/1], Step [1543/3504], Loss: 0.4288\n",
      "Epoch [1/1], Step [1544/3504], Loss: 0.4249\n",
      "Epoch [1/1], Step [1545/3504], Loss: 0.3968\n",
      "Epoch [1/1], Step [1546/3504], Loss: 0.4271\n",
      "Epoch [1/1], Step [1547/3504], Loss: 0.5830\n",
      "Epoch [1/1], Step [1548/3504], Loss: 0.3142\n",
      "Epoch [1/1], Step [1549/3504], Loss: 0.4914\n",
      "Epoch [1/1], Step [1550/3504], Loss: 0.4313\n",
      "Epoch [1/1], Step [1551/3504], Loss: 0.4429\n",
      "Epoch [1/1], Step [1552/3504], Loss: 0.3385\n",
      "Epoch [1/1], Step [1553/3504], Loss: 0.5054\n",
      "Epoch [1/1], Step [1554/3504], Loss: 0.5353\n",
      "Epoch [1/1], Step [1555/3504], Loss: 0.3917\n",
      "Epoch [1/1], Step [1556/3504], Loss: 0.3719\n",
      "Epoch [1/1], Step [1557/3504], Loss: 0.4252\n",
      "Epoch [1/1], Step [1558/3504], Loss: 0.4066\n",
      "Epoch [1/1], Step [1559/3504], Loss: 0.4591\n",
      "Epoch [1/1], Step [1560/3504], Loss: 0.3859\n",
      "Epoch [1/1], Step [1561/3504], Loss: 0.4048\n",
      "Epoch [1/1], Step [1562/3504], Loss: 0.3688\n",
      "Epoch [1/1], Step [1563/3504], Loss: 0.4231\n",
      "Epoch [1/1], Step [1564/3504], Loss: 0.4353\n",
      "Epoch [1/1], Step [1565/3504], Loss: 0.4410\n",
      "Epoch [1/1], Step [1566/3504], Loss: 0.4147\n",
      "Epoch [1/1], Step [1567/3504], Loss: 0.3855\n",
      "Epoch [1/1], Step [1568/3504], Loss: 0.4461\n",
      "Epoch [1/1], Step [1569/3504], Loss: 0.4334\n",
      "Epoch [1/1], Step [1570/3504], Loss: 0.4303\n",
      "Epoch [1/1], Step [1571/3504], Loss: 0.3561\n",
      "Epoch [1/1], Step [1572/3504], Loss: 0.3300\n",
      "Epoch [1/1], Step [1573/3504], Loss: 0.4391\n",
      "Epoch [1/1], Step [1574/3504], Loss: 0.4072\n",
      "Epoch [1/1], Step [1575/3504], Loss: 0.5461\n",
      "Epoch [1/1], Step [1576/3504], Loss: 0.5316\n",
      "Epoch [1/1], Step [1577/3504], Loss: 0.4419\n",
      "Epoch [1/1], Step [1578/3504], Loss: 0.4370\n",
      "Epoch [1/1], Step [1579/3504], Loss: 0.3358\n",
      "Epoch [1/1], Step [1580/3504], Loss: 0.3670\n",
      "Epoch [1/1], Step [1581/3504], Loss: 0.4461\n",
      "Epoch [1/1], Step [1582/3504], Loss: 0.4756\n",
      "Epoch [1/1], Step [1583/3504], Loss: 0.4088\n",
      "Epoch [1/1], Step [1584/3504], Loss: 0.5293\n",
      "Epoch [1/1], Step [1585/3504], Loss: 0.3493\n",
      "Epoch [1/1], Step [1586/3504], Loss: 0.5162\n",
      "Epoch [1/1], Step [1587/3504], Loss: 0.4745\n",
      "Epoch [1/1], Step [1588/3504], Loss: 0.5201\n",
      "Epoch [1/1], Step [1589/3504], Loss: 0.2964\n",
      "Epoch [1/1], Step [1590/3504], Loss: 0.4134\n",
      "Epoch [1/1], Step [1591/3504], Loss: 0.3422\n",
      "Epoch [1/1], Step [1592/3504], Loss: 0.5307\n",
      "Epoch [1/1], Step [1593/3504], Loss: 0.5149\n",
      "Epoch [1/1], Step [1594/3504], Loss: 0.3928\n",
      "Epoch [1/1], Step [1595/3504], Loss: 0.3696\n",
      "Epoch [1/1], Step [1596/3504], Loss: 0.4356\n",
      "Epoch [1/1], Step [1597/3504], Loss: 0.3670\n",
      "Epoch [1/1], Step [1598/3504], Loss: 0.4539\n",
      "Epoch [1/1], Step [1599/3504], Loss: 0.5474\n",
      "Epoch [1/1], Step [1600/3504], Loss: 0.3819\n",
      "Epoch [1/1], Step [1601/3504], Loss: 0.3760\n",
      "Epoch [1/1], Step [1602/3504], Loss: 0.4678\n",
      "Epoch [1/1], Step [1603/3504], Loss: 0.5201\n",
      "Epoch [1/1], Step [1604/3504], Loss: 0.5227\n",
      "Epoch [1/1], Step [1605/3504], Loss: 0.3116\n",
      "Epoch [1/1], Step [1606/3504], Loss: 0.4137\n",
      "Epoch [1/1], Step [1607/3504], Loss: 0.3913\n",
      "Epoch [1/1], Step [1608/3504], Loss: 0.5143\n",
      "Epoch [1/1], Step [1609/3504], Loss: 0.3914\n",
      "Epoch [1/1], Step [1610/3504], Loss: 0.4615\n",
      "Epoch [1/1], Step [1611/3504], Loss: 0.3067\n",
      "Epoch [1/1], Step [1612/3504], Loss: 0.4329\n",
      "Epoch [1/1], Step [1613/3504], Loss: 0.3973\n",
      "Epoch [1/1], Step [1614/3504], Loss: 0.3347\n",
      "Epoch [1/1], Step [1615/3504], Loss: 0.4237\n",
      "Epoch [1/1], Step [1616/3504], Loss: 0.4432\n",
      "Epoch [1/1], Step [1617/3504], Loss: 0.3535\n",
      "Epoch [1/1], Step [1618/3504], Loss: 0.4112\n",
      "Epoch [1/1], Step [1619/3504], Loss: 0.2895\n",
      "Epoch [1/1], Step [1620/3504], Loss: 0.4937\n",
      "Epoch [1/1], Step [1621/3504], Loss: 0.4540\n",
      "Epoch [1/1], Step [1622/3504], Loss: 0.4140\n",
      "Epoch [1/1], Step [1623/3504], Loss: 0.4715\n",
      "Epoch [1/1], Step [1624/3504], Loss: 0.3865\n",
      "Epoch [1/1], Step [1625/3504], Loss: 0.4266\n",
      "Epoch [1/1], Step [1626/3504], Loss: 0.4432\n",
      "Epoch [1/1], Step [1627/3504], Loss: 0.4937\n",
      "Epoch [1/1], Step [1628/3504], Loss: 0.4100\n",
      "Epoch [1/1], Step [1629/3504], Loss: 0.4994\n",
      "Epoch [1/1], Step [1630/3504], Loss: 0.4085\n",
      "Epoch [1/1], Step [1631/3504], Loss: 0.3771\n",
      "Epoch [1/1], Step [1632/3504], Loss: 0.3537\n",
      "Epoch [1/1], Step [1633/3504], Loss: 0.4046\n",
      "Epoch [1/1], Step [1634/3504], Loss: 0.3536\n",
      "Epoch [1/1], Step [1635/3504], Loss: 0.4043\n",
      "Epoch [1/1], Step [1636/3504], Loss: 0.4590\n",
      "Epoch [1/1], Step [1637/3504], Loss: 0.5167\n",
      "Epoch [1/1], Step [1638/3504], Loss: 0.3949\n",
      "Epoch [1/1], Step [1639/3504], Loss: 0.3970\n",
      "Epoch [1/1], Step [1640/3504], Loss: 0.4085\n",
      "Epoch [1/1], Step [1641/3504], Loss: 0.4147\n",
      "Epoch [1/1], Step [1642/3504], Loss: 0.3615\n",
      "Epoch [1/1], Step [1643/3504], Loss: 0.4228\n",
      "Epoch [1/1], Step [1644/3504], Loss: 0.4212\n",
      "Epoch [1/1], Step [1645/3504], Loss: 0.4176\n",
      "Epoch [1/1], Step [1646/3504], Loss: 0.5475\n",
      "Epoch [1/1], Step [1647/3504], Loss: 0.3269\n",
      "Epoch [1/1], Step [1648/3504], Loss: 0.4312\n",
      "Epoch [1/1], Step [1649/3504], Loss: 0.5642\n",
      "Epoch [1/1], Step [1650/3504], Loss: 0.5716\n",
      "Epoch [1/1], Step [1651/3504], Loss: 0.4696\n",
      "Epoch [1/1], Step [1652/3504], Loss: 0.4206\n",
      "Epoch [1/1], Step [1653/3504], Loss: 0.3570\n",
      "Epoch [1/1], Step [1654/3504], Loss: 0.4672\n",
      "Epoch [1/1], Step [1655/3504], Loss: 0.4023\n",
      "Epoch [1/1], Step [1656/3504], Loss: 0.4535\n",
      "Epoch [1/1], Step [1657/3504], Loss: 0.3999\n",
      "Epoch [1/1], Step [1658/3504], Loss: 0.3295\n",
      "Epoch [1/1], Step [1659/3504], Loss: 0.4000\n",
      "Epoch [1/1], Step [1660/3504], Loss: 0.4851\n",
      "Epoch [1/1], Step [1661/3504], Loss: 0.4621\n",
      "Epoch [1/1], Step [1662/3504], Loss: 0.4288\n",
      "Epoch [1/1], Step [1663/3504], Loss: 0.3791\n",
      "Epoch [1/1], Step [1664/3504], Loss: 0.4612\n",
      "Epoch [1/1], Step [1665/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [1666/3504], Loss: 0.4124\n",
      "Epoch [1/1], Step [1667/3504], Loss: 0.4356\n",
      "Epoch [1/1], Step [1668/3504], Loss: 0.3887\n",
      "Epoch [1/1], Step [1669/3504], Loss: 0.4939\n",
      "Epoch [1/1], Step [1670/3504], Loss: 0.4644\n",
      "Epoch [1/1], Step [1671/3504], Loss: 0.4130\n",
      "Epoch [1/1], Step [1672/3504], Loss: 0.4040\n",
      "Epoch [1/1], Step [1673/3504], Loss: 0.3088\n",
      "Epoch [1/1], Step [1674/3504], Loss: 0.3332\n",
      "Epoch [1/1], Step [1675/3504], Loss: 0.3712\n",
      "Epoch [1/1], Step [1676/3504], Loss: 0.5912\n",
      "Epoch [1/1], Step [1677/3504], Loss: 0.4090\n",
      "Epoch [1/1], Step [1678/3504], Loss: 0.4333\n",
      "Epoch [1/1], Step [1679/3504], Loss: 0.3938\n",
      "Epoch [1/1], Step [1680/3504], Loss: 0.5660\n",
      "Epoch [1/1], Step [1681/3504], Loss: 0.5269\n",
      "Epoch [1/1], Step [1682/3504], Loss: 0.5012\n",
      "Epoch [1/1], Step [1683/3504], Loss: 0.4723\n",
      "Epoch [1/1], Step [1684/3504], Loss: 0.4400\n",
      "Epoch [1/1], Step [1685/3504], Loss: 0.3745\n",
      "Epoch [1/1], Step [1686/3504], Loss: 0.4310\n",
      "Epoch [1/1], Step [1687/3504], Loss: 0.4068\n",
      "Epoch [1/1], Step [1688/3504], Loss: 0.6149\n",
      "Epoch [1/1], Step [1689/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [1690/3504], Loss: 0.4510\n",
      "Epoch [1/1], Step [1691/3504], Loss: 0.3158\n",
      "Epoch [1/1], Step [1692/3504], Loss: 0.5056\n",
      "Epoch [1/1], Step [1693/3504], Loss: 0.4039\n",
      "Epoch [1/1], Step [1694/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [1695/3504], Loss: 0.4349\n",
      "Epoch [1/1], Step [1696/3504], Loss: 0.3979\n",
      "Epoch [1/1], Step [1697/3504], Loss: 0.4935\n",
      "Epoch [1/1], Step [1698/3504], Loss: 0.2282\n",
      "Epoch [1/1], Step [1699/3504], Loss: 0.4478\n",
      "Epoch [1/1], Step [1700/3504], Loss: 0.3771\n",
      "Epoch [1/1], Step [1701/3504], Loss: 0.5079\n",
      "Epoch [1/1], Step [1702/3504], Loss: 0.3437\n",
      "Epoch [1/1], Step [1703/3504], Loss: 0.4712\n",
      "Epoch [1/1], Step [1704/3504], Loss: 0.3366\n",
      "Epoch [1/1], Step [1705/3504], Loss: 0.4463\n",
      "Epoch [1/1], Step [1706/3504], Loss: 0.3900\n",
      "Epoch [1/1], Step [1707/3504], Loss: 0.4755\n",
      "Epoch [1/1], Step [1708/3504], Loss: 0.4332\n",
      "Epoch [1/1], Step [1709/3504], Loss: 0.4360\n",
      "Epoch [1/1], Step [1710/3504], Loss: 0.4564\n",
      "Epoch [1/1], Step [1711/3504], Loss: 0.4674\n",
      "Epoch [1/1], Step [1712/3504], Loss: 0.5182\n",
      "Epoch [1/1], Step [1713/3504], Loss: 0.3329\n",
      "Epoch [1/1], Step [1714/3504], Loss: 0.4710\n",
      "Epoch [1/1], Step [1715/3504], Loss: 0.4703\n",
      "Epoch [1/1], Step [1716/3504], Loss: 0.4972\n",
      "Epoch [1/1], Step [1717/3504], Loss: 0.4295\n",
      "Epoch [1/1], Step [1718/3504], Loss: 0.4101\n",
      "Epoch [1/1], Step [1719/3504], Loss: 0.4278\n",
      "Epoch [1/1], Step [1720/3504], Loss: 0.5251\n",
      "Epoch [1/1], Step [1721/3504], Loss: 0.5729\n",
      "Epoch [1/1], Step [1722/3504], Loss: 0.4562\n",
      "Epoch [1/1], Step [1723/3504], Loss: 0.4133\n",
      "Epoch [1/1], Step [1724/3504], Loss: 0.3863\n",
      "Epoch [1/1], Step [1725/3504], Loss: 0.4081\n",
      "Epoch [1/1], Step [1726/3504], Loss: 0.4145\n",
      "Epoch [1/1], Step [1727/3504], Loss: 0.4464\n",
      "Epoch [1/1], Step [1728/3504], Loss: 0.4238\n",
      "Epoch [1/1], Step [1729/3504], Loss: 0.4147\n",
      "Epoch [1/1], Step [1730/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [1731/3504], Loss: 0.3716\n",
      "Epoch [1/1], Step [1732/3504], Loss: 0.5360\n",
      "Epoch [1/1], Step [1733/3504], Loss: 0.3976\n",
      "Epoch [1/1], Step [1734/3504], Loss: 0.3336\n",
      "Epoch [1/1], Step [1735/3504], Loss: 0.3638\n",
      "Epoch [1/1], Step [1736/3504], Loss: 0.4415\n",
      "Epoch [1/1], Step [1737/3504], Loss: 0.5110\n",
      "Epoch [1/1], Step [1738/3504], Loss: 0.3568\n",
      "Epoch [1/1], Step [1739/3504], Loss: 0.4228\n",
      "Epoch [1/1], Step [1740/3504], Loss: 0.4406\n",
      "Epoch [1/1], Step [1741/3504], Loss: 0.3988\n",
      "Epoch [1/1], Step [1742/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [1743/3504], Loss: 0.4041\n",
      "Epoch [1/1], Step [1744/3504], Loss: 0.4257\n",
      "Epoch [1/1], Step [1745/3504], Loss: 0.3591\n",
      "Epoch [1/1], Step [1746/3504], Loss: 0.5275\n",
      "Epoch [1/1], Step [1747/3504], Loss: 0.3315\n",
      "Epoch [1/1], Step [1748/3504], Loss: 0.4282\n",
      "Epoch [1/1], Step [1749/3504], Loss: 0.3269\n",
      "Epoch [1/1], Step [1750/3504], Loss: 0.4176\n",
      "Epoch [1/1], Step [1751/3504], Loss: 0.3996\n",
      "Epoch [1/1], Step [1752/3504], Loss: 0.3587\n",
      "Epoch [1/1], Step [1753/3504], Loss: 0.5179\n",
      "Epoch [1/1], Step [1754/3504], Loss: 0.4403\n",
      "Epoch [1/1], Step [1755/3504], Loss: 0.3518\n",
      "Epoch [1/1], Step [1756/3504], Loss: 0.5148\n",
      "Epoch [1/1], Step [1757/3504], Loss: 0.4264\n",
      "Epoch [1/1], Step [1758/3504], Loss: 0.3918\n",
      "Epoch [1/1], Step [1759/3504], Loss: 0.4899\n",
      "Epoch [1/1], Step [1760/3504], Loss: 0.4966\n",
      "Epoch [1/1], Step [1761/3504], Loss: 0.4336\n",
      "Epoch [1/1], Step [1762/3504], Loss: 0.4938\n",
      "Epoch [1/1], Step [1763/3504], Loss: 0.4535\n",
      "Epoch [1/1], Step [1764/3504], Loss: 0.5800\n",
      "Epoch [1/1], Step [1765/3504], Loss: 0.3538\n",
      "Epoch [1/1], Step [1766/3504], Loss: 0.3794\n",
      "Epoch [1/1], Step [1767/3504], Loss: 0.4962\n",
      "Epoch [1/1], Step [1768/3504], Loss: 0.4142\n",
      "Epoch [1/1], Step [1769/3504], Loss: 0.4008\n",
      "Epoch [1/1], Step [1770/3504], Loss: 0.4940\n",
      "Epoch [1/1], Step [1771/3504], Loss: 0.6010\n",
      "Epoch [1/1], Step [1772/3504], Loss: 0.5214\n",
      "Epoch [1/1], Step [1773/3504], Loss: 0.4453\n",
      "Epoch [1/1], Step [1774/3504], Loss: 0.3500\n",
      "Epoch [1/1], Step [1775/3504], Loss: 0.3155\n",
      "Epoch [1/1], Step [1776/3504], Loss: 0.4889\n",
      "Epoch [1/1], Step [1777/3504], Loss: 0.4068\n",
      "Epoch [1/1], Step [1778/3504], Loss: 0.3735\n",
      "Epoch [1/1], Step [1779/3504], Loss: 0.3118\n",
      "Epoch [1/1], Step [1780/3504], Loss: 0.3505\n",
      "Epoch [1/1], Step [1781/3504], Loss: 0.3844\n",
      "Epoch [1/1], Step [1782/3504], Loss: 0.4664\n",
      "Epoch [1/1], Step [1783/3504], Loss: 0.4074\n",
      "Epoch [1/1], Step [1784/3504], Loss: 0.4169\n",
      "Epoch [1/1], Step [1785/3504], Loss: 0.5738\n",
      "Epoch [1/1], Step [1786/3504], Loss: 0.4117\n",
      "Epoch [1/1], Step [1787/3504], Loss: 0.4580\n",
      "Epoch [1/1], Step [1788/3504], Loss: 0.4585\n",
      "Epoch [1/1], Step [1789/3504], Loss: 0.3241\n",
      "Epoch [1/1], Step [1790/3504], Loss: 0.4847\n",
      "Epoch [1/1], Step [1791/3504], Loss: 0.3469\n",
      "Epoch [1/1], Step [1792/3504], Loss: 0.4548\n",
      "Epoch [1/1], Step [1793/3504], Loss: 0.4730\n",
      "Epoch [1/1], Step [1794/3504], Loss: 0.3814\n",
      "Epoch [1/1], Step [1795/3504], Loss: 0.5269\n",
      "Epoch [1/1], Step [1796/3504], Loss: 0.3889\n",
      "Epoch [1/1], Step [1797/3504], Loss: 0.4672\n",
      "Epoch [1/1], Step [1798/3504], Loss: 0.4804\n",
      "Epoch [1/1], Step [1799/3504], Loss: 0.4583\n",
      "Epoch [1/1], Step [1800/3504], Loss: 0.3563\n",
      "Epoch [1/1], Step [1801/3504], Loss: 0.4920\n",
      "Epoch [1/1], Step [1802/3504], Loss: 0.4485\n",
      "Epoch [1/1], Step [1803/3504], Loss: 0.4190\n",
      "Epoch [1/1], Step [1804/3504], Loss: 0.3418\n",
      "Epoch [1/1], Step [1805/3504], Loss: 0.4188\n",
      "Epoch [1/1], Step [1806/3504], Loss: 0.3582\n",
      "Epoch [1/1], Step [1807/3504], Loss: 0.5065\n",
      "Epoch [1/1], Step [1808/3504], Loss: 0.3743\n",
      "Epoch [1/1], Step [1809/3504], Loss: 0.4639\n",
      "Epoch [1/1], Step [1810/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [1811/3504], Loss: 0.3353\n",
      "Epoch [1/1], Step [1812/3504], Loss: 0.4170\n",
      "Epoch [1/1], Step [1813/3504], Loss: 0.4784\n",
      "Epoch [1/1], Step [1814/3504], Loss: 0.4405\n",
      "Epoch [1/1], Step [1815/3504], Loss: 0.3988\n",
      "Epoch [1/1], Step [1816/3504], Loss: 0.4893\n",
      "Epoch [1/1], Step [1817/3504], Loss: 0.5128\n",
      "Epoch [1/1], Step [1818/3504], Loss: 0.4006\n",
      "Epoch [1/1], Step [1819/3504], Loss: 0.4222\n",
      "Epoch [1/1], Step [1820/3504], Loss: 0.4846\n",
      "Epoch [1/1], Step [1821/3504], Loss: 0.3807\n",
      "Epoch [1/1], Step [1822/3504], Loss: 0.4857\n",
      "Epoch [1/1], Step [1823/3504], Loss: 0.4453\n",
      "Epoch [1/1], Step [1824/3504], Loss: 0.4803\n",
      "Epoch [1/1], Step [1825/3504], Loss: 0.4351\n",
      "Epoch [1/1], Step [1826/3504], Loss: 0.3733\n",
      "Epoch [1/1], Step [1827/3504], Loss: 0.3355\n",
      "Epoch [1/1], Step [1828/3504], Loss: 0.3583\n",
      "Epoch [1/1], Step [1829/3504], Loss: 0.4344\n",
      "Epoch [1/1], Step [1830/3504], Loss: 0.2775\n",
      "Epoch [1/1], Step [1831/3504], Loss: 0.4778\n",
      "Epoch [1/1], Step [1832/3504], Loss: 0.4035\n",
      "Epoch [1/1], Step [1833/3504], Loss: 0.6551\n",
      "Epoch [1/1], Step [1834/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [1835/3504], Loss: 0.3903\n",
      "Epoch [1/1], Step [1836/3504], Loss: 0.4697\n",
      "Epoch [1/1], Step [1837/3504], Loss: 0.4198\n",
      "Epoch [1/1], Step [1838/3504], Loss: 0.4865\n",
      "Epoch [1/1], Step [1839/3504], Loss: 0.4071\n",
      "Epoch [1/1], Step [1840/3504], Loss: 0.5436\n",
      "Epoch [1/1], Step [1841/3504], Loss: 0.4126\n",
      "Epoch [1/1], Step [1842/3504], Loss: 0.3322\n",
      "Epoch [1/1], Step [1843/3504], Loss: 0.3279\n",
      "Epoch [1/1], Step [1844/3504], Loss: 0.3700\n",
      "Epoch [1/1], Step [1845/3504], Loss: 0.4593\n",
      "Epoch [1/1], Step [1846/3504], Loss: 0.4967\n",
      "Epoch [1/1], Step [1847/3504], Loss: 0.4661\n",
      "Epoch [1/1], Step [1848/3504], Loss: 0.4839\n",
      "Epoch [1/1], Step [1849/3504], Loss: 0.4203\n",
      "Epoch [1/1], Step [1850/3504], Loss: 0.3850\n",
      "Epoch [1/1], Step [1851/3504], Loss: 0.4865\n",
      "Epoch [1/1], Step [1852/3504], Loss: 0.4305\n",
      "Epoch [1/1], Step [1853/3504], Loss: 0.4959\n",
      "Epoch [1/1], Step [1854/3504], Loss: 0.5742\n",
      "Epoch [1/1], Step [1855/3504], Loss: 0.4739\n",
      "Epoch [1/1], Step [1856/3504], Loss: 0.4300\n",
      "Epoch [1/1], Step [1857/3504], Loss: 0.3391\n",
      "Epoch [1/1], Step [1858/3504], Loss: 0.3897\n",
      "Epoch [1/1], Step [1859/3504], Loss: 0.3967\n",
      "Epoch [1/1], Step [1860/3504], Loss: 0.4325\n",
      "Epoch [1/1], Step [1861/3504], Loss: 0.3913\n",
      "Epoch [1/1], Step [1862/3504], Loss: 0.4441\n",
      "Epoch [1/1], Step [1863/3504], Loss: 0.4192\n",
      "Epoch [1/1], Step [1864/3504], Loss: 0.4070\n",
      "Epoch [1/1], Step [1865/3504], Loss: 0.3742\n",
      "Epoch [1/1], Step [1866/3504], Loss: 0.4123\n",
      "Epoch [1/1], Step [1867/3504], Loss: 0.4990\n",
      "Epoch [1/1], Step [1868/3504], Loss: 0.5668\n",
      "Epoch [1/1], Step [1869/3504], Loss: 0.4355\n",
      "Epoch [1/1], Step [1870/3504], Loss: 0.3556\n",
      "Epoch [1/1], Step [1871/3504], Loss: 0.3893\n",
      "Epoch [1/1], Step [1872/3504], Loss: 0.4132\n",
      "Epoch [1/1], Step [1873/3504], Loss: 0.3794\n",
      "Epoch [1/1], Step [1874/3504], Loss: 0.4611\n",
      "Epoch [1/1], Step [1875/3504], Loss: 0.4981\n",
      "Epoch [1/1], Step [1876/3504], Loss: 0.4503\n",
      "Epoch [1/1], Step [1877/3504], Loss: 0.4539\n",
      "Epoch [1/1], Step [1878/3504], Loss: 0.4628\n",
      "Epoch [1/1], Step [1879/3504], Loss: 0.4424\n",
      "Epoch [1/1], Step [1880/3504], Loss: 0.5308\n",
      "Epoch [1/1], Step [1881/3504], Loss: 0.4711\n",
      "Epoch [1/1], Step [1882/3504], Loss: 0.4574\n",
      "Epoch [1/1], Step [1883/3504], Loss: 0.4193\n",
      "Epoch [1/1], Step [1884/3504], Loss: 0.3839\n",
      "Epoch [1/1], Step [1885/3504], Loss: 0.4285\n",
      "Epoch [1/1], Step [1886/3504], Loss: 0.4648\n",
      "Epoch [1/1], Step [1887/3504], Loss: 0.3996\n",
      "Epoch [1/1], Step [1888/3504], Loss: 0.3911\n",
      "Epoch [1/1], Step [1889/3504], Loss: 0.3463\n",
      "Epoch [1/1], Step [1890/3504], Loss: 0.4423\n",
      "Epoch [1/1], Step [1891/3504], Loss: 0.4061\n",
      "Epoch [1/1], Step [1892/3504], Loss: 0.3956\n",
      "Epoch [1/1], Step [1893/3504], Loss: 0.4096\n",
      "Epoch [1/1], Step [1894/3504], Loss: 0.3372\n",
      "Epoch [1/1], Step [1895/3504], Loss: 0.4004\n",
      "Epoch [1/1], Step [1896/3504], Loss: 0.3992\n",
      "Epoch [1/1], Step [1897/3504], Loss: 0.3655\n",
      "Epoch [1/1], Step [1898/3504], Loss: 0.3045\n",
      "Epoch [1/1], Step [1899/3504], Loss: 0.4066\n",
      "Epoch [1/1], Step [1900/3504], Loss: 0.3932\n",
      "Epoch [1/1], Step [1901/3504], Loss: 0.4202\n",
      "Epoch [1/1], Step [1902/3504], Loss: 0.4294\n",
      "Epoch [1/1], Step [1903/3504], Loss: 0.4621\n",
      "Epoch [1/1], Step [1904/3504], Loss: 0.5447\n",
      "Epoch [1/1], Step [1905/3504], Loss: 0.3599\n",
      "Epoch [1/1], Step [1906/3504], Loss: 0.3957\n",
      "Epoch [1/1], Step [1907/3504], Loss: 0.2999\n",
      "Epoch [1/1], Step [1908/3504], Loss: 0.3856\n",
      "Epoch [1/1], Step [1909/3504], Loss: 0.5186\n",
      "Epoch [1/1], Step [1910/3504], Loss: 0.3524\n",
      "Epoch [1/1], Step [1911/3504], Loss: 0.4719\n",
      "Epoch [1/1], Step [1912/3504], Loss: 0.5480\n",
      "Epoch [1/1], Step [1913/3504], Loss: 0.3814\n",
      "Epoch [1/1], Step [1914/3504], Loss: 0.3346\n",
      "Epoch [1/1], Step [1915/3504], Loss: 0.3741\n",
      "Epoch [1/1], Step [1916/3504], Loss: 0.4242\n",
      "Epoch [1/1], Step [1917/3504], Loss: 0.4419\n",
      "Epoch [1/1], Step [1918/3504], Loss: 0.4411\n",
      "Epoch [1/1], Step [1919/3504], Loss: 0.3966\n",
      "Epoch [1/1], Step [1920/3504], Loss: 0.4536\n",
      "Epoch [1/1], Step [1921/3504], Loss: 0.3135\n",
      "Epoch [1/1], Step [1922/3504], Loss: 0.4032\n",
      "Epoch [1/1], Step [1923/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [1924/3504], Loss: 0.3033\n",
      "Epoch [1/1], Step [1925/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [1926/3504], Loss: 0.4962\n",
      "Epoch [1/1], Step [1927/3504], Loss: 0.4792\n",
      "Epoch [1/1], Step [1928/3504], Loss: 0.3853\n",
      "Epoch [1/1], Step [1929/3504], Loss: 0.3839\n",
      "Epoch [1/1], Step [1930/3504], Loss: 0.4800\n",
      "Epoch [1/1], Step [1931/3504], Loss: 0.4355\n",
      "Epoch [1/1], Step [1932/3504], Loss: 0.4055\n",
      "Epoch [1/1], Step [1933/3504], Loss: 0.4662\n",
      "Epoch [1/1], Step [1934/3504], Loss: 0.4768\n",
      "Epoch [1/1], Step [1935/3504], Loss: 0.3715\n",
      "Epoch [1/1], Step [1936/3504], Loss: 0.2909\n",
      "Epoch [1/1], Step [1937/3504], Loss: 0.4161\n",
      "Epoch [1/1], Step [1938/3504], Loss: 0.3297\n",
      "Epoch [1/1], Step [1939/3504], Loss: 0.4585\n",
      "Epoch [1/1], Step [1940/3504], Loss: 0.5719\n",
      "Epoch [1/1], Step [1941/3504], Loss: 0.4064\n",
      "Epoch [1/1], Step [1942/3504], Loss: 0.3684\n",
      "Epoch [1/1], Step [1943/3504], Loss: 0.5924\n",
      "Epoch [1/1], Step [1944/3504], Loss: 0.3817\n",
      "Epoch [1/1], Step [1945/3504], Loss: 0.5037\n",
      "Epoch [1/1], Step [1946/3504], Loss: 0.4273\n",
      "Epoch [1/1], Step [1947/3504], Loss: 0.3533\n",
      "Epoch [1/1], Step [1948/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [1949/3504], Loss: 0.6068\n",
      "Epoch [1/1], Step [1950/3504], Loss: 0.4042\n",
      "Epoch [1/1], Step [1951/3504], Loss: 0.4070\n",
      "Epoch [1/1], Step [1952/3504], Loss: 0.4069\n",
      "Epoch [1/1], Step [1953/3504], Loss: 0.3776\n",
      "Epoch [1/1], Step [1954/3504], Loss: 0.4335\n",
      "Epoch [1/1], Step [1955/3504], Loss: 0.4334\n",
      "Epoch [1/1], Step [1956/3504], Loss: 0.4403\n",
      "Epoch [1/1], Step [1957/3504], Loss: 0.4899\n",
      "Epoch [1/1], Step [1958/3504], Loss: 0.4528\n",
      "Epoch [1/1], Step [1959/3504], Loss: 0.4431\n",
      "Epoch [1/1], Step [1960/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [1961/3504], Loss: 0.4837\n",
      "Epoch [1/1], Step [1962/3504], Loss: 0.4244\n",
      "Epoch [1/1], Step [1963/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [1964/3504], Loss: 0.4106\n",
      "Epoch [1/1], Step [1965/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [1966/3504], Loss: 0.5571\n",
      "Epoch [1/1], Step [1967/3504], Loss: 0.3502\n",
      "Epoch [1/1], Step [1968/3504], Loss: 0.3452\n",
      "Epoch [1/1], Step [1969/3504], Loss: 0.4607\n",
      "Epoch [1/1], Step [1970/3504], Loss: 0.5217\n",
      "Epoch [1/1], Step [1971/3504], Loss: 0.4698\n",
      "Epoch [1/1], Step [1972/3504], Loss: 0.3694\n",
      "Epoch [1/1], Step [1973/3504], Loss: 0.4479\n",
      "Epoch [1/1], Step [1974/3504], Loss: 0.3743\n",
      "Epoch [1/1], Step [1975/3504], Loss: 0.3458\n",
      "Epoch [1/1], Step [1976/3504], Loss: 0.3886\n",
      "Epoch [1/1], Step [1977/3504], Loss: 0.4595\n",
      "Epoch [1/1], Step [1978/3504], Loss: 0.4062\n",
      "Epoch [1/1], Step [1979/3504], Loss: 0.4158\n",
      "Epoch [1/1], Step [1980/3504], Loss: 0.3761\n",
      "Epoch [1/1], Step [1981/3504], Loss: 0.3013\n",
      "Epoch [1/1], Step [1982/3504], Loss: 0.4453\n",
      "Epoch [1/1], Step [1983/3504], Loss: 0.3674\n",
      "Epoch [1/1], Step [1984/3504], Loss: 0.3858\n",
      "Epoch [1/1], Step [1985/3504], Loss: 0.3196\n",
      "Epoch [1/1], Step [1986/3504], Loss: 0.3369\n",
      "Epoch [1/1], Step [1987/3504], Loss: 0.4057\n",
      "Epoch [1/1], Step [1988/3504], Loss: 0.4464\n",
      "Epoch [1/1], Step [1989/3504], Loss: 0.4555\n",
      "Epoch [1/1], Step [1990/3504], Loss: 0.3040\n",
      "Epoch [1/1], Step [1991/3504], Loss: 0.4948\n",
      "Epoch [1/1], Step [1992/3504], Loss: 0.3672\n",
      "Epoch [1/1], Step [1993/3504], Loss: 0.4279\n",
      "Epoch [1/1], Step [1994/3504], Loss: 0.4144\n",
      "Epoch [1/1], Step [1995/3504], Loss: 0.4558\n",
      "Epoch [1/1], Step [1996/3504], Loss: 0.4554\n",
      "Epoch [1/1], Step [1997/3504], Loss: 0.4486\n",
      "Epoch [1/1], Step [1998/3504], Loss: 0.4080\n",
      "Epoch [1/1], Step [1999/3504], Loss: 0.3971\n",
      "Epoch [1/1], Step [2000/3504], Loss: 0.4503\n",
      "Epoch [1/1], Step [2001/3504], Loss: 0.3697\n",
      "Epoch [1/1], Step [2002/3504], Loss: 0.5764\n",
      "Epoch [1/1], Step [2003/3504], Loss: 0.3503\n",
      "Epoch [1/1], Step [2004/3504], Loss: 0.4152\n",
      "Epoch [1/1], Step [2005/3504], Loss: 0.3519\n",
      "Epoch [1/1], Step [2006/3504], Loss: 0.3960\n",
      "Epoch [1/1], Step [2007/3504], Loss: 0.4102\n",
      "Epoch [1/1], Step [2008/3504], Loss: 0.3589\n",
      "Epoch [1/1], Step [2009/3504], Loss: 0.5206\n",
      "Epoch [1/1], Step [2010/3504], Loss: 0.4851\n",
      "Epoch [1/1], Step [2011/3504], Loss: 0.4728\n",
      "Epoch [1/1], Step [2012/3504], Loss: 0.4539\n",
      "Epoch [1/1], Step [2013/3504], Loss: 0.5769\n",
      "Epoch [1/1], Step [2014/3504], Loss: 0.5568\n",
      "Epoch [1/1], Step [2015/3504], Loss: 0.4724\n",
      "Epoch [1/1], Step [2016/3504], Loss: 0.5090\n",
      "Epoch [1/1], Step [2017/3504], Loss: 0.4610\n",
      "Epoch [1/1], Step [2018/3504], Loss: 0.3982\n",
      "Epoch [1/1], Step [2019/3504], Loss: 0.4501\n",
      "Epoch [1/1], Step [2020/3504], Loss: 0.5171\n",
      "Epoch [1/1], Step [2021/3504], Loss: 0.3992\n",
      "Epoch [1/1], Step [2022/3504], Loss: 0.4537\n",
      "Epoch [1/1], Step [2023/3504], Loss: 0.5604\n",
      "Epoch [1/1], Step [2024/3504], Loss: 0.3907\n",
      "Epoch [1/1], Step [2025/3504], Loss: 0.4679\n",
      "Epoch [1/1], Step [2026/3504], Loss: 0.5094\n",
      "Epoch [1/1], Step [2027/3504], Loss: 0.4166\n",
      "Epoch [1/1], Step [2028/3504], Loss: 0.5537\n",
      "Epoch [1/1], Step [2029/3504], Loss: 0.4467\n",
      "Epoch [1/1], Step [2030/3504], Loss: 0.4880\n",
      "Epoch [1/1], Step [2031/3504], Loss: 0.2927\n",
      "Epoch [1/1], Step [2032/3504], Loss: 0.3399\n",
      "Epoch [1/1], Step [2033/3504], Loss: 0.3737\n",
      "Epoch [1/1], Step [2034/3504], Loss: 0.3830\n",
      "Epoch [1/1], Step [2035/3504], Loss: 0.5011\n",
      "Epoch [1/1], Step [2036/3504], Loss: 0.4301\n",
      "Epoch [1/1], Step [2037/3504], Loss: 0.5048\n",
      "Epoch [1/1], Step [2038/3504], Loss: 0.4459\n",
      "Epoch [1/1], Step [2039/3504], Loss: 0.4704\n",
      "Epoch [1/1], Step [2040/3504], Loss: 0.4766\n",
      "Epoch [1/1], Step [2041/3504], Loss: 0.4673\n",
      "Epoch [1/1], Step [2042/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [2043/3504], Loss: 0.5959\n",
      "Epoch [1/1], Step [2044/3504], Loss: 0.4962\n",
      "Epoch [1/1], Step [2045/3504], Loss: 0.3317\n",
      "Epoch [1/1], Step [2046/3504], Loss: 0.4084\n",
      "Epoch [1/1], Step [2047/3504], Loss: 0.4100\n",
      "Epoch [1/1], Step [2048/3504], Loss: 0.3590\n",
      "Epoch [1/1], Step [2049/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [2050/3504], Loss: 0.4157\n",
      "Epoch [1/1], Step [2051/3504], Loss: 0.4581\n",
      "Epoch [1/1], Step [2052/3504], Loss: 0.3680\n",
      "Epoch [1/1], Step [2053/3504], Loss: 0.4287\n",
      "Epoch [1/1], Step [2054/3504], Loss: 0.3681\n",
      "Epoch [1/1], Step [2055/3504], Loss: 0.3601\n",
      "Epoch [1/1], Step [2056/3504], Loss: 0.4179\n",
      "Epoch [1/1], Step [2057/3504], Loss: 0.4178\n",
      "Epoch [1/1], Step [2058/3504], Loss: 0.3502\n",
      "Epoch [1/1], Step [2059/3504], Loss: 0.4689\n",
      "Epoch [1/1], Step [2060/3504], Loss: 0.4191\n",
      "Epoch [1/1], Step [2061/3504], Loss: 0.5963\n",
      "Epoch [1/1], Step [2062/3504], Loss: 0.3297\n",
      "Epoch [1/1], Step [2063/3504], Loss: 0.4389\n",
      "Epoch [1/1], Step [2064/3504], Loss: 0.4707\n",
      "Epoch [1/1], Step [2065/3504], Loss: 0.4296\n",
      "Epoch [1/1], Step [2066/3504], Loss: 0.4269\n",
      "Epoch [1/1], Step [2067/3504], Loss: 0.4186\n",
      "Epoch [1/1], Step [2068/3504], Loss: 0.3217\n",
      "Epoch [1/1], Step [2069/3504], Loss: 0.4578\n",
      "Epoch [1/1], Step [2070/3504], Loss: 0.4308\n",
      "Epoch [1/1], Step [2071/3504], Loss: 0.4393\n",
      "Epoch [1/1], Step [2072/3504], Loss: 0.2828\n",
      "Epoch [1/1], Step [2073/3504], Loss: 0.3939\n",
      "Epoch [1/1], Step [2074/3504], Loss: 0.3434\n",
      "Epoch [1/1], Step [2075/3504], Loss: 0.3952\n",
      "Epoch [1/1], Step [2076/3504], Loss: 0.3752\n",
      "Epoch [1/1], Step [2077/3504], Loss: 0.3732\n",
      "Epoch [1/1], Step [2078/3504], Loss: 0.3292\n",
      "Epoch [1/1], Step [2079/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [2080/3504], Loss: 0.3952\n",
      "Epoch [1/1], Step [2081/3504], Loss: 0.5348\n",
      "Epoch [1/1], Step [2082/3504], Loss: 0.3904\n",
      "Epoch [1/1], Step [2083/3504], Loss: 0.4499\n",
      "Epoch [1/1], Step [2084/3504], Loss: 0.3899\n",
      "Epoch [1/1], Step [2085/3504], Loss: 0.5028\n",
      "Epoch [1/1], Step [2086/3504], Loss: 0.4391\n",
      "Epoch [1/1], Step [2087/3504], Loss: 0.4434\n",
      "Epoch [1/1], Step [2088/3504], Loss: 0.3104\n",
      "Epoch [1/1], Step [2089/3504], Loss: 0.3675\n",
      "Epoch [1/1], Step [2090/3504], Loss: 0.4207\n",
      "Epoch [1/1], Step [2091/3504], Loss: 0.4427\n",
      "Epoch [1/1], Step [2092/3504], Loss: 0.3857\n",
      "Epoch [1/1], Step [2093/3504], Loss: 0.4011\n",
      "Epoch [1/1], Step [2094/3504], Loss: 0.6011\n",
      "Epoch [1/1], Step [2095/3504], Loss: 0.4242\n",
      "Epoch [1/1], Step [2096/3504], Loss: 0.3755\n",
      "Epoch [1/1], Step [2097/3504], Loss: 0.4193\n",
      "Epoch [1/1], Step [2098/3504], Loss: 0.4488\n",
      "Epoch [1/1], Step [2099/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [2100/3504], Loss: 0.4356\n",
      "Epoch [1/1], Step [2101/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [2102/3504], Loss: 0.5219\n",
      "Epoch [1/1], Step [2103/3504], Loss: 0.3944\n",
      "Epoch [1/1], Step [2104/3504], Loss: 0.3442\n",
      "Epoch [1/1], Step [2105/3504], Loss: 0.4537\n",
      "Epoch [1/1], Step [2106/3504], Loss: 0.4545\n",
      "Epoch [1/1], Step [2107/3504], Loss: 0.3687\n",
      "Epoch [1/1], Step [2108/3504], Loss: 0.3627\n",
      "Epoch [1/1], Step [2109/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [2110/3504], Loss: 0.4079\n",
      "Epoch [1/1], Step [2111/3504], Loss: 0.3250\n",
      "Epoch [1/1], Step [2112/3504], Loss: 0.5015\n",
      "Epoch [1/1], Step [2113/3504], Loss: 0.3630\n",
      "Epoch [1/1], Step [2114/3504], Loss: 0.5245\n",
      "Epoch [1/1], Step [2115/3504], Loss: 0.4907\n",
      "Epoch [1/1], Step [2116/3504], Loss: 0.3295\n",
      "Epoch [1/1], Step [2117/3504], Loss: 0.3788\n",
      "Epoch [1/1], Step [2118/3504], Loss: 0.5599\n",
      "Epoch [1/1], Step [2119/3504], Loss: 0.4975\n",
      "Epoch [1/1], Step [2120/3504], Loss: 0.3779\n",
      "Epoch [1/1], Step [2121/3504], Loss: 0.3666\n",
      "Epoch [1/1], Step [2122/3504], Loss: 0.5102\n",
      "Epoch [1/1], Step [2123/3504], Loss: 0.3091\n",
      "Epoch [1/1], Step [2124/3504], Loss: 0.3478\n",
      "Epoch [1/1], Step [2125/3504], Loss: 0.4118\n",
      "Epoch [1/1], Step [2126/3504], Loss: 0.5324\n",
      "Epoch [1/1], Step [2127/3504], Loss: 0.4758\n",
      "Epoch [1/1], Step [2128/3504], Loss: 0.4357\n",
      "Epoch [1/1], Step [2129/3504], Loss: 0.4080\n",
      "Epoch [1/1], Step [2130/3504], Loss: 0.4216\n",
      "Epoch [1/1], Step [2131/3504], Loss: 0.4578\n",
      "Epoch [1/1], Step [2132/3504], Loss: 0.4184\n",
      "Epoch [1/1], Step [2133/3504], Loss: 0.5288\n",
      "Epoch [1/1], Step [2134/3504], Loss: 0.4417\n",
      "Epoch [1/1], Step [2135/3504], Loss: 0.3565\n",
      "Epoch [1/1], Step [2136/3504], Loss: 0.3201\n",
      "Epoch [1/1], Step [2137/3504], Loss: 0.3562\n",
      "Epoch [1/1], Step [2138/3504], Loss: 0.4102\n",
      "Epoch [1/1], Step [2139/3504], Loss: 0.4742\n",
      "Epoch [1/1], Step [2140/3504], Loss: 0.4792\n",
      "Epoch [1/1], Step [2141/3504], Loss: 0.3944\n",
      "Epoch [1/1], Step [2142/3504], Loss: 0.3826\n",
      "Epoch [1/1], Step [2143/3504], Loss: 0.4542\n",
      "Epoch [1/1], Step [2144/3504], Loss: 0.3683\n",
      "Epoch [1/1], Step [2145/3504], Loss: 0.5866\n",
      "Epoch [1/1], Step [2146/3504], Loss: 0.4276\n",
      "Epoch [1/1], Step [2147/3504], Loss: 0.5286\n",
      "Epoch [1/1], Step [2148/3504], Loss: 0.5515\n",
      "Epoch [1/1], Step [2149/3504], Loss: 0.4370\n",
      "Epoch [1/1], Step [2150/3504], Loss: 0.3136\n",
      "Epoch [1/1], Step [2151/3504], Loss: 0.4798\n",
      "Epoch [1/1], Step [2152/3504], Loss: 0.3952\n",
      "Epoch [1/1], Step [2153/3504], Loss: 0.4681\n",
      "Epoch [1/1], Step [2154/3504], Loss: 0.5456\n",
      "Epoch [1/1], Step [2155/3504], Loss: 0.4637\n",
      "Epoch [1/1], Step [2156/3504], Loss: 0.5052\n",
      "Epoch [1/1], Step [2157/3504], Loss: 0.4376\n",
      "Epoch [1/1], Step [2158/3504], Loss: 0.4077\n",
      "Epoch [1/1], Step [2159/3504], Loss: 0.3328\n",
      "Epoch [1/1], Step [2160/3504], Loss: 0.3372\n",
      "Epoch [1/1], Step [2161/3504], Loss: 0.3776\n",
      "Epoch [1/1], Step [2162/3504], Loss: 0.4400\n",
      "Epoch [1/1], Step [2163/3504], Loss: 0.5421\n",
      "Epoch [1/1], Step [2164/3504], Loss: 0.5453\n",
      "Epoch [1/1], Step [2165/3504], Loss: 0.4541\n",
      "Epoch [1/1], Step [2166/3504], Loss: 0.4525\n",
      "Epoch [1/1], Step [2167/3504], Loss: 0.3615\n",
      "Epoch [1/1], Step [2168/3504], Loss: 0.3998\n",
      "Epoch [1/1], Step [2169/3504], Loss: 0.4258\n",
      "Epoch [1/1], Step [2170/3504], Loss: 0.4608\n",
      "Epoch [1/1], Step [2171/3504], Loss: 0.3746\n",
      "Epoch [1/1], Step [2172/3504], Loss: 0.3652\n",
      "Epoch [1/1], Step [2173/3504], Loss: 0.3847\n",
      "Epoch [1/1], Step [2174/3504], Loss: 0.4770\n",
      "Epoch [1/1], Step [2175/3504], Loss: 0.4439\n",
      "Epoch [1/1], Step [2176/3504], Loss: 0.3672\n",
      "Epoch [1/1], Step [2177/3504], Loss: 0.3770\n",
      "Epoch [1/1], Step [2178/3504], Loss: 0.4344\n",
      "Epoch [1/1], Step [2179/3504], Loss: 0.5206\n",
      "Epoch [1/1], Step [2180/3504], Loss: 0.5013\n",
      "Epoch [1/1], Step [2181/3504], Loss: 0.4040\n",
      "Epoch [1/1], Step [2182/3504], Loss: 0.5624\n",
      "Epoch [1/1], Step [2183/3504], Loss: 0.5026\n",
      "Epoch [1/1], Step [2184/3504], Loss: 0.4268\n",
      "Epoch [1/1], Step [2185/3504], Loss: 0.3941\n",
      "Epoch [1/1], Step [2186/3504], Loss: 0.4259\n",
      "Epoch [1/1], Step [2187/3504], Loss: 0.3728\n",
      "Epoch [1/1], Step [2188/3504], Loss: 0.4299\n",
      "Epoch [1/1], Step [2189/3504], Loss: 0.4545\n",
      "Epoch [1/1], Step [2190/3504], Loss: 0.4149\n",
      "Epoch [1/1], Step [2191/3504], Loss: 0.4018\n",
      "Epoch [1/1], Step [2192/3504], Loss: 0.4049\n",
      "Epoch [1/1], Step [2193/3504], Loss: 0.5082\n",
      "Epoch [1/1], Step [2194/3504], Loss: 0.3266\n",
      "Epoch [1/1], Step [2195/3504], Loss: 0.2891\n",
      "Epoch [1/1], Step [2196/3504], Loss: 0.4199\n",
      "Epoch [1/1], Step [2197/3504], Loss: 0.4574\n",
      "Epoch [1/1], Step [2198/3504], Loss: 0.3634\n",
      "Epoch [1/1], Step [2199/3504], Loss: 0.5561\n",
      "Epoch [1/1], Step [2200/3504], Loss: 0.3638\n",
      "Epoch [1/1], Step [2201/3504], Loss: 0.3230\n",
      "Epoch [1/1], Step [2202/3504], Loss: 0.4103\n",
      "Epoch [1/1], Step [2203/3504], Loss: 0.4144\n",
      "Epoch [1/1], Step [2204/3504], Loss: 0.4843\n",
      "Epoch [1/1], Step [2205/3504], Loss: 0.5197\n",
      "Epoch [1/1], Step [2206/3504], Loss: 0.4446\n",
      "Epoch [1/1], Step [2207/3504], Loss: 0.3522\n",
      "Epoch [1/1], Step [2208/3504], Loss: 0.5052\n",
      "Epoch [1/1], Step [2209/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [2210/3504], Loss: 0.5418\n",
      "Epoch [1/1], Step [2211/3504], Loss: 0.4609\n",
      "Epoch [1/1], Step [2212/3504], Loss: 0.4381\n",
      "Epoch [1/1], Step [2213/3504], Loss: 0.4454\n",
      "Epoch [1/1], Step [2214/3504], Loss: 0.5432\n",
      "Epoch [1/1], Step [2215/3504], Loss: 0.4039\n",
      "Epoch [1/1], Step [2216/3504], Loss: 0.3994\n",
      "Epoch [1/1], Step [2217/3504], Loss: 0.4088\n",
      "Epoch [1/1], Step [2218/3504], Loss: 0.4494\n",
      "Epoch [1/1], Step [2219/3504], Loss: 0.4251\n",
      "Epoch [1/1], Step [2220/3504], Loss: 0.5300\n",
      "Epoch [1/1], Step [2221/3504], Loss: 0.4775\n",
      "Epoch [1/1], Step [2222/3504], Loss: 0.3966\n",
      "Epoch [1/1], Step [2223/3504], Loss: 0.3039\n",
      "Epoch [1/1], Step [2224/3504], Loss: 0.4092\n",
      "Epoch [1/1], Step [2225/3504], Loss: 0.4710\n",
      "Epoch [1/1], Step [2226/3504], Loss: 0.3540\n",
      "Epoch [1/1], Step [2227/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [2228/3504], Loss: 0.4393\n",
      "Epoch [1/1], Step [2229/3504], Loss: 0.3361\n",
      "Epoch [1/1], Step [2230/3504], Loss: 0.4138\n",
      "Epoch [1/1], Step [2231/3504], Loss: 0.3901\n",
      "Epoch [1/1], Step [2232/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [2233/3504], Loss: 0.4046\n",
      "Epoch [1/1], Step [2234/3504], Loss: 0.3667\n",
      "Epoch [1/1], Step [2235/3504], Loss: 0.4509\n",
      "Epoch [1/1], Step [2236/3504], Loss: 0.3004\n",
      "Epoch [1/1], Step [2237/3504], Loss: 0.4391\n",
      "Epoch [1/1], Step [2238/3504], Loss: 0.4584\n",
      "Epoch [1/1], Step [2239/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [2240/3504], Loss: 0.4839\n",
      "Epoch [1/1], Step [2241/3504], Loss: 0.4517\n",
      "Epoch [1/1], Step [2242/3504], Loss: 0.4100\n",
      "Epoch [1/1], Step [2243/3504], Loss: 0.4068\n",
      "Epoch [1/1], Step [2244/3504], Loss: 0.4249\n",
      "Epoch [1/1], Step [2245/3504], Loss: 0.4777\n",
      "Epoch [1/1], Step [2246/3504], Loss: 0.3976\n",
      "Epoch [1/1], Step [2247/3504], Loss: 0.4457\n",
      "Epoch [1/1], Step [2248/3504], Loss: 0.4237\n",
      "Epoch [1/1], Step [2249/3504], Loss: 0.4443\n",
      "Epoch [1/1], Step [2250/3504], Loss: 0.5349\n",
      "Epoch [1/1], Step [2251/3504], Loss: 0.4320\n",
      "Epoch [1/1], Step [2252/3504], Loss: 0.5120\n",
      "Epoch [1/1], Step [2253/3504], Loss: 0.3721\n",
      "Epoch [1/1], Step [2254/3504], Loss: 0.4119\n",
      "Epoch [1/1], Step [2255/3504], Loss: 0.3610\n",
      "Epoch [1/1], Step [2256/3504], Loss: 0.4423\n",
      "Epoch [1/1], Step [2257/3504], Loss: 0.4043\n",
      "Epoch [1/1], Step [2258/3504], Loss: 0.4870\n",
      "Epoch [1/1], Step [2259/3504], Loss: 0.4873\n",
      "Epoch [1/1], Step [2260/3504], Loss: 0.4592\n",
      "Epoch [1/1], Step [2261/3504], Loss: 0.4032\n",
      "Epoch [1/1], Step [2262/3504], Loss: 0.3948\n",
      "Epoch [1/1], Step [2263/3504], Loss: 0.3989\n",
      "Epoch [1/1], Step [2264/3504], Loss: 0.4889\n",
      "Epoch [1/1], Step [2265/3504], Loss: 0.5298\n",
      "Epoch [1/1], Step [2266/3504], Loss: 0.3194\n",
      "Epoch [1/1], Step [2267/3504], Loss: 0.4406\n",
      "Epoch [1/1], Step [2268/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [2269/3504], Loss: 0.3967\n",
      "Epoch [1/1], Step [2270/3504], Loss: 0.3627\n",
      "Epoch [1/1], Step [2271/3504], Loss: 0.3125\n",
      "Epoch [1/1], Step [2272/3504], Loss: 0.4897\n",
      "Epoch [1/1], Step [2273/3504], Loss: 0.3202\n",
      "Epoch [1/1], Step [2274/3504], Loss: 0.5934\n",
      "Epoch [1/1], Step [2275/3504], Loss: 0.4495\n",
      "Epoch [1/1], Step [2276/3504], Loss: 0.4058\n",
      "Epoch [1/1], Step [2277/3504], Loss: 0.4459\n",
      "Epoch [1/1], Step [2278/3504], Loss: 0.3362\n",
      "Epoch [1/1], Step [2279/3504], Loss: 0.4938\n",
      "Epoch [1/1], Step [2280/3504], Loss: 0.5450\n",
      "Epoch [1/1], Step [2281/3504], Loss: 0.5481\n",
      "Epoch [1/1], Step [2282/3504], Loss: 0.3778\n",
      "Epoch [1/1], Step [2283/3504], Loss: 0.4346\n",
      "Epoch [1/1], Step [2284/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [2285/3504], Loss: 0.6663\n",
      "Epoch [1/1], Step [2286/3504], Loss: 0.4346\n",
      "Epoch [1/1], Step [2287/3504], Loss: 0.4712\n",
      "Epoch [1/1], Step [2288/3504], Loss: 0.4315\n",
      "Epoch [1/1], Step [2289/3504], Loss: 0.5283\n",
      "Epoch [1/1], Step [2290/3504], Loss: 0.4250\n",
      "Epoch [1/1], Step [2291/3504], Loss: 0.4736\n",
      "Epoch [1/1], Step [2292/3504], Loss: 0.4274\n",
      "Epoch [1/1], Step [2293/3504], Loss: 0.3724\n",
      "Epoch [1/1], Step [2294/3504], Loss: 0.3933\n",
      "Epoch [1/1], Step [2295/3504], Loss: 0.3550\n",
      "Epoch [1/1], Step [2296/3504], Loss: 0.4362\n",
      "Epoch [1/1], Step [2297/3504], Loss: 0.5026\n",
      "Epoch [1/1], Step [2298/3504], Loss: 0.3052\n",
      "Epoch [1/1], Step [2299/3504], Loss: 0.4149\n",
      "Epoch [1/1], Step [2300/3504], Loss: 0.4488\n",
      "Epoch [1/1], Step [2301/3504], Loss: 0.5140\n",
      "Epoch [1/1], Step [2302/3504], Loss: 0.4955\n",
      "Epoch [1/1], Step [2303/3504], Loss: 0.5471\n",
      "Epoch [1/1], Step [2304/3504], Loss: 0.3843\n",
      "Epoch [1/1], Step [2305/3504], Loss: 0.4132\n",
      "Epoch [1/1], Step [2306/3504], Loss: 0.4265\n",
      "Epoch [1/1], Step [2307/3504], Loss: 0.4267\n",
      "Epoch [1/1], Step [2308/3504], Loss: 0.4986\n",
      "Epoch [1/1], Step [2309/3504], Loss: 0.4414\n",
      "Epoch [1/1], Step [2310/3504], Loss: 0.3770\n",
      "Epoch [1/1], Step [2311/3504], Loss: 0.4629\n",
      "Epoch [1/1], Step [2312/3504], Loss: 0.4011\n",
      "Epoch [1/1], Step [2313/3504], Loss: 0.4599\n",
      "Epoch [1/1], Step [2314/3504], Loss: 0.4290\n",
      "Epoch [1/1], Step [2315/3504], Loss: 0.5054\n",
      "Epoch [1/1], Step [2316/3504], Loss: 0.5393\n",
      "Epoch [1/1], Step [2317/3504], Loss: 0.4649\n",
      "Epoch [1/1], Step [2318/3504], Loss: 0.4267\n",
      "Epoch [1/1], Step [2319/3504], Loss: 0.4019\n",
      "Epoch [1/1], Step [2320/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [2321/3504], Loss: 0.5005\n",
      "Epoch [1/1], Step [2322/3504], Loss: 0.3991\n",
      "Epoch [1/1], Step [2323/3504], Loss: 0.4690\n",
      "Epoch [1/1], Step [2324/3504], Loss: 0.2137\n",
      "Epoch [1/1], Step [2325/3504], Loss: 0.3650\n",
      "Epoch [1/1], Step [2326/3504], Loss: 0.4990\n",
      "Epoch [1/1], Step [2327/3504], Loss: 0.4715\n",
      "Epoch [1/1], Step [2328/3504], Loss: 0.3538\n",
      "Epoch [1/1], Step [2329/3504], Loss: 0.4638\n",
      "Epoch [1/1], Step [2330/3504], Loss: 0.4062\n",
      "Epoch [1/1], Step [2331/3504], Loss: 0.4480\n",
      "Epoch [1/1], Step [2332/3504], Loss: 0.5225\n",
      "Epoch [1/1], Step [2333/3504], Loss: 0.3733\n",
      "Epoch [1/1], Step [2334/3504], Loss: 0.3836\n",
      "Epoch [1/1], Step [2335/3504], Loss: 0.4437\n",
      "Epoch [1/1], Step [2336/3504], Loss: 0.4087\n",
      "Epoch [1/1], Step [2337/3504], Loss: 0.4495\n",
      "Epoch [1/1], Step [2338/3504], Loss: 0.5630\n",
      "Epoch [1/1], Step [2339/3504], Loss: 0.4358\n",
      "Epoch [1/1], Step [2340/3504], Loss: 0.4835\n",
      "Epoch [1/1], Step [2341/3504], Loss: 0.3490\n",
      "Epoch [1/1], Step [2342/3504], Loss: 0.4292\n",
      "Epoch [1/1], Step [2343/3504], Loss: 0.2757\n",
      "Epoch [1/1], Step [2344/3504], Loss: 0.3960\n",
      "Epoch [1/1], Step [2345/3504], Loss: 0.4500\n",
      "Epoch [1/1], Step [2346/3504], Loss: 0.4459\n",
      "Epoch [1/1], Step [2347/3504], Loss: 0.5213\n",
      "Epoch [1/1], Step [2348/3504], Loss: 0.4151\n",
      "Epoch [1/1], Step [2349/3504], Loss: 0.4557\n",
      "Epoch [1/1], Step [2350/3504], Loss: 0.3895\n",
      "Epoch [1/1], Step [2351/3504], Loss: 0.5373\n",
      "Epoch [1/1], Step [2352/3504], Loss: 0.2559\n",
      "Epoch [1/1], Step [2353/3504], Loss: 0.3452\n",
      "Epoch [1/1], Step [2354/3504], Loss: 0.3977\n",
      "Epoch [1/1], Step [2355/3504], Loss: 0.4993\n",
      "Epoch [1/1], Step [2356/3504], Loss: 0.3582\n",
      "Epoch [1/1], Step [2357/3504], Loss: 0.5248\n",
      "Epoch [1/1], Step [2358/3504], Loss: 0.3850\n",
      "Epoch [1/1], Step [2359/3504], Loss: 0.4614\n",
      "Epoch [1/1], Step [2360/3504], Loss: 0.3585\n",
      "Epoch [1/1], Step [2361/3504], Loss: 0.4656\n",
      "Epoch [1/1], Step [2362/3504], Loss: 0.4678\n",
      "Epoch [1/1], Step [2363/3504], Loss: 0.4333\n",
      "Epoch [1/1], Step [2364/3504], Loss: 0.3981\n",
      "Epoch [1/1], Step [2365/3504], Loss: 0.3993\n",
      "Epoch [1/1], Step [2366/3504], Loss: 0.4394\n",
      "Epoch [1/1], Step [2367/3504], Loss: 0.5341\n",
      "Epoch [1/1], Step [2368/3504], Loss: 0.4641\n",
      "Epoch [1/1], Step [2369/3504], Loss: 0.3771\n",
      "Epoch [1/1], Step [2370/3504], Loss: 0.4319\n",
      "Epoch [1/1], Step [2371/3504], Loss: 0.4918\n",
      "Epoch [1/1], Step [2372/3504], Loss: 0.4459\n",
      "Epoch [1/1], Step [2373/3504], Loss: 0.4049\n",
      "Epoch [1/1], Step [2374/3504], Loss: 0.4409\n",
      "Epoch [1/1], Step [2375/3504], Loss: 0.3420\n",
      "Epoch [1/1], Step [2376/3504], Loss: 0.4725\n",
      "Epoch [1/1], Step [2377/3504], Loss: 0.5191\n",
      "Epoch [1/1], Step [2378/3504], Loss: 0.4415\n",
      "Epoch [1/1], Step [2379/3504], Loss: 0.3057\n",
      "Epoch [1/1], Step [2380/3504], Loss: 0.4096\n",
      "Epoch [1/1], Step [2381/3504], Loss: 0.4373\n",
      "Epoch [1/1], Step [2382/3504], Loss: 0.3811\n",
      "Epoch [1/1], Step [2383/3504], Loss: 0.3731\n",
      "Epoch [1/1], Step [2384/3504], Loss: 0.3958\n",
      "Epoch [1/1], Step [2385/3504], Loss: 0.3241\n",
      "Epoch [1/1], Step [2386/3504], Loss: 0.3973\n",
      "Epoch [1/1], Step [2387/3504], Loss: 0.4999\n",
      "Epoch [1/1], Step [2388/3504], Loss: 0.3801\n",
      "Epoch [1/1], Step [2389/3504], Loss: 0.3154\n",
      "Epoch [1/1], Step [2390/3504], Loss: 0.4168\n",
      "Epoch [1/1], Step [2391/3504], Loss: 0.4889\n",
      "Epoch [1/1], Step [2392/3504], Loss: 0.3700\n",
      "Epoch [1/1], Step [2393/3504], Loss: 0.5288\n",
      "Epoch [1/1], Step [2394/3504], Loss: 0.3857\n",
      "Epoch [1/1], Step [2395/3504], Loss: 0.3537\n",
      "Epoch [1/1], Step [2396/3504], Loss: 0.5538\n",
      "Epoch [1/1], Step [2397/3504], Loss: 0.4209\n",
      "Epoch [1/1], Step [2398/3504], Loss: 0.3330\n",
      "Epoch [1/1], Step [2399/3504], Loss: 0.3940\n",
      "Epoch [1/1], Step [2400/3504], Loss: 0.4339\n",
      "Epoch [1/1], Step [2401/3504], Loss: 0.3273\n",
      "Epoch [1/1], Step [2402/3504], Loss: 0.5277\n",
      "Epoch [1/1], Step [2403/3504], Loss: 0.5226\n",
      "Epoch [1/1], Step [2404/3504], Loss: 0.3790\n",
      "Epoch [1/1], Step [2405/3504], Loss: 0.3482\n",
      "Epoch [1/1], Step [2406/3504], Loss: 0.3284\n",
      "Epoch [1/1], Step [2407/3504], Loss: 0.4177\n",
      "Epoch [1/1], Step [2408/3504], Loss: 0.3064\n",
      "Epoch [1/1], Step [2409/3504], Loss: 0.4677\n",
      "Epoch [1/1], Step [2410/3504], Loss: 0.3780\n",
      "Epoch [1/1], Step [2411/3504], Loss: 0.4345\n",
      "Epoch [1/1], Step [2412/3504], Loss: 0.6231\n",
      "Epoch [1/1], Step [2413/3504], Loss: 0.4233\n",
      "Epoch [1/1], Step [2414/3504], Loss: 0.5663\n",
      "Epoch [1/1], Step [2415/3504], Loss: 0.3752\n",
      "Epoch [1/1], Step [2416/3504], Loss: 0.4531\n",
      "Epoch [1/1], Step [2417/3504], Loss: 0.5509\n",
      "Epoch [1/1], Step [2418/3504], Loss: 0.4947\n",
      "Epoch [1/1], Step [2419/3504], Loss: 0.4717\n",
      "Epoch [1/1], Step [2420/3504], Loss: 0.5745\n",
      "Epoch [1/1], Step [2421/3504], Loss: 0.3811\n",
      "Epoch [1/1], Step [2422/3504], Loss: 0.4414\n",
      "Epoch [1/1], Step [2423/3504], Loss: 0.4305\n",
      "Epoch [1/1], Step [2424/3504], Loss: 0.3647\n",
      "Epoch [1/1], Step [2425/3504], Loss: 0.4285\n",
      "Epoch [1/1], Step [2426/3504], Loss: 0.5239\n",
      "Epoch [1/1], Step [2427/3504], Loss: 0.3908\n",
      "Epoch [1/1], Step [2428/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [2429/3504], Loss: 0.5207\n",
      "Epoch [1/1], Step [2430/3504], Loss: 0.3407\n",
      "Epoch [1/1], Step [2431/3504], Loss: 0.5287\n",
      "Epoch [1/1], Step [2432/3504], Loss: 0.4238\n",
      "Epoch [1/1], Step [2433/3504], Loss: 0.5119\n",
      "Epoch [1/1], Step [2434/3504], Loss: 0.4131\n",
      "Epoch [1/1], Step [2435/3504], Loss: 0.4031\n",
      "Epoch [1/1], Step [2436/3504], Loss: 0.4309\n",
      "Epoch [1/1], Step [2437/3504], Loss: 0.3522\n",
      "Epoch [1/1], Step [2438/3504], Loss: 0.3866\n",
      "Epoch [1/1], Step [2439/3504], Loss: 0.4743\n",
      "Epoch [1/1], Step [2440/3504], Loss: 0.4693\n",
      "Epoch [1/1], Step [2441/3504], Loss: 0.3122\n",
      "Epoch [1/1], Step [2442/3504], Loss: 0.4192\n",
      "Epoch [1/1], Step [2443/3504], Loss: 0.4016\n",
      "Epoch [1/1], Step [2444/3504], Loss: 0.4663\n",
      "Epoch [1/1], Step [2445/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [2446/3504], Loss: 0.3298\n",
      "Epoch [1/1], Step [2447/3504], Loss: 0.4188\n",
      "Epoch [1/1], Step [2448/3504], Loss: 0.4050\n",
      "Epoch [1/1], Step [2449/3504], Loss: 0.3490\n",
      "Epoch [1/1], Step [2450/3504], Loss: 0.4493\n",
      "Epoch [1/1], Step [2451/3504], Loss: 0.3896\n",
      "Epoch [1/1], Step [2452/3504], Loss: 0.4827\n",
      "Epoch [1/1], Step [2453/3504], Loss: 0.3394\n",
      "Epoch [1/1], Step [2454/3504], Loss: 0.5822\n",
      "Epoch [1/1], Step [2455/3504], Loss: 0.2750\n",
      "Epoch [1/1], Step [2456/3504], Loss: 0.3237\n",
      "Epoch [1/1], Step [2457/3504], Loss: 0.5022\n",
      "Epoch [1/1], Step [2458/3504], Loss: 0.3524\n",
      "Epoch [1/1], Step [2459/3504], Loss: 0.3601\n",
      "Epoch [1/1], Step [2460/3504], Loss: 0.4306\n",
      "Epoch [1/1], Step [2461/3504], Loss: 0.4240\n",
      "Epoch [1/1], Step [2462/3504], Loss: 0.3230\n",
      "Epoch [1/1], Step [2463/3504], Loss: 0.3403\n",
      "Epoch [1/1], Step [2464/3504], Loss: 0.4989\n",
      "Epoch [1/1], Step [2465/3504], Loss: 0.4837\n",
      "Epoch [1/1], Step [2466/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [2467/3504], Loss: 0.4295\n",
      "Epoch [1/1], Step [2468/3504], Loss: 0.3774\n",
      "Epoch [1/1], Step [2469/3504], Loss: 0.3766\n",
      "Epoch [1/1], Step [2470/3504], Loss: 0.4911\n",
      "Epoch [1/1], Step [2471/3504], Loss: 0.3606\n",
      "Epoch [1/1], Step [2472/3504], Loss: 0.3919\n",
      "Epoch [1/1], Step [2473/3504], Loss: 0.3970\n",
      "Epoch [1/1], Step [2474/3504], Loss: 0.4170\n",
      "Epoch [1/1], Step [2475/3504], Loss: 0.4947\n",
      "Epoch [1/1], Step [2476/3504], Loss: 0.4578\n",
      "Epoch [1/1], Step [2477/3504], Loss: 0.6492\n",
      "Epoch [1/1], Step [2478/3504], Loss: 0.5177\n",
      "Epoch [1/1], Step [2479/3504], Loss: 0.4104\n",
      "Epoch [1/1], Step [2480/3504], Loss: 0.4930\n",
      "Epoch [1/1], Step [2481/3504], Loss: 0.4648\n",
      "Epoch [1/1], Step [2482/3504], Loss: 0.3196\n",
      "Epoch [1/1], Step [2483/3504], Loss: 0.5091\n",
      "Epoch [1/1], Step [2484/3504], Loss: 0.4764\n",
      "Epoch [1/1], Step [2485/3504], Loss: 0.5109\n",
      "Epoch [1/1], Step [2486/3504], Loss: 0.2964\n",
      "Epoch [1/1], Step [2487/3504], Loss: 0.5910\n",
      "Epoch [1/1], Step [2488/3504], Loss: 0.4583\n",
      "Epoch [1/1], Step [2489/3504], Loss: 0.4289\n",
      "Epoch [1/1], Step [2490/3504], Loss: 0.3766\n",
      "Epoch [1/1], Step [2491/3504], Loss: 0.4157\n",
      "Epoch [1/1], Step [2492/3504], Loss: 0.4468\n",
      "Epoch [1/1], Step [2493/3504], Loss: 0.4432\n",
      "Epoch [1/1], Step [2494/3504], Loss: 0.4503\n",
      "Epoch [1/1], Step [2495/3504], Loss: 0.3171\n",
      "Epoch [1/1], Step [2496/3504], Loss: 0.4541\n",
      "Epoch [1/1], Step [2497/3504], Loss: 0.4182\n",
      "Epoch [1/1], Step [2498/3504], Loss: 0.6376\n",
      "Epoch [1/1], Step [2499/3504], Loss: 0.4144\n",
      "Epoch [1/1], Step [2500/3504], Loss: 0.3788\n",
      "Epoch [1/1], Step [2501/3504], Loss: 0.4993\n",
      "Epoch [1/1], Step [2502/3504], Loss: 0.4792\n",
      "Epoch [1/1], Step [2503/3504], Loss: 0.4278\n",
      "Epoch [1/1], Step [2504/3504], Loss: 0.5062\n",
      "Epoch [1/1], Step [2505/3504], Loss: 0.4987\n",
      "Epoch [1/1], Step [2506/3504], Loss: 0.4944\n",
      "Epoch [1/1], Step [2507/3504], Loss: 0.3772\n",
      "Epoch [1/1], Step [2508/3504], Loss: 0.3529\n",
      "Epoch [1/1], Step [2509/3504], Loss: 0.4183\n",
      "Epoch [1/1], Step [2510/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [2511/3504], Loss: 0.5305\n",
      "Epoch [1/1], Step [2512/3504], Loss: 0.4601\n",
      "Epoch [1/1], Step [2513/3504], Loss: 0.3408\n",
      "Epoch [1/1], Step [2514/3504], Loss: 0.3711\n",
      "Epoch [1/1], Step [2515/3504], Loss: 0.5161\n",
      "Epoch [1/1], Step [2516/3504], Loss: 0.4966\n",
      "Epoch [1/1], Step [2517/3504], Loss: 0.4178\n",
      "Epoch [1/1], Step [2518/3504], Loss: 0.5863\n",
      "Epoch [1/1], Step [2519/3504], Loss: 0.4356\n",
      "Epoch [1/1], Step [2520/3504], Loss: 0.4688\n",
      "Epoch [1/1], Step [2521/3504], Loss: 0.3927\n",
      "Epoch [1/1], Step [2522/3504], Loss: 0.3397\n",
      "Epoch [1/1], Step [2523/3504], Loss: 0.3201\n",
      "Epoch [1/1], Step [2524/3504], Loss: 0.4027\n",
      "Epoch [1/1], Step [2525/3504], Loss: 0.3680\n",
      "Epoch [1/1], Step [2526/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [2527/3504], Loss: 0.3738\n",
      "Epoch [1/1], Step [2528/3504], Loss: 0.4234\n",
      "Epoch [1/1], Step [2529/3504], Loss: 0.4289\n",
      "Epoch [1/1], Step [2530/3504], Loss: 0.2896\n",
      "Epoch [1/1], Step [2531/3504], Loss: 0.3723\n",
      "Epoch [1/1], Step [2532/3504], Loss: 0.4279\n",
      "Epoch [1/1], Step [2533/3504], Loss: 0.3876\n",
      "Epoch [1/1], Step [2534/3504], Loss: 0.3101\n",
      "Epoch [1/1], Step [2535/3504], Loss: 0.3335\n",
      "Epoch [1/1], Step [2536/3504], Loss: 0.3559\n",
      "Epoch [1/1], Step [2537/3504], Loss: 0.4812\n",
      "Epoch [1/1], Step [2538/3504], Loss: 0.5125\n",
      "Epoch [1/1], Step [2539/3504], Loss: 0.3429\n",
      "Epoch [1/1], Step [2540/3504], Loss: 0.4300\n",
      "Epoch [1/1], Step [2541/3504], Loss: 0.4534\n",
      "Epoch [1/1], Step [2542/3504], Loss: 0.4073\n",
      "Epoch [1/1], Step [2543/3504], Loss: 0.3985\n",
      "Epoch [1/1], Step [2544/3504], Loss: 0.3835\n",
      "Epoch [1/1], Step [2545/3504], Loss: 0.3394\n",
      "Epoch [1/1], Step [2546/3504], Loss: 0.4289\n",
      "Epoch [1/1], Step [2547/3504], Loss: 0.4481\n",
      "Epoch [1/1], Step [2548/3504], Loss: 0.4493\n",
      "Epoch [1/1], Step [2549/3504], Loss: 0.4375\n",
      "Epoch [1/1], Step [2550/3504], Loss: 0.5205\n",
      "Epoch [1/1], Step [2551/3504], Loss: 0.4945\n",
      "Epoch [1/1], Step [2552/3504], Loss: 0.3867\n",
      "Epoch [1/1], Step [2553/3504], Loss: 0.3443\n",
      "Epoch [1/1], Step [2554/3504], Loss: 0.5229\n",
      "Epoch [1/1], Step [2555/3504], Loss: 0.4142\n",
      "Epoch [1/1], Step [2556/3504], Loss: 0.4242\n",
      "Epoch [1/1], Step [2557/3504], Loss: 0.3439\n",
      "Epoch [1/1], Step [2558/3504], Loss: 0.4385\n",
      "Epoch [1/1], Step [2559/3504], Loss: 0.4224\n",
      "Epoch [1/1], Step [2560/3504], Loss: 0.4591\n",
      "Epoch [1/1], Step [2561/3504], Loss: 0.4302\n",
      "Epoch [1/1], Step [2562/3504], Loss: 0.3991\n",
      "Epoch [1/1], Step [2563/3504], Loss: 0.4620\n",
      "Epoch [1/1], Step [2564/3504], Loss: 0.3393\n",
      "Epoch [1/1], Step [2565/3504], Loss: 0.4861\n",
      "Epoch [1/1], Step [2566/3504], Loss: 0.4607\n",
      "Epoch [1/1], Step [2567/3504], Loss: 0.4648\n",
      "Epoch [1/1], Step [2568/3504], Loss: 0.4159\n",
      "Epoch [1/1], Step [2569/3504], Loss: 0.3651\n",
      "Epoch [1/1], Step [2570/3504], Loss: 0.4533\n",
      "Epoch [1/1], Step [2571/3504], Loss: 0.4818\n",
      "Epoch [1/1], Step [2572/3504], Loss: 0.5594\n",
      "Epoch [1/1], Step [2573/3504], Loss: 0.4596\n",
      "Epoch [1/1], Step [2574/3504], Loss: 0.4142\n",
      "Epoch [1/1], Step [2575/3504], Loss: 0.5544\n",
      "Epoch [1/1], Step [2576/3504], Loss: 0.3924\n",
      "Epoch [1/1], Step [2577/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [2578/3504], Loss: 0.5281\n",
      "Epoch [1/1], Step [2579/3504], Loss: 0.4378\n",
      "Epoch [1/1], Step [2580/3504], Loss: 0.5604\n",
      "Epoch [1/1], Step [2581/3504], Loss: 0.4203\n",
      "Epoch [1/1], Step [2582/3504], Loss: 0.3285\n",
      "Epoch [1/1], Step [2583/3504], Loss: 0.3923\n",
      "Epoch [1/1], Step [2584/3504], Loss: 0.2608\n",
      "Epoch [1/1], Step [2585/3504], Loss: 0.4222\n",
      "Epoch [1/1], Step [2586/3504], Loss: 0.4409\n",
      "Epoch [1/1], Step [2587/3504], Loss: 0.4759\n",
      "Epoch [1/1], Step [2588/3504], Loss: 0.3356\n",
      "Epoch [1/1], Step [2589/3504], Loss: 0.3756\n",
      "Epoch [1/1], Step [2590/3504], Loss: 0.3830\n",
      "Epoch [1/1], Step [2591/3504], Loss: 0.4928\n",
      "Epoch [1/1], Step [2592/3504], Loss: 0.5038\n",
      "Epoch [1/1], Step [2593/3504], Loss: 0.4561\n",
      "Epoch [1/1], Step [2594/3504], Loss: 0.3737\n",
      "Epoch [1/1], Step [2595/3504], Loss: 0.4498\n",
      "Epoch [1/1], Step [2596/3504], Loss: 0.4648\n",
      "Epoch [1/1], Step [2597/3504], Loss: 0.4092\n",
      "Epoch [1/1], Step [2598/3504], Loss: 0.5202\n",
      "Epoch [1/1], Step [2599/3504], Loss: 0.5356\n",
      "Epoch [1/1], Step [2600/3504], Loss: 0.6193\n",
      "Epoch [1/1], Step [2601/3504], Loss: 0.4436\n",
      "Epoch [1/1], Step [2602/3504], Loss: 0.4623\n",
      "Epoch [1/1], Step [2603/3504], Loss: 0.4583\n",
      "Epoch [1/1], Step [2604/3504], Loss: 0.4904\n",
      "Epoch [1/1], Step [2605/3504], Loss: 0.4444\n",
      "Epoch [1/1], Step [2606/3504], Loss: 0.4356\n",
      "Epoch [1/1], Step [2607/3504], Loss: 0.4915\n",
      "Epoch [1/1], Step [2608/3504], Loss: 0.4240\n",
      "Epoch [1/1], Step [2609/3504], Loss: 0.3741\n",
      "Epoch [1/1], Step [2610/3504], Loss: 0.4681\n",
      "Epoch [1/1], Step [2611/3504], Loss: 0.3268\n",
      "Epoch [1/1], Step [2612/3504], Loss: 0.4390\n",
      "Epoch [1/1], Step [2613/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [2614/3504], Loss: 0.6499\n",
      "Epoch [1/1], Step [2615/3504], Loss: 0.5548\n",
      "Epoch [1/1], Step [2616/3504], Loss: 0.4035\n",
      "Epoch [1/1], Step [2617/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [2618/3504], Loss: 0.3665\n",
      "Epoch [1/1], Step [2619/3504], Loss: 0.5561\n",
      "Epoch [1/1], Step [2620/3504], Loss: 0.5450\n",
      "Epoch [1/1], Step [2621/3504], Loss: 0.3432\n",
      "Epoch [1/1], Step [2622/3504], Loss: 0.4238\n",
      "Epoch [1/1], Step [2623/3504], Loss: 0.3869\n",
      "Epoch [1/1], Step [2624/3504], Loss: 0.4267\n",
      "Epoch [1/1], Step [2625/3504], Loss: 0.4333\n",
      "Epoch [1/1], Step [2626/3504], Loss: 0.5783\n",
      "Epoch [1/1], Step [2627/3504], Loss: 0.4973\n",
      "Epoch [1/1], Step [2628/3504], Loss: 0.3297\n",
      "Epoch [1/1], Step [2629/3504], Loss: 0.4934\n",
      "Epoch [1/1], Step [2630/3504], Loss: 0.5406\n",
      "Epoch [1/1], Step [2631/3504], Loss: 0.3846\n",
      "Epoch [1/1], Step [2632/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [2633/3504], Loss: 0.3782\n",
      "Epoch [1/1], Step [2634/3504], Loss: 0.3835\n",
      "Epoch [1/1], Step [2635/3504], Loss: 0.3728\n",
      "Epoch [1/1], Step [2636/3504], Loss: 0.4478\n",
      "Epoch [1/1], Step [2637/3504], Loss: 0.4101\n",
      "Epoch [1/1], Step [2638/3504], Loss: 0.3262\n",
      "Epoch [1/1], Step [2639/3504], Loss: 0.4149\n",
      "Epoch [1/1], Step [2640/3504], Loss: 0.4882\n",
      "Epoch [1/1], Step [2641/3504], Loss: 0.4134\n",
      "Epoch [1/1], Step [2642/3504], Loss: 0.3383\n",
      "Epoch [1/1], Step [2643/3504], Loss: 0.2861\n",
      "Epoch [1/1], Step [2644/3504], Loss: 0.3927\n",
      "Epoch [1/1], Step [2645/3504], Loss: 0.5081\n",
      "Epoch [1/1], Step [2646/3504], Loss: 0.3818\n",
      "Epoch [1/1], Step [2647/3504], Loss: 0.5010\n",
      "Epoch [1/1], Step [2648/3504], Loss: 0.4089\n",
      "Epoch [1/1], Step [2649/3504], Loss: 0.3860\n",
      "Epoch [1/1], Step [2650/3504], Loss: 0.3761\n",
      "Epoch [1/1], Step [2651/3504], Loss: 0.4308\n",
      "Epoch [1/1], Step [2652/3504], Loss: 0.4928\n",
      "Epoch [1/1], Step [2653/3504], Loss: 0.5517\n",
      "Epoch [1/1], Step [2654/3504], Loss: 0.3147\n",
      "Epoch [1/1], Step [2655/3504], Loss: 0.3886\n",
      "Epoch [1/1], Step [2656/3504], Loss: 0.4447\n",
      "Epoch [1/1], Step [2657/3504], Loss: 0.5247\n",
      "Epoch [1/1], Step [2658/3504], Loss: 0.4364\n",
      "Epoch [1/1], Step [2659/3504], Loss: 0.5183\n",
      "Epoch [1/1], Step [2660/3504], Loss: 0.4283\n",
      "Epoch [1/1], Step [2661/3504], Loss: 0.4829\n",
      "Epoch [1/1], Step [2662/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [2663/3504], Loss: 0.3484\n",
      "Epoch [1/1], Step [2664/3504], Loss: 0.4673\n",
      "Epoch [1/1], Step [2665/3504], Loss: 0.4634\n",
      "Epoch [1/1], Step [2666/3504], Loss: 0.5034\n",
      "Epoch [1/1], Step [2667/3504], Loss: 0.4660\n",
      "Epoch [1/1], Step [2668/3504], Loss: 0.4849\n",
      "Epoch [1/1], Step [2669/3504], Loss: 0.3692\n",
      "Epoch [1/1], Step [2670/3504], Loss: 0.5304\n",
      "Epoch [1/1], Step [2671/3504], Loss: 0.2620\n",
      "Epoch [1/1], Step [2672/3504], Loss: 0.4291\n",
      "Epoch [1/1], Step [2673/3504], Loss: 0.3889\n",
      "Epoch [1/1], Step [2674/3504], Loss: 0.5234\n",
      "Epoch [1/1], Step [2675/3504], Loss: 0.4392\n",
      "Epoch [1/1], Step [2676/3504], Loss: 0.4575\n",
      "Epoch [1/1], Step [2677/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [2678/3504], Loss: 0.3525\n",
      "Epoch [1/1], Step [2679/3504], Loss: 0.4706\n",
      "Epoch [1/1], Step [2680/3504], Loss: 0.6027\n",
      "Epoch [1/1], Step [2681/3504], Loss: 0.3763\n",
      "Epoch [1/1], Step [2682/3504], Loss: 0.4506\n",
      "Epoch [1/1], Step [2683/3504], Loss: 0.4768\n",
      "Epoch [1/1], Step [2684/3504], Loss: 0.5235\n",
      "Epoch [1/1], Step [2685/3504], Loss: 0.4537\n",
      "Epoch [1/1], Step [2686/3504], Loss: 0.4920\n",
      "Epoch [1/1], Step [2687/3504], Loss: 0.5664\n",
      "Epoch [1/1], Step [2688/3504], Loss: 0.4492\n",
      "Epoch [1/1], Step [2689/3504], Loss: 0.4147\n",
      "Epoch [1/1], Step [2690/3504], Loss: 0.4079\n",
      "Epoch [1/1], Step [2691/3504], Loss: 0.4394\n",
      "Epoch [1/1], Step [2692/3504], Loss: 0.4309\n",
      "Epoch [1/1], Step [2693/3504], Loss: 0.5565\n",
      "Epoch [1/1], Step [2694/3504], Loss: 0.4594\n",
      "Epoch [1/1], Step [2695/3504], Loss: 0.3975\n",
      "Epoch [1/1], Step [2696/3504], Loss: 0.5220\n",
      "Epoch [1/1], Step [2697/3504], Loss: 0.4780\n",
      "Epoch [1/1], Step [2698/3504], Loss: 0.4165\n",
      "Epoch [1/1], Step [2699/3504], Loss: 0.3998\n",
      "Epoch [1/1], Step [2700/3504], Loss: 0.3846\n",
      "Epoch [1/1], Step [2701/3504], Loss: 0.3980\n",
      "Epoch [1/1], Step [2702/3504], Loss: 0.4007\n",
      "Epoch [1/1], Step [2703/3504], Loss: 0.5117\n",
      "Epoch [1/1], Step [2704/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [2705/3504], Loss: 0.3307\n",
      "Epoch [1/1], Step [2706/3504], Loss: 0.5535\n",
      "Epoch [1/1], Step [2707/3504], Loss: 0.4876\n",
      "Epoch [1/1], Step [2708/3504], Loss: 0.3876\n",
      "Epoch [1/1], Step [2709/3504], Loss: 0.3867\n",
      "Epoch [1/1], Step [2710/3504], Loss: 0.4126\n",
      "Epoch [1/1], Step [2711/3504], Loss: 0.3287\n",
      "Epoch [1/1], Step [2712/3504], Loss: 0.4203\n",
      "Epoch [1/1], Step [2713/3504], Loss: 0.4510\n",
      "Epoch [1/1], Step [2714/3504], Loss: 0.4348\n",
      "Epoch [1/1], Step [2715/3504], Loss: 0.4263\n",
      "Epoch [1/1], Step [2716/3504], Loss: 0.2915\n",
      "Epoch [1/1], Step [2717/3504], Loss: 0.3566\n",
      "Epoch [1/1], Step [2718/3504], Loss: 0.3588\n",
      "Epoch [1/1], Step [2719/3504], Loss: 0.4316\n",
      "Epoch [1/1], Step [2720/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [2721/3504], Loss: 0.5612\n",
      "Epoch [1/1], Step [2722/3504], Loss: 0.4717\n",
      "Epoch [1/1], Step [2723/3504], Loss: 0.5110\n",
      "Epoch [1/1], Step [2724/3504], Loss: 0.4239\n",
      "Epoch [1/1], Step [2725/3504], Loss: 0.4484\n",
      "Epoch [1/1], Step [2726/3504], Loss: 0.3234\n",
      "Epoch [1/1], Step [2727/3504], Loss: 0.4641\n",
      "Epoch [1/1], Step [2728/3504], Loss: 0.3945\n",
      "Epoch [1/1], Step [2729/3504], Loss: 0.4130\n",
      "Epoch [1/1], Step [2730/3504], Loss: 0.4348\n",
      "Epoch [1/1], Step [2731/3504], Loss: 0.4631\n",
      "Epoch [1/1], Step [2732/3504], Loss: 0.4632\n",
      "Epoch [1/1], Step [2733/3504], Loss: 0.4766\n",
      "Epoch [1/1], Step [2734/3504], Loss: 0.5625\n",
      "Epoch [1/1], Step [2735/3504], Loss: 0.4292\n",
      "Epoch [1/1], Step [2736/3504], Loss: 0.4211\n",
      "Epoch [1/1], Step [2737/3504], Loss: 0.4619\n",
      "Epoch [1/1], Step [2738/3504], Loss: 0.4311\n",
      "Epoch [1/1], Step [2739/3504], Loss: 0.5163\n",
      "Epoch [1/1], Step [2740/3504], Loss: 0.5088\n",
      "Epoch [1/1], Step [2741/3504], Loss: 0.4230\n",
      "Epoch [1/1], Step [2742/3504], Loss: 0.3153\n",
      "Epoch [1/1], Step [2743/3504], Loss: 0.4276\n",
      "Epoch [1/1], Step [2744/3504], Loss: 0.5025\n",
      "Epoch [1/1], Step [2745/3504], Loss: 0.4598\n",
      "Epoch [1/1], Step [2746/3504], Loss: 0.4806\n",
      "Epoch [1/1], Step [2747/3504], Loss: 0.6249\n",
      "Epoch [1/1], Step [2748/3504], Loss: 0.3229\n",
      "Epoch [1/1], Step [2749/3504], Loss: 0.5666\n",
      "Epoch [1/1], Step [2750/3504], Loss: 0.3179\n",
      "Epoch [1/1], Step [2751/3504], Loss: 0.3778\n",
      "Epoch [1/1], Step [2752/3504], Loss: 0.4707\n",
      "Epoch [1/1], Step [2753/3504], Loss: 0.5230\n",
      "Epoch [1/1], Step [2754/3504], Loss: 0.5101\n",
      "Epoch [1/1], Step [2755/3504], Loss: 0.4104\n",
      "Epoch [1/1], Step [2756/3504], Loss: 0.3566\n",
      "Epoch [1/1], Step [2757/3504], Loss: 0.3617\n",
      "Epoch [1/1], Step [2758/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [2759/3504], Loss: 0.4218\n",
      "Epoch [1/1], Step [2760/3504], Loss: 0.5061\n",
      "Epoch [1/1], Step [2761/3504], Loss: 0.4497\n",
      "Epoch [1/1], Step [2762/3504], Loss: 0.4688\n",
      "Epoch [1/1], Step [2763/3504], Loss: 0.4051\n",
      "Epoch [1/1], Step [2764/3504], Loss: 0.3497\n",
      "Epoch [1/1], Step [2765/3504], Loss: 0.4227\n",
      "Epoch [1/1], Step [2766/3504], Loss: 0.4083\n",
      "Epoch [1/1], Step [2767/3504], Loss: 0.4964\n",
      "Epoch [1/1], Step [2768/3504], Loss: 0.4639\n",
      "Epoch [1/1], Step [2769/3504], Loss: 0.3822\n",
      "Epoch [1/1], Step [2770/3504], Loss: 0.3520\n",
      "Epoch [1/1], Step [2771/3504], Loss: 0.3025\n",
      "Epoch [1/1], Step [2772/3504], Loss: 0.3957\n",
      "Epoch [1/1], Step [2773/3504], Loss: 0.5086\n",
      "Epoch [1/1], Step [2774/3504], Loss: 0.4074\n",
      "Epoch [1/1], Step [2775/3504], Loss: 0.5596\n",
      "Epoch [1/1], Step [2776/3504], Loss: 0.5134\n",
      "Epoch [1/1], Step [2777/3504], Loss: 0.4023\n",
      "Epoch [1/1], Step [2778/3504], Loss: 0.3999\n",
      "Epoch [1/1], Step [2779/3504], Loss: 0.3348\n",
      "Epoch [1/1], Step [2780/3504], Loss: 0.3493\n",
      "Epoch [1/1], Step [2781/3504], Loss: 0.4550\n",
      "Epoch [1/1], Step [2782/3504], Loss: 0.3972\n",
      "Epoch [1/1], Step [2783/3504], Loss: 0.4601\n",
      "Epoch [1/1], Step [2784/3504], Loss: 0.3325\n",
      "Epoch [1/1], Step [2785/3504], Loss: 0.4384\n",
      "Epoch [1/1], Step [2786/3504], Loss: 0.4163\n",
      "Epoch [1/1], Step [2787/3504], Loss: 0.4322\n",
      "Epoch [1/1], Step [2788/3504], Loss: 0.4164\n",
      "Epoch [1/1], Step [2789/3504], Loss: 0.3649\n",
      "Epoch [1/1], Step [2790/3504], Loss: 0.3725\n",
      "Epoch [1/1], Step [2791/3504], Loss: 0.4938\n",
      "Epoch [1/1], Step [2792/3504], Loss: 0.4808\n",
      "Epoch [1/1], Step [2793/3504], Loss: 0.3835\n",
      "Epoch [1/1], Step [2794/3504], Loss: 0.4881\n",
      "Epoch [1/1], Step [2795/3504], Loss: 0.3973\n",
      "Epoch [1/1], Step [2796/3504], Loss: 0.4285\n",
      "Epoch [1/1], Step [2797/3504], Loss: 0.5504\n",
      "Epoch [1/1], Step [2798/3504], Loss: 0.5956\n",
      "Epoch [1/1], Step [2799/3504], Loss: 0.4865\n",
      "Epoch [1/1], Step [2800/3504], Loss: 0.4444\n",
      "Epoch [1/1], Step [2801/3504], Loss: 0.3553\n",
      "Epoch [1/1], Step [2802/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [2803/3504], Loss: 0.4319\n",
      "Epoch [1/1], Step [2804/3504], Loss: 0.4457\n",
      "Epoch [1/1], Step [2805/3504], Loss: 0.4406\n",
      "Epoch [1/1], Step [2806/3504], Loss: 0.4410\n",
      "Epoch [1/1], Step [2807/3504], Loss: 0.4776\n",
      "Epoch [1/1], Step [2808/3504], Loss: 0.4208\n",
      "Epoch [1/1], Step [2809/3504], Loss: 0.4412\n",
      "Epoch [1/1], Step [2810/3504], Loss: 0.3967\n",
      "Epoch [1/1], Step [2811/3504], Loss: 0.3675\n",
      "Epoch [1/1], Step [2812/3504], Loss: 0.4659\n",
      "Epoch [1/1], Step [2813/3504], Loss: 0.3627\n",
      "Epoch [1/1], Step [2814/3504], Loss: 0.5780\n",
      "Epoch [1/1], Step [2815/3504], Loss: 0.3292\n",
      "Epoch [1/1], Step [2816/3504], Loss: 0.3505\n",
      "Epoch [1/1], Step [2817/3504], Loss: 0.5326\n",
      "Epoch [1/1], Step [2818/3504], Loss: 0.3176\n",
      "Epoch [1/1], Step [2819/3504], Loss: 0.3268\n",
      "Epoch [1/1], Step [2820/3504], Loss: 0.4295\n",
      "Epoch [1/1], Step [2821/3504], Loss: 0.2737\n",
      "Epoch [1/1], Step [2822/3504], Loss: 0.4264\n",
      "Epoch [1/1], Step [2823/3504], Loss: 0.4653\n",
      "Epoch [1/1], Step [2824/3504], Loss: 0.4521\n",
      "Epoch [1/1], Step [2825/3504], Loss: 0.3293\n",
      "Epoch [1/1], Step [2826/3504], Loss: 0.4560\n",
      "Epoch [1/1], Step [2827/3504], Loss: 0.5317\n",
      "Epoch [1/1], Step [2828/3504], Loss: 0.6110\n",
      "Epoch [1/1], Step [2829/3504], Loss: 0.4976\n",
      "Epoch [1/1], Step [2830/3504], Loss: 0.4245\n",
      "Epoch [1/1], Step [2831/3504], Loss: 0.3717\n",
      "Epoch [1/1], Step [2832/3504], Loss: 0.5029\n",
      "Epoch [1/1], Step [2833/3504], Loss: 0.4438\n",
      "Epoch [1/1], Step [2834/3504], Loss: 0.4772\n",
      "Epoch [1/1], Step [2835/3504], Loss: 0.4829\n",
      "Epoch [1/1], Step [2836/3504], Loss: 0.4405\n",
      "Epoch [1/1], Step [2837/3504], Loss: 0.5207\n",
      "Epoch [1/1], Step [2838/3504], Loss: 0.4429\n",
      "Epoch [1/1], Step [2839/3504], Loss: 0.5057\n",
      "Epoch [1/1], Step [2840/3504], Loss: 0.4389\n",
      "Epoch [1/1], Step [2841/3504], Loss: 0.2848\n",
      "Epoch [1/1], Step [2842/3504], Loss: 0.4998\n",
      "Epoch [1/1], Step [2843/3504], Loss: 0.4376\n",
      "Epoch [1/1], Step [2844/3504], Loss: 0.4613\n",
      "Epoch [1/1], Step [2845/3504], Loss: 0.4892\n",
      "Epoch [1/1], Step [2846/3504], Loss: 0.4937\n",
      "Epoch [1/1], Step [2847/3504], Loss: 0.3900\n",
      "Epoch [1/1], Step [2848/3504], Loss: 0.4963\n",
      "Epoch [1/1], Step [2849/3504], Loss: 0.5171\n",
      "Epoch [1/1], Step [2850/3504], Loss: 0.4247\n",
      "Epoch [1/1], Step [2851/3504], Loss: 0.6650\n",
      "Epoch [1/1], Step [2852/3504], Loss: 0.4286\n",
      "Epoch [1/1], Step [2853/3504], Loss: 0.3849\n",
      "Epoch [1/1], Step [2854/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [2855/3504], Loss: 0.3287\n",
      "Epoch [1/1], Step [2856/3504], Loss: 0.5249\n",
      "Epoch [1/1], Step [2857/3504], Loss: 0.4500\n",
      "Epoch [1/1], Step [2858/3504], Loss: 0.3757\n",
      "Epoch [1/1], Step [2859/3504], Loss: 0.4942\n",
      "Epoch [1/1], Step [2860/3504], Loss: 0.5037\n",
      "Epoch [1/1], Step [2861/3504], Loss: 0.4292\n",
      "Epoch [1/1], Step [2862/3504], Loss: 0.4611\n",
      "Epoch [1/1], Step [2863/3504], Loss: 0.3876\n",
      "Epoch [1/1], Step [2864/3504], Loss: 0.4655\n",
      "Epoch [1/1], Step [2865/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [2866/3504], Loss: 0.5476\n",
      "Epoch [1/1], Step [2867/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [2868/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [2869/3504], Loss: 0.3588\n",
      "Epoch [1/1], Step [2870/3504], Loss: 0.3814\n",
      "Epoch [1/1], Step [2871/3504], Loss: 0.4854\n",
      "Epoch [1/1], Step [2872/3504], Loss: 0.3670\n",
      "Epoch [1/1], Step [2873/3504], Loss: 0.4786\n",
      "Epoch [1/1], Step [2874/3504], Loss: 0.3911\n",
      "Epoch [1/1], Step [2875/3504], Loss: 0.4225\n",
      "Epoch [1/1], Step [2876/3504], Loss: 0.3504\n",
      "Epoch [1/1], Step [2877/3504], Loss: 0.4496\n",
      "Epoch [1/1], Step [2878/3504], Loss: 0.4049\n",
      "Epoch [1/1], Step [2879/3504], Loss: 0.3972\n",
      "Epoch [1/1], Step [2880/3504], Loss: 0.4272\n",
      "Epoch [1/1], Step [2881/3504], Loss: 0.3494\n",
      "Epoch [1/1], Step [2882/3504], Loss: 0.4155\n",
      "Epoch [1/1], Step [2883/3504], Loss: 0.3952\n",
      "Epoch [1/1], Step [2884/3504], Loss: 0.5461\n",
      "Epoch [1/1], Step [2885/3504], Loss: 0.4915\n",
      "Epoch [1/1], Step [2886/3504], Loss: 0.3474\n",
      "Epoch [1/1], Step [2887/3504], Loss: 0.4429\n",
      "Epoch [1/1], Step [2888/3504], Loss: 0.5101\n",
      "Epoch [1/1], Step [2889/3504], Loss: 0.3308\n",
      "Epoch [1/1], Step [2890/3504], Loss: 0.3357\n",
      "Epoch [1/1], Step [2891/3504], Loss: 0.4252\n",
      "Epoch [1/1], Step [2892/3504], Loss: 0.4273\n",
      "Epoch [1/1], Step [2893/3504], Loss: 0.6011\n",
      "Epoch [1/1], Step [2894/3504], Loss: 0.3086\n",
      "Epoch [1/1], Step [2895/3504], Loss: 0.4050\n",
      "Epoch [1/1], Step [2896/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [2897/3504], Loss: 0.3715\n",
      "Epoch [1/1], Step [2898/3504], Loss: 0.3735\n",
      "Epoch [1/1], Step [2899/3504], Loss: 0.3857\n",
      "Epoch [1/1], Step [2900/3504], Loss: 0.4319\n",
      "Epoch [1/1], Step [2901/3504], Loss: 0.4788\n",
      "Epoch [1/1], Step [2902/3504], Loss: 0.3502\n",
      "Epoch [1/1], Step [2903/3504], Loss: 0.3897\n",
      "Epoch [1/1], Step [2904/3504], Loss: 0.4166\n",
      "Epoch [1/1], Step [2905/3504], Loss: 0.4793\n",
      "Epoch [1/1], Step [2906/3504], Loss: 0.4172\n",
      "Epoch [1/1], Step [2907/3504], Loss: 0.4264\n",
      "Epoch [1/1], Step [2908/3504], Loss: 0.4938\n",
      "Epoch [1/1], Step [2909/3504], Loss: 0.3697\n",
      "Epoch [1/1], Step [2910/3504], Loss: 0.4613\n",
      "Epoch [1/1], Step [2911/3504], Loss: 0.3436\n",
      "Epoch [1/1], Step [2912/3504], Loss: 0.4486\n",
      "Epoch [1/1], Step [2913/3504], Loss: 0.4494\n",
      "Epoch [1/1], Step [2914/3504], Loss: 0.4357\n",
      "Epoch [1/1], Step [2915/3504], Loss: 0.4509\n",
      "Epoch [1/1], Step [2916/3504], Loss: 0.5680\n",
      "Epoch [1/1], Step [2917/3504], Loss: 0.4164\n",
      "Epoch [1/1], Step [2918/3504], Loss: 0.3618\n",
      "Epoch [1/1], Step [2919/3504], Loss: 0.5717\n",
      "Epoch [1/1], Step [2920/3504], Loss: 0.4265\n",
      "Epoch [1/1], Step [2921/3504], Loss: 0.4720\n",
      "Epoch [1/1], Step [2922/3504], Loss: 0.3828\n",
      "Epoch [1/1], Step [2923/3504], Loss: 0.3623\n",
      "Epoch [1/1], Step [2924/3504], Loss: 0.3893\n",
      "Epoch [1/1], Step [2925/3504], Loss: 0.4521\n",
      "Epoch [1/1], Step [2926/3504], Loss: 0.3782\n",
      "Epoch [1/1], Step [2927/3504], Loss: 0.4745\n",
      "Epoch [1/1], Step [2928/3504], Loss: 0.4247\n",
      "Epoch [1/1], Step [2929/3504], Loss: 0.4709\n",
      "Epoch [1/1], Step [2930/3504], Loss: 0.3747\n",
      "Epoch [1/1], Step [2931/3504], Loss: 0.5295\n",
      "Epoch [1/1], Step [2932/3504], Loss: 0.3365\n",
      "Epoch [1/1], Step [2933/3504], Loss: 0.4961\n",
      "Epoch [1/1], Step [2934/3504], Loss: 0.3487\n",
      "Epoch [1/1], Step [2935/3504], Loss: 0.3463\n",
      "Epoch [1/1], Step [2936/3504], Loss: 0.4600\n",
      "Epoch [1/1], Step [2937/3504], Loss: 0.3390\n",
      "Epoch [1/1], Step [2938/3504], Loss: 0.3968\n",
      "Epoch [1/1], Step [2939/3504], Loss: 0.5374\n",
      "Epoch [1/1], Step [2940/3504], Loss: 0.3316\n",
      "Epoch [1/1], Step [2941/3504], Loss: 0.4244\n",
      "Epoch [1/1], Step [2942/3504], Loss: 0.3243\n",
      "Epoch [1/1], Step [2943/3504], Loss: 0.2797\n",
      "Epoch [1/1], Step [2944/3504], Loss: 0.5117\n",
      "Epoch [1/1], Step [2945/3504], Loss: 0.3822\n",
      "Epoch [1/1], Step [2946/3504], Loss: 0.5174\n",
      "Epoch [1/1], Step [2947/3504], Loss: 0.3498\n",
      "Epoch [1/1], Step [2948/3504], Loss: 0.3942\n",
      "Epoch [1/1], Step [2949/3504], Loss: 0.3968\n",
      "Epoch [1/1], Step [2950/3504], Loss: 0.4352\n",
      "Epoch [1/1], Step [2951/3504], Loss: 0.6023\n",
      "Epoch [1/1], Step [2952/3504], Loss: 0.4524\n",
      "Epoch [1/1], Step [2953/3504], Loss: 0.5258\n",
      "Epoch [1/1], Step [2954/3504], Loss: 0.6030\n",
      "Epoch [1/1], Step [2955/3504], Loss: 0.5072\n",
      "Epoch [1/1], Step [2956/3504], Loss: 0.4706\n",
      "Epoch [1/1], Step [2957/3504], Loss: 0.5486\n",
      "Epoch [1/1], Step [2958/3504], Loss: 0.4623\n",
      "Epoch [1/1], Step [2959/3504], Loss: 0.4152\n",
      "Epoch [1/1], Step [2960/3504], Loss: 0.4037\n",
      "Epoch [1/1], Step [2961/3504], Loss: 0.5452\n",
      "Epoch [1/1], Step [2962/3504], Loss: 0.4192\n",
      "Epoch [1/1], Step [2963/3504], Loss: 0.4082\n",
      "Epoch [1/1], Step [2964/3504], Loss: 0.3618\n",
      "Epoch [1/1], Step [2965/3504], Loss: 0.4220\n",
      "Epoch [1/1], Step [2966/3504], Loss: 0.3934\n",
      "Epoch [1/1], Step [2967/3504], Loss: 0.3667\n",
      "Epoch [1/1], Step [2968/3504], Loss: 0.3566\n",
      "Epoch [1/1], Step [2969/3504], Loss: 0.4783\n",
      "Epoch [1/1], Step [2970/3504], Loss: 0.5191\n",
      "Epoch [1/1], Step [2971/3504], Loss: 0.4298\n",
      "Epoch [1/1], Step [2972/3504], Loss: 0.5053\n",
      "Epoch [1/1], Step [2973/3504], Loss: 0.5182\n",
      "Epoch [1/1], Step [2974/3504], Loss: 0.4570\n",
      "Epoch [1/1], Step [2975/3504], Loss: 0.4757\n",
      "Epoch [1/1], Step [2976/3504], Loss: 0.4713\n",
      "Epoch [1/1], Step [2977/3504], Loss: 0.3310\n",
      "Epoch [1/1], Step [2978/3504], Loss: 0.4042\n",
      "Epoch [1/1], Step [2979/3504], Loss: 0.3284\n",
      "Epoch [1/1], Step [2980/3504], Loss: 0.3903\n",
      "Epoch [1/1], Step [2981/3504], Loss: 0.3188\n",
      "Epoch [1/1], Step [2982/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [2983/3504], Loss: 0.3892\n",
      "Epoch [1/1], Step [2984/3504], Loss: 0.3959\n",
      "Epoch [1/1], Step [2985/3504], Loss: 0.5053\n",
      "Epoch [1/1], Step [2986/3504], Loss: 0.3779\n",
      "Epoch [1/1], Step [2987/3504], Loss: 0.5029\n",
      "Epoch [1/1], Step [2988/3504], Loss: 0.5084\n",
      "Epoch [1/1], Step [2989/3504], Loss: 0.3763\n",
      "Epoch [1/1], Step [2990/3504], Loss: 0.5054\n",
      "Epoch [1/1], Step [2991/3504], Loss: 0.4258\n",
      "Epoch [1/1], Step [2992/3504], Loss: 0.3153\n",
      "Epoch [1/1], Step [2993/3504], Loss: 0.4310\n",
      "Epoch [1/1], Step [2994/3504], Loss: 0.3581\n",
      "Epoch [1/1], Step [2995/3504], Loss: 0.3340\n",
      "Epoch [1/1], Step [2996/3504], Loss: 0.4657\n",
      "Epoch [1/1], Step [2997/3504], Loss: 0.5313\n",
      "Epoch [1/1], Step [2998/3504], Loss: 0.5231\n",
      "Epoch [1/1], Step [2999/3504], Loss: 0.4657\n",
      "Epoch [1/1], Step [3000/3504], Loss: 0.3089\n",
      "Epoch [1/1], Step [3001/3504], Loss: 0.3685\n",
      "Epoch [1/1], Step [3002/3504], Loss: 0.3738\n",
      "Epoch [1/1], Step [3003/3504], Loss: 0.4356\n",
      "Epoch [1/1], Step [3004/3504], Loss: 0.3278\n",
      "Epoch [1/1], Step [3005/3504], Loss: 0.3502\n",
      "Epoch [1/1], Step [3006/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [3007/3504], Loss: 0.3025\n",
      "Epoch [1/1], Step [3008/3504], Loss: 0.5193\n",
      "Epoch [1/1], Step [3009/3504], Loss: 0.3598\n",
      "Epoch [1/1], Step [3010/3504], Loss: 0.5424\n",
      "Epoch [1/1], Step [3011/3504], Loss: 0.4002\n",
      "Epoch [1/1], Step [3012/3504], Loss: 0.4424\n",
      "Epoch [1/1], Step [3013/3504], Loss: 0.3877\n",
      "Epoch [1/1], Step [3014/3504], Loss: 0.3440\n",
      "Epoch [1/1], Step [3015/3504], Loss: 0.5141\n",
      "Epoch [1/1], Step [3016/3504], Loss: 0.4937\n",
      "Epoch [1/1], Step [3017/3504], Loss: 0.3858\n",
      "Epoch [1/1], Step [3018/3504], Loss: 0.3707\n",
      "Epoch [1/1], Step [3019/3504], Loss: 0.5232\n",
      "Epoch [1/1], Step [3020/3504], Loss: 0.3558\n",
      "Epoch [1/1], Step [3021/3504], Loss: 0.4739\n",
      "Epoch [1/1], Step [3022/3504], Loss: 0.4171\n",
      "Epoch [1/1], Step [3023/3504], Loss: 0.4825\n",
      "Epoch [1/1], Step [3024/3504], Loss: 0.3269\n",
      "Epoch [1/1], Step [3025/3504], Loss: 0.3702\n",
      "Epoch [1/1], Step [3026/3504], Loss: 0.4063\n",
      "Epoch [1/1], Step [3027/3504], Loss: 0.2987\n",
      "Epoch [1/1], Step [3028/3504], Loss: 0.5008\n",
      "Epoch [1/1], Step [3029/3504], Loss: 0.4126\n",
      "Epoch [1/1], Step [3030/3504], Loss: 0.4023\n",
      "Epoch [1/1], Step [3031/3504], Loss: 0.4710\n",
      "Epoch [1/1], Step [3032/3504], Loss: 0.2578\n",
      "Epoch [1/1], Step [3033/3504], Loss: 0.4257\n",
      "Epoch [1/1], Step [3034/3504], Loss: 0.3674\n",
      "Epoch [1/1], Step [3035/3504], Loss: 0.3643\n",
      "Epoch [1/1], Step [3036/3504], Loss: 0.5380\n",
      "Epoch [1/1], Step [3037/3504], Loss: 0.4449\n",
      "Epoch [1/1], Step [3038/3504], Loss: 0.4615\n",
      "Epoch [1/1], Step [3039/3504], Loss: 0.3673\n",
      "Epoch [1/1], Step [3040/3504], Loss: 0.3873\n",
      "Epoch [1/1], Step [3041/3504], Loss: 0.3829\n",
      "Epoch [1/1], Step [3042/3504], Loss: 0.4716\n",
      "Epoch [1/1], Step [3043/3504], Loss: 0.3945\n",
      "Epoch [1/1], Step [3044/3504], Loss: 0.4433\n",
      "Epoch [1/1], Step [3045/3504], Loss: 0.4146\n",
      "Epoch [1/1], Step [3046/3504], Loss: 0.4045\n",
      "Epoch [1/1], Step [3047/3504], Loss: 0.5020\n",
      "Epoch [1/1], Step [3048/3504], Loss: 0.4610\n",
      "Epoch [1/1], Step [3049/3504], Loss: 0.4393\n",
      "Epoch [1/1], Step [3050/3504], Loss: 0.4207\n",
      "Epoch [1/1], Step [3051/3504], Loss: 0.4473\n",
      "Epoch [1/1], Step [3052/3504], Loss: 0.4865\n",
      "Epoch [1/1], Step [3053/3504], Loss: 0.3395\n",
      "Epoch [1/1], Step [3054/3504], Loss: 0.2883\n",
      "Epoch [1/1], Step [3055/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [3056/3504], Loss: 0.4556\n",
      "Epoch [1/1], Step [3057/3504], Loss: 0.4665\n",
      "Epoch [1/1], Step [3058/3504], Loss: 0.3790\n",
      "Epoch [1/1], Step [3059/3504], Loss: 0.4097\n",
      "Epoch [1/1], Step [3060/3504], Loss: 0.4562\n",
      "Epoch [1/1], Step [3061/3504], Loss: 0.5012\n",
      "Epoch [1/1], Step [3062/3504], Loss: 0.3568\n",
      "Epoch [1/1], Step [3063/3504], Loss: 0.6163\n",
      "Epoch [1/1], Step [3064/3504], Loss: 0.4550\n",
      "Epoch [1/1], Step [3065/3504], Loss: 0.2697\n",
      "Epoch [1/1], Step [3066/3504], Loss: 0.4703\n",
      "Epoch [1/1], Step [3067/3504], Loss: 0.4864\n",
      "Epoch [1/1], Step [3068/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [3069/3504], Loss: 0.3327\n",
      "Epoch [1/1], Step [3070/3504], Loss: 0.4924\n",
      "Epoch [1/1], Step [3071/3504], Loss: 0.4460\n",
      "Epoch [1/1], Step [3072/3504], Loss: 0.4291\n",
      "Epoch [1/1], Step [3073/3504], Loss: 0.5396\n",
      "Epoch [1/1], Step [3074/3504], Loss: 0.5961\n",
      "Epoch [1/1], Step [3075/3504], Loss: 0.6173\n",
      "Epoch [1/1], Step [3076/3504], Loss: 0.4731\n",
      "Epoch [1/1], Step [3077/3504], Loss: 0.4336\n",
      "Epoch [1/1], Step [3078/3504], Loss: 0.4306\n",
      "Epoch [1/1], Step [3079/3504], Loss: 0.4551\n",
      "Epoch [1/1], Step [3080/3504], Loss: 0.4588\n",
      "Epoch [1/1], Step [3081/3504], Loss: 0.5321\n",
      "Epoch [1/1], Step [3082/3504], Loss: 0.6116\n",
      "Epoch [1/1], Step [3083/3504], Loss: 0.5134\n",
      "Epoch [1/1], Step [3084/3504], Loss: 0.4090\n",
      "Epoch [1/1], Step [3085/3504], Loss: 0.4886\n",
      "Epoch [1/1], Step [3086/3504], Loss: 0.5113\n",
      "Epoch [1/1], Step [3087/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [3088/3504], Loss: 0.4285\n",
      "Epoch [1/1], Step [3089/3504], Loss: 0.4613\n",
      "Epoch [1/1], Step [3090/3504], Loss: 0.3982\n",
      "Epoch [1/1], Step [3091/3504], Loss: 0.4702\n",
      "Epoch [1/1], Step [3092/3504], Loss: 0.4832\n",
      "Epoch [1/1], Step [3093/3504], Loss: 0.4908\n",
      "Epoch [1/1], Step [3094/3504], Loss: 0.4130\n",
      "Epoch [1/1], Step [3095/3504], Loss: 0.3795\n",
      "Epoch [1/1], Step [3096/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [3097/3504], Loss: 0.3463\n",
      "Epoch [1/1], Step [3098/3504], Loss: 0.5080\n",
      "Epoch [1/1], Step [3099/3504], Loss: 0.4478\n",
      "Epoch [1/1], Step [3100/3504], Loss: 0.4275\n",
      "Epoch [1/1], Step [3101/3504], Loss: 0.4679\n",
      "Epoch [1/1], Step [3102/3504], Loss: 0.4717\n",
      "Epoch [1/1], Step [3103/3504], Loss: 0.3575\n",
      "Epoch [1/1], Step [3104/3504], Loss: 0.4032\n",
      "Epoch [1/1], Step [3105/3504], Loss: 0.4465\n",
      "Epoch [1/1], Step [3106/3504], Loss: 0.4473\n",
      "Epoch [1/1], Step [3107/3504], Loss: 0.4222\n",
      "Epoch [1/1], Step [3108/3504], Loss: 0.3814\n",
      "Epoch [1/1], Step [3109/3504], Loss: 0.4600\n",
      "Epoch [1/1], Step [3110/3504], Loss: 0.5488\n",
      "Epoch [1/1], Step [3111/3504], Loss: 0.4444\n",
      "Epoch [1/1], Step [3112/3504], Loss: 0.5009\n",
      "Epoch [1/1], Step [3113/3504], Loss: 0.3030\n",
      "Epoch [1/1], Step [3114/3504], Loss: 0.3134\n",
      "Epoch [1/1], Step [3115/3504], Loss: 0.4355\n",
      "Epoch [1/1], Step [3116/3504], Loss: 0.3308\n",
      "Epoch [1/1], Step [3117/3504], Loss: 0.5202\n",
      "Epoch [1/1], Step [3118/3504], Loss: 0.4268\n",
      "Epoch [1/1], Step [3119/3504], Loss: 0.4691\n",
      "Epoch [1/1], Step [3120/3504], Loss: 0.3744\n",
      "Epoch [1/1], Step [3121/3504], Loss: 0.3858\n",
      "Epoch [1/1], Step [3122/3504], Loss: 0.5071\n",
      "Epoch [1/1], Step [3123/3504], Loss: 0.5045\n",
      "Epoch [1/1], Step [3124/3504], Loss: 0.3886\n",
      "Epoch [1/1], Step [3125/3504], Loss: 0.3711\n",
      "Epoch [1/1], Step [3126/3504], Loss: 0.4099\n",
      "Epoch [1/1], Step [3127/3504], Loss: 0.4212\n",
      "Epoch [1/1], Step [3128/3504], Loss: 0.6886\n",
      "Epoch [1/1], Step [3129/3504], Loss: 0.4060\n",
      "Epoch [1/1], Step [3130/3504], Loss: 0.4636\n",
      "Epoch [1/1], Step [3131/3504], Loss: 0.4782\n",
      "Epoch [1/1], Step [3132/3504], Loss: 0.4188\n",
      "Epoch [1/1], Step [3133/3504], Loss: 0.4702\n",
      "Epoch [1/1], Step [3134/3504], Loss: 0.5418\n",
      "Epoch [1/1], Step [3135/3504], Loss: 0.2803\n",
      "Epoch [1/1], Step [3136/3504], Loss: 0.2530\n",
      "Epoch [1/1], Step [3137/3504], Loss: 0.4420\n",
      "Epoch [1/1], Step [3138/3504], Loss: 0.4532\n",
      "Epoch [1/1], Step [3139/3504], Loss: 0.4936\n",
      "Epoch [1/1], Step [3140/3504], Loss: 0.3773\n",
      "Epoch [1/1], Step [3141/3504], Loss: 0.4234\n",
      "Epoch [1/1], Step [3142/3504], Loss: 0.4172\n",
      "Epoch [1/1], Step [3143/3504], Loss: 0.5343\n",
      "Epoch [1/1], Step [3144/3504], Loss: 0.4079\n",
      "Epoch [1/1], Step [3145/3504], Loss: 0.3520\n",
      "Epoch [1/1], Step [3146/3504], Loss: 0.4082\n",
      "Epoch [1/1], Step [3147/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [3148/3504], Loss: 0.3845\n",
      "Epoch [1/1], Step [3149/3504], Loss: 0.4957\n",
      "Epoch [1/1], Step [3150/3504], Loss: 0.4865\n",
      "Epoch [1/1], Step [3151/3504], Loss: 0.4265\n",
      "Epoch [1/1], Step [3152/3504], Loss: 0.3270\n",
      "Epoch [1/1], Step [3153/3504], Loss: 0.4875\n",
      "Epoch [1/1], Step [3154/3504], Loss: 0.4406\n",
      "Epoch [1/1], Step [3155/3504], Loss: 0.4725\n",
      "Epoch [1/1], Step [3156/3504], Loss: 0.4765\n",
      "Epoch [1/1], Step [3157/3504], Loss: 0.4271\n",
      "Epoch [1/1], Step [3158/3504], Loss: 0.4139\n",
      "Epoch [1/1], Step [3159/3504], Loss: 0.3783\n",
      "Epoch [1/1], Step [3160/3504], Loss: 0.3286\n",
      "Epoch [1/1], Step [3161/3504], Loss: 0.4412\n",
      "Epoch [1/1], Step [3162/3504], Loss: 0.4279\n",
      "Epoch [1/1], Step [3163/3504], Loss: 0.2832\n",
      "Epoch [1/1], Step [3164/3504], Loss: 0.3805\n",
      "Epoch [1/1], Step [3165/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [3166/3504], Loss: 0.4240\n",
      "Epoch [1/1], Step [3167/3504], Loss: 0.4631\n",
      "Epoch [1/1], Step [3168/3504], Loss: 0.3928\n",
      "Epoch [1/1], Step [3169/3504], Loss: 0.4227\n",
      "Epoch [1/1], Step [3170/3504], Loss: 0.3444\n",
      "Epoch [1/1], Step [3171/3504], Loss: 0.3942\n",
      "Epoch [1/1], Step [3172/3504], Loss: 0.5464\n",
      "Epoch [1/1], Step [3173/3504], Loss: 0.4222\n",
      "Epoch [1/1], Step [3174/3504], Loss: 0.3576\n",
      "Epoch [1/1], Step [3175/3504], Loss: 0.4138\n",
      "Epoch [1/1], Step [3176/3504], Loss: 0.4066\n",
      "Epoch [1/1], Step [3177/3504], Loss: 0.3999\n",
      "Epoch [1/1], Step [3178/3504], Loss: 0.5161\n",
      "Epoch [1/1], Step [3179/3504], Loss: 0.3854\n",
      "Epoch [1/1], Step [3180/3504], Loss: 0.3636\n",
      "Epoch [1/1], Step [3181/3504], Loss: 0.6011\n",
      "Epoch [1/1], Step [3182/3504], Loss: 0.3174\n",
      "Epoch [1/1], Step [3183/3504], Loss: 0.3064\n",
      "Epoch [1/1], Step [3184/3504], Loss: 0.4175\n",
      "Epoch [1/1], Step [3185/3504], Loss: 0.5082\n",
      "Epoch [1/1], Step [3186/3504], Loss: 0.5496\n",
      "Epoch [1/1], Step [3187/3504], Loss: 0.6206\n",
      "Epoch [1/1], Step [3188/3504], Loss: 0.4111\n",
      "Epoch [1/1], Step [3189/3504], Loss: 0.3377\n",
      "Epoch [1/1], Step [3190/3504], Loss: 0.5156\n",
      "Epoch [1/1], Step [3191/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [3192/3504], Loss: 0.3060\n",
      "Epoch [1/1], Step [3193/3504], Loss: 0.5025\n",
      "Epoch [1/1], Step [3194/3504], Loss: 0.4644\n",
      "Epoch [1/1], Step [3195/3504], Loss: 0.4460\n",
      "Epoch [1/1], Step [3196/3504], Loss: 0.3645\n",
      "Epoch [1/1], Step [3197/3504], Loss: 0.4353\n",
      "Epoch [1/1], Step [3198/3504], Loss: 0.6346\n",
      "Epoch [1/1], Step [3199/3504], Loss: 0.4555\n",
      "Epoch [1/1], Step [3200/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [3201/3504], Loss: 0.3736\n",
      "Epoch [1/1], Step [3202/3504], Loss: 0.5319\n",
      "Epoch [1/1], Step [3203/3504], Loss: 0.3892\n",
      "Epoch [1/1], Step [3204/3504], Loss: 0.4892\n",
      "Epoch [1/1], Step [3205/3504], Loss: 0.3441\n",
      "Epoch [1/1], Step [3206/3504], Loss: 0.4087\n",
      "Epoch [1/1], Step [3207/3504], Loss: 0.3613\n",
      "Epoch [1/1], Step [3208/3504], Loss: 0.4969\n",
      "Epoch [1/1], Step [3209/3504], Loss: 0.3697\n",
      "Epoch [1/1], Step [3210/3504], Loss: 0.4386\n",
      "Epoch [1/1], Step [3211/3504], Loss: 0.3919\n",
      "Epoch [1/1], Step [3212/3504], Loss: 0.3839\n",
      "Epoch [1/1], Step [3213/3504], Loss: 0.4029\n",
      "Epoch [1/1], Step [3214/3504], Loss: 0.3520\n",
      "Epoch [1/1], Step [3215/3504], Loss: 0.3509\n",
      "Epoch [1/1], Step [3216/3504], Loss: 0.3772\n",
      "Epoch [1/1], Step [3217/3504], Loss: 0.4331\n",
      "Epoch [1/1], Step [3218/3504], Loss: 0.3586\n",
      "Epoch [1/1], Step [3219/3504], Loss: 0.4631\n",
      "Epoch [1/1], Step [3220/3504], Loss: 0.2882\n",
      "Epoch [1/1], Step [3221/3504], Loss: 0.5565\n",
      "Epoch [1/1], Step [3222/3504], Loss: 0.4350\n",
      "Epoch [1/1], Step [3223/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [3224/3504], Loss: 0.6348\n",
      "Epoch [1/1], Step [3225/3504], Loss: 0.4013\n",
      "Epoch [1/1], Step [3226/3504], Loss: 0.5433\n",
      "Epoch [1/1], Step [3227/3504], Loss: 0.4347\n",
      "Epoch [1/1], Step [3228/3504], Loss: 0.3681\n",
      "Epoch [1/1], Step [3229/3504], Loss: 0.3617\n",
      "Epoch [1/1], Step [3230/3504], Loss: 0.3529\n",
      "Epoch [1/1], Step [3231/3504], Loss: 0.4413\n",
      "Epoch [1/1], Step [3232/3504], Loss: 0.4314\n",
      "Epoch [1/1], Step [3233/3504], Loss: 0.3877\n",
      "Epoch [1/1], Step [3234/3504], Loss: 0.4203\n",
      "Epoch [1/1], Step [3235/3504], Loss: 0.4695\n",
      "Epoch [1/1], Step [3236/3504], Loss: 0.4854\n",
      "Epoch [1/1], Step [3237/3504], Loss: 0.4580\n",
      "Epoch [1/1], Step [3238/3504], Loss: 0.3701\n",
      "Epoch [1/1], Step [3239/3504], Loss: 0.3339\n",
      "Epoch [1/1], Step [3240/3504], Loss: 0.3674\n",
      "Epoch [1/1], Step [3241/3504], Loss: 0.4560\n",
      "Epoch [1/1], Step [3242/3504], Loss: 0.4067\n",
      "Epoch [1/1], Step [3243/3504], Loss: 0.3818\n",
      "Epoch [1/1], Step [3244/3504], Loss: 0.3155\n",
      "Epoch [1/1], Step [3245/3504], Loss: 0.3927\n",
      "Epoch [1/1], Step [3246/3504], Loss: 0.3899\n",
      "Epoch [1/1], Step [3247/3504], Loss: 0.3552\n",
      "Epoch [1/1], Step [3248/3504], Loss: 0.4093\n",
      "Epoch [1/1], Step [3249/3504], Loss: 0.3306\n",
      "Epoch [1/1], Step [3250/3504], Loss: 0.4321\n",
      "Epoch [1/1], Step [3251/3504], Loss: 0.5121\n",
      "Epoch [1/1], Step [3252/3504], Loss: 0.4709\n",
      "Epoch [1/1], Step [3253/3504], Loss: 0.4081\n",
      "Epoch [1/1], Step [3254/3504], Loss: 0.4316\n",
      "Epoch [1/1], Step [3255/3504], Loss: 0.4939\n",
      "Epoch [1/1], Step [3256/3504], Loss: 0.3419\n",
      "Epoch [1/1], Step [3257/3504], Loss: 0.5077\n",
      "Epoch [1/1], Step [3258/3504], Loss: 0.4238\n",
      "Epoch [1/1], Step [3259/3504], Loss: 0.3553\n",
      "Epoch [1/1], Step [3260/3504], Loss: 0.3664\n",
      "Epoch [1/1], Step [3261/3504], Loss: 0.4085\n",
      "Epoch [1/1], Step [3262/3504], Loss: 0.4312\n",
      "Epoch [1/1], Step [3263/3504], Loss: 0.4658\n",
      "Epoch [1/1], Step [3264/3504], Loss: 0.4458\n",
      "Epoch [1/1], Step [3265/3504], Loss: 0.2947\n",
      "Epoch [1/1], Step [3266/3504], Loss: 0.3904\n",
      "Epoch [1/1], Step [3267/3504], Loss: 0.5340\n",
      "Epoch [1/1], Step [3268/3504], Loss: 0.4123\n",
      "Epoch [1/1], Step [3269/3504], Loss: 0.5387\n",
      "Epoch [1/1], Step [3270/3504], Loss: 0.3857\n",
      "Epoch [1/1], Step [3271/3504], Loss: 0.4878\n",
      "Epoch [1/1], Step [3272/3504], Loss: 0.6077\n",
      "Epoch [1/1], Step [3273/3504], Loss: 0.3918\n",
      "Epoch [1/1], Step [3274/3504], Loss: 0.4524\n",
      "Epoch [1/1], Step [3275/3504], Loss: 0.3038\n",
      "Epoch [1/1], Step [3276/3504], Loss: 0.4698\n",
      "Epoch [1/1], Step [3277/3504], Loss: 0.3686\n",
      "Epoch [1/1], Step [3278/3504], Loss: 0.4025\n",
      "Epoch [1/1], Step [3279/3504], Loss: 0.3259\n",
      "Epoch [1/1], Step [3280/3504], Loss: 0.5308\n",
      "Epoch [1/1], Step [3281/3504], Loss: 0.6396\n",
      "Epoch [1/1], Step [3282/3504], Loss: 0.3942\n",
      "Epoch [1/1], Step [3283/3504], Loss: 0.3816\n",
      "Epoch [1/1], Step [3284/3504], Loss: 0.4951\n",
      "Epoch [1/1], Step [3285/3504], Loss: 0.3986\n",
      "Epoch [1/1], Step [3286/3504], Loss: 0.5237\n",
      "Epoch [1/1], Step [3287/3504], Loss: 0.4197\n",
      "Epoch [1/1], Step [3288/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [3289/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [3290/3504], Loss: 0.3946\n",
      "Epoch [1/1], Step [3291/3504], Loss: 0.4964\n",
      "Epoch [1/1], Step [3292/3504], Loss: 0.3351\n",
      "Epoch [1/1], Step [3293/3504], Loss: 0.4168\n",
      "Epoch [1/1], Step [3294/3504], Loss: 0.4647\n",
      "Epoch [1/1], Step [3295/3504], Loss: 0.4685\n",
      "Epoch [1/1], Step [3296/3504], Loss: 0.4599\n",
      "Epoch [1/1], Step [3297/3504], Loss: 0.3292\n",
      "Epoch [1/1], Step [3298/3504], Loss: 0.3587\n",
      "Epoch [1/1], Step [3299/3504], Loss: 0.5411\n",
      "Epoch [1/1], Step [3300/3504], Loss: 0.4411\n",
      "Epoch [1/1], Step [3301/3504], Loss: 0.4562\n",
      "Epoch [1/1], Step [3302/3504], Loss: 0.4034\n",
      "Epoch [1/1], Step [3303/3504], Loss: 0.4076\n",
      "Epoch [1/1], Step [3304/3504], Loss: 0.4554\n",
      "Epoch [1/1], Step [3305/3504], Loss: 0.3643\n",
      "Epoch [1/1], Step [3306/3504], Loss: 0.5013\n",
      "Epoch [1/1], Step [3307/3504], Loss: 0.5469\n",
      "Epoch [1/1], Step [3308/3504], Loss: 0.3643\n",
      "Epoch [1/1], Step [3309/3504], Loss: 0.3907\n",
      "Epoch [1/1], Step [3310/3504], Loss: 0.4205\n",
      "Epoch [1/1], Step [3311/3504], Loss: 0.4247\n",
      "Epoch [1/1], Step [3312/3504], Loss: 0.4710\n",
      "Epoch [1/1], Step [3313/3504], Loss: 0.5270\n",
      "Epoch [1/1], Step [3314/3504], Loss: 0.6052\n",
      "Epoch [1/1], Step [3315/3504], Loss: 0.5119\n",
      "Epoch [1/1], Step [3316/3504], Loss: 0.4554\n",
      "Epoch [1/1], Step [3317/3504], Loss: 0.5163\n",
      "Epoch [1/1], Step [3318/3504], Loss: 0.4813\n",
      "Epoch [1/1], Step [3319/3504], Loss: 0.3851\n",
      "Epoch [1/1], Step [3320/3504], Loss: 0.4828\n",
      "Epoch [1/1], Step [3321/3504], Loss: 0.5477\n",
      "Epoch [1/1], Step [3322/3504], Loss: 0.3498\n",
      "Epoch [1/1], Step [3323/3504], Loss: 0.5480\n",
      "Epoch [1/1], Step [3324/3504], Loss: 0.4389\n",
      "Epoch [1/1], Step [3325/3504], Loss: 0.5458\n",
      "Epoch [1/1], Step [3326/3504], Loss: 0.4085\n",
      "Epoch [1/1], Step [3327/3504], Loss: 0.4518\n",
      "Epoch [1/1], Step [3328/3504], Loss: 0.4009\n",
      "Epoch [1/1], Step [3329/3504], Loss: 0.4765\n",
      "Epoch [1/1], Step [3330/3504], Loss: 0.3174\n",
      "Epoch [1/1], Step [3331/3504], Loss: 0.4717\n",
      "Epoch [1/1], Step [3332/3504], Loss: 0.4557\n",
      "Epoch [1/1], Step [3333/3504], Loss: 0.2988\n",
      "Epoch [1/1], Step [3334/3504], Loss: 0.4069\n",
      "Epoch [1/1], Step [3335/3504], Loss: 0.4251\n",
      "Epoch [1/1], Step [3336/3504], Loss: 0.3135\n",
      "Epoch [1/1], Step [3337/3504], Loss: 0.4598\n",
      "Epoch [1/1], Step [3338/3504], Loss: 0.2898\n",
      "Epoch [1/1], Step [3339/3504], Loss: 0.4637\n",
      "Epoch [1/1], Step [3340/3504], Loss: 0.4016\n",
      "Epoch [1/1], Step [3341/3504], Loss: 0.4053\n",
      "Epoch [1/1], Step [3342/3504], Loss: 0.3250\n",
      "Epoch [1/1], Step [3343/3504], Loss: 0.3837\n",
      "Epoch [1/1], Step [3344/3504], Loss: 0.4705\n",
      "Epoch [1/1], Step [3345/3504], Loss: 0.4216\n",
      "Epoch [1/1], Step [3346/3504], Loss: 0.4901\n",
      "Epoch [1/1], Step [3347/3504], Loss: 0.3582\n",
      "Epoch [1/1], Step [3348/3504], Loss: 0.3147\n",
      "Epoch [1/1], Step [3349/3504], Loss: 0.4117\n",
      "Epoch [1/1], Step [3350/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [3351/3504], Loss: 0.4404\n",
      "Epoch [1/1], Step [3352/3504], Loss: 0.3907\n",
      "Epoch [1/1], Step [3353/3504], Loss: 0.5086\n",
      "Epoch [1/1], Step [3354/3504], Loss: 0.4401\n",
      "Epoch [1/1], Step [3355/3504], Loss: 0.3682\n",
      "Epoch [1/1], Step [3356/3504], Loss: 0.3400\n",
      "Epoch [1/1], Step [3357/3504], Loss: 0.3856\n",
      "Epoch [1/1], Step [3358/3504], Loss: 0.4959\n",
      "Epoch [1/1], Step [3359/3504], Loss: 0.5595\n",
      "Epoch [1/1], Step [3360/3504], Loss: 0.4839\n",
      "Epoch [1/1], Step [3361/3504], Loss: 0.4865\n",
      "Epoch [1/1], Step [3362/3504], Loss: 0.3515\n",
      "Epoch [1/1], Step [3363/3504], Loss: 0.6384\n",
      "Epoch [1/1], Step [3364/3504], Loss: 0.4331\n",
      "Epoch [1/1], Step [3365/3504], Loss: 0.3907\n",
      "Epoch [1/1], Step [3366/3504], Loss: 0.5365\n",
      "Epoch [1/1], Step [3367/3504], Loss: 0.3580\n",
      "Epoch [1/1], Step [3368/3504], Loss: 0.4008\n",
      "Epoch [1/1], Step [3369/3504], Loss: 0.3344\n",
      "Epoch [1/1], Step [3370/3504], Loss: 0.5362\n",
      "Epoch [1/1], Step [3371/3504], Loss: 0.4267\n",
      "Epoch [1/1], Step [3372/3504], Loss: 0.5033\n",
      "Epoch [1/1], Step [3373/3504], Loss: 0.3925\n",
      "Epoch [1/1], Step [3374/3504], Loss: 0.4390\n",
      "Epoch [1/1], Step [3375/3504], Loss: 0.4497\n",
      "Epoch [1/1], Step [3376/3504], Loss: 0.5001\n",
      "Epoch [1/1], Step [3377/3504], Loss: 0.4288\n",
      "Epoch [1/1], Step [3378/3504], Loss: 0.3575\n",
      "Epoch [1/1], Step [3379/3504], Loss: 0.3303\n",
      "Epoch [1/1], Step [3380/3504], Loss: 0.4225\n",
      "Epoch [1/1], Step [3381/3504], Loss: 0.3707\n",
      "Epoch [1/1], Step [3382/3504], Loss: 0.3836\n",
      "Epoch [1/1], Step [3383/3504], Loss: 0.3303\n",
      "Epoch [1/1], Step [3384/3504], Loss: 0.3461\n",
      "Epoch [1/1], Step [3385/3504], Loss: 0.5205\n",
      "Epoch [1/1], Step [3386/3504], Loss: 0.5519\n",
      "Epoch [1/1], Step [3387/3504], Loss: 0.4321\n",
      "Epoch [1/1], Step [3388/3504], Loss: 0.4090\n",
      "Epoch [1/1], Step [3389/3504], Loss: 0.5672\n",
      "Epoch [1/1], Step [3390/3504], Loss: 0.4568\n",
      "Epoch [1/1], Step [3391/3504], Loss: 0.5176\n",
      "Epoch [1/1], Step [3392/3504], Loss: 0.3949\n",
      "Epoch [1/1], Step [3393/3504], Loss: 0.3135\n",
      "Epoch [1/1], Step [3394/3504], Loss: 0.4104\n",
      "Epoch [1/1], Step [3395/3504], Loss: 0.4144\n",
      "Epoch [1/1], Step [3396/3504], Loss: 0.4229\n",
      "Epoch [1/1], Step [3397/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [3398/3504], Loss: 0.3821\n",
      "Epoch [1/1], Step [3399/3504], Loss: 0.4467\n",
      "Epoch [1/1], Step [3400/3504], Loss: 0.4190\n",
      "Epoch [1/1], Step [3401/3504], Loss: 0.4877\n",
      "Epoch [1/1], Step [3402/3504], Loss: 0.4246\n",
      "Epoch [1/1], Step [3403/3504], Loss: 0.5067\n",
      "Epoch [1/1], Step [3404/3504], Loss: 0.5346\n",
      "Epoch [1/1], Step [3405/3504], Loss: 0.4793\n",
      "Epoch [1/1], Step [3406/3504], Loss: 0.4349\n",
      "Epoch [1/1], Step [3407/3504], Loss: 0.5157\n",
      "Epoch [1/1], Step [3408/3504], Loss: 0.3913\n",
      "Epoch [1/1], Step [3409/3504], Loss: 0.4116\n",
      "Epoch [1/1], Step [3410/3504], Loss: 0.4796\n",
      "Epoch [1/1], Step [3411/3504], Loss: 0.4408\n",
      "Epoch [1/1], Step [3412/3504], Loss: 0.5872\n",
      "Epoch [1/1], Step [3413/3504], Loss: 0.5671\n",
      "Epoch [1/1], Step [3414/3504], Loss: 0.3964\n",
      "Epoch [1/1], Step [3415/3504], Loss: 0.4254\n",
      "Epoch [1/1], Step [3416/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [3417/3504], Loss: 0.3668\n",
      "Epoch [1/1], Step [3418/3504], Loss: 0.3905\n",
      "Epoch [1/1], Step [3419/3504], Loss: 0.4563\n",
      "Epoch [1/1], Step [3420/3504], Loss: 0.2944\n",
      "Epoch [1/1], Step [3421/3504], Loss: 0.4775\n",
      "Epoch [1/1], Step [3422/3504], Loss: 0.3604\n",
      "Epoch [1/1], Step [3423/3504], Loss: 0.4040\n",
      "Epoch [1/1], Step [3424/3504], Loss: 0.3244\n",
      "Epoch [1/1], Step [3425/3504], Loss: 0.3425\n",
      "Epoch [1/1], Step [3426/3504], Loss: 0.3813\n",
      "Epoch [1/1], Step [3427/3504], Loss: 0.4684\n",
      "Epoch [1/1], Step [3428/3504], Loss: 0.3459\n",
      "Epoch [1/1], Step [3429/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [3430/3504], Loss: 0.4743\n",
      "Epoch [1/1], Step [3431/3504], Loss: 0.6507\n",
      "Epoch [1/1], Step [3432/3504], Loss: 0.4022\n",
      "Epoch [1/1], Step [3433/3504], Loss: 0.5055\n",
      "Epoch [1/1], Step [3434/3504], Loss: 0.3669\n",
      "Epoch [1/1], Step [3435/3504], Loss: 0.4366\n",
      "Epoch [1/1], Step [3436/3504], Loss: 0.4072\n",
      "Epoch [1/1], Step [3437/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [3438/3504], Loss: 0.4969\n",
      "Epoch [1/1], Step [3439/3504], Loss: 0.5267\n",
      "Epoch [1/1], Step [3440/3504], Loss: 0.4035\n",
      "Epoch [1/1], Step [3441/3504], Loss: 0.3880\n",
      "Epoch [1/1], Step [3442/3504], Loss: 0.4201\n",
      "Epoch [1/1], Step [3443/3504], Loss: 0.4405\n",
      "Epoch [1/1], Step [3444/3504], Loss: 0.3791\n",
      "Epoch [1/1], Step [3445/3504], Loss: 0.3337\n",
      "Epoch [1/1], Step [3446/3504], Loss: 0.5049\n",
      "Epoch [1/1], Step [3447/3504], Loss: 0.4103\n",
      "Epoch [1/1], Step [3448/3504], Loss: 0.4067\n",
      "Epoch [1/1], Step [3449/3504], Loss: 0.3435\n",
      "Epoch [1/1], Step [3450/3504], Loss: 0.5423\n",
      "Epoch [1/1], Step [3451/3504], Loss: 0.3257\n",
      "Epoch [1/1], Step [3452/3504], Loss: 0.4617\n",
      "Epoch [1/1], Step [3453/3504], Loss: 0.4116\n",
      "Epoch [1/1], Step [3454/3504], Loss: 0.3487\n",
      "Epoch [1/1], Step [3455/3504], Loss: 0.4021\n",
      "Epoch [1/1], Step [3456/3504], Loss: 0.4475\n",
      "Epoch [1/1], Step [3457/3504], Loss: 0.5066\n",
      "Epoch [1/1], Step [3458/3504], Loss: 0.2999\n",
      "Epoch [1/1], Step [3459/3504], Loss: 0.3971\n",
      "Epoch [1/1], Step [3460/3504], Loss: 0.3194\n",
      "Epoch [1/1], Step [3461/3504], Loss: 0.6180\n",
      "Epoch [1/1], Step [3462/3504], Loss: 0.3258\n",
      "Epoch [1/1], Step [3463/3504], Loss: 0.3515\n",
      "Epoch [1/1], Step [3464/3504], Loss: 0.5322\n",
      "Epoch [1/1], Step [3465/3504], Loss: 0.4684\n",
      "Epoch [1/1], Step [3466/3504], Loss: 0.3697\n",
      "Epoch [1/1], Step [3467/3504], Loss: 0.4310\n",
      "Epoch [1/1], Step [3468/3504], Loss: 0.4905\n",
      "Epoch [1/1], Step [3469/3504], Loss: 0.4278\n",
      "Epoch [1/1], Step [3470/3504], Loss: 0.4123\n",
      "Epoch [1/1], Step [3471/3504], Loss: 0.4410\n",
      "Epoch [1/1], Step [3472/3504], Loss: 0.4746\n",
      "Epoch [1/1], Step [3473/3504], Loss: 0.3493\n",
      "Epoch [1/1], Step [3474/3504], Loss: 0.4306\n",
      "Epoch [1/1], Step [3475/3504], Loss: 0.3969\n",
      "Epoch [1/1], Step [3476/3504], Loss: 0.5357\n",
      "Epoch [1/1], Step [3477/3504], Loss: 0.4879\n",
      "Epoch [1/1], Step [3478/3504], Loss: 0.4115\n",
      "Epoch [1/1], Step [3479/3504], Loss: 0.3777\n",
      "Epoch [1/1], Step [3480/3504], Loss: 0.4503\n",
      "Epoch [1/1], Step [3481/3504], Loss: 0.4779\n",
      "Epoch [1/1], Step [3482/3504], Loss: 0.5038\n",
      "Epoch [1/1], Step [3483/3504], Loss: 0.4852\n",
      "Epoch [1/1], Step [3484/3504], Loss: 0.4287\n",
      "Epoch [1/1], Step [3485/3504], Loss: 0.4244\n",
      "Epoch [1/1], Step [3486/3504], Loss: 0.5163\n",
      "Epoch [1/1], Step [3487/3504], Loss: 0.3973\n",
      "Epoch [1/1], Step [3488/3504], Loss: 0.5010\n",
      "Epoch [1/1], Step [3489/3504], Loss: 0.3640\n",
      "Epoch [1/1], Step [3490/3504], Loss: 0.4271\n",
      "Epoch [1/1], Step [3491/3504], Loss: 0.4347\n",
      "Epoch [1/1], Step [3492/3504], Loss: 0.4393\n",
      "Epoch [1/1], Step [3493/3504], Loss: 0.3719\n",
      "Epoch [1/1], Step [3494/3504], Loss: 0.4138\n",
      "Epoch [1/1], Step [3495/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [3496/3504], Loss: 0.3959\n",
      "Epoch [1/1], Step [3497/3504], Loss: 0.4744\n",
      "Epoch [1/1], Step [3498/3504], Loss: 0.5466\n",
      "Epoch [1/1], Step [3499/3504], Loss: 0.3848\n",
      "Epoch [1/1], Step [3500/3504], Loss: 0.4003\n",
      "Epoch [1/1], Step [3501/3504], Loss: 0.4220\n",
      "Epoch [1/1], Step [3502/3504], Loss: 0.4135\n",
      "Epoch [1/1], Step [3503/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [3504/3504], Loss: 1.2289\n",
      "Epoch [1/1], Train Accuracy: 0.2979\n",
      "Epoch [1/1], Test Accuracy: 0.2950\n"
     ]
    }
   ],
   "source": [
    "train_accs2, test_accs2 = trainer(paper_model, train_dataloader, test_dataloader, 1, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d250f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
