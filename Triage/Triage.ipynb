{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12533e4d",
   "metadata": {},
   "source": [
    "# Triage Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10cb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32a5cb",
   "metadata": {},
   "source": [
    "## Get and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54905a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Hospital Triage and Patient History.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310dad2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 560486 entries, 0 to 560485\n",
      "Data columns (total 972 columns):\n",
      " #    Column                                              Dtype  \n",
      "---   ------                                              -----  \n",
      " 0    dep_name                                            object \n",
      " 1    esi                                                 float64\n",
      " 2    age                                                 float64\n",
      " 3    gender                                              object \n",
      " 4    ethnicity                                           object \n",
      " 5    race                                                object \n",
      " 6    lang                                                object \n",
      " 7    religion                                            object \n",
      " 8    maritalstatus                                       object \n",
      " 9    employstatus                                        object \n",
      " 10   insurance_status                                    object \n",
      " 11   disposition                                         object \n",
      " 12   arrivalmode                                         object \n",
      " 13   arrivalmonth                                        object \n",
      " 14   arrivalday                                          object \n",
      " 15   arrivalhour_bin                                     object \n",
      " 16   previousdispo                                       object \n",
      " 17   2ndarymalig                                         float64\n",
      " 18   abdomhernia                                         float64\n",
      " 19   abdomnlpain                                         float64\n",
      " 20   abortcompl                                          float64\n",
      " 21   acqfootdef                                          float64\n",
      " 22   acrenlfail                                          float64\n",
      " 23   acutecvd                                            float64\n",
      " 24   acutemi                                             float64\n",
      " 25   acutphanm                                           float64\n",
      " 26   adjustmentdisorders                                 float64\n",
      " 27   adltrespfl                                          float64\n",
      " 28   alcoholrelateddisorders                             float64\n",
      " 29   allergy                                             float64\n",
      " 30   amniosdx                                            float64\n",
      " 31   analrectal                                          float64\n",
      " 32   anemia                                              float64\n",
      " 33   aneurysm                                            float64\n",
      " 34   anxietydisorders                                    float64\n",
      " 35   appendicitis                                        float64\n",
      " 36   artembolism                                         float64\n",
      " 37   asppneumon                                          float64\n",
      " 38   asthma                                              float64\n",
      " 39   attentiondeficitconductdisruptivebeha               float64\n",
      " 40   backproblem                                         float64\n",
      " 41   biliarydx                                           float64\n",
      " 42   birthasphyx                                         float64\n",
      " 43   birthtrauma                                         float64\n",
      " 44   bladdercncr                                         float64\n",
      " 45   blindness                                           float64\n",
      " 46   bnignutneo                                          float64\n",
      " 47   bonectcncr                                          float64\n",
      " 48   bph                                                 float64\n",
      " 49   brainnscan                                          float64\n",
      " 50   breastcancr                                         float64\n",
      " 51   breastdx                                            float64\n",
      " 52   brnchlngca                                          float64\n",
      " 53   bronchitis                                          float64\n",
      " 54   burns                                               float64\n",
      " 55   cardiaarrst                                         float64\n",
      " 56   cardiacanom                                         float64\n",
      " 57   carditis                                            float64\n",
      " 58   cataract                                            float64\n",
      " 59   cervixcancr                                         float64\n",
      " 60   chestpain                                           float64\n",
      " 61   chfnonhp                                            float64\n",
      " 62   chrkidneydisease                                    float64\n",
      " 63   coaghemrdx                                          float64\n",
      " 64   coloncancer                                         float64\n",
      " 65   comabrndmg                                          float64\n",
      " 66   complicdevi                                         float64\n",
      " 67   complicproc                                         float64\n",
      " 68   conduction                                          float64\n",
      " 69   contraceptiv                                        float64\n",
      " 70   copd                                                float64\n",
      " 71   coronathero                                         float64\n",
      " 72   crushinjury                                         float64\n",
      " 73   cysticfibro                                         float64\n",
      " 74   deliriumdementiaamnesticothercognitiv               float64\n",
      " 75   developmentaldisorders                              float64\n",
      " 76   diabmelnoc                                          float64\n",
      " 77   diabmelwcm                                          float64\n",
      " 78   disordersusuallydiagnosedininfancych                float64\n",
      " 79   diverticulos                                        float64\n",
      " 80   dizziness                                           float64\n",
      " 81   dminpreg                                            float64\n",
      " 82   dysrhythmia                                         float64\n",
      " 83   earlylabor                                          float64\n",
      " 84   ecodesadverseeffectsofmedicalcare                   float64\n",
      " 85   ecodesadverseeffectsofmedicaldrugs                  float64\n",
      " 86   ecodescutpierce                                     float64\n",
      " 87   ecodesdrowningsubmersion                            float64\n",
      " 88   ecodesfall                                          float64\n",
      " 89   ecodesfirearm                                       float64\n",
      " 90   ecodesfireburn                                      float64\n",
      " 91   ecodesmachinery                                     float64\n",
      " 92   ecodesmotorvehicletrafficmvt                        float64\n",
      " 93   ecodesnaturalenvironment                            float64\n",
      " 94   ecodesotherspecifiedandclassifiable                 float64\n",
      " 95   ecodesotherspecifiednec                             float64\n",
      " 96   ecodespedalcyclistnotmvt                            float64\n",
      " 97   ecodesplaceofoccurrence                             float64\n",
      " 98   ecodespoisoning                                     float64\n",
      " 99   ecodesstruckbyagainst                               float64\n",
      " 100  ecodessuffocation                                   float64\n",
      " 101  ecodestransportnotmvt                               float64\n",
      " 102  ecodesunspecified                                   float64\n",
      " 103  ectopicpreg                                         float64\n",
      " 104  encephalitis                                        float64\n",
      " 105  endometrios                                         float64\n",
      " 106  epilepsycnv                                         float64\n",
      " 107  esophcancer                                         float64\n",
      " 108  esophgealdx                                         float64\n",
      " 109  exameval                                            float64\n",
      " 110  eyeinfectn                                          float64\n",
      " 111  fatigue                                             float64\n",
      " 112  femgenitca                                          float64\n",
      " 113  feminfertil                                         float64\n",
      " 114  fetaldistrs                                         float64\n",
      " 115  fluidelcdx                                          float64\n",
      " 116  fuo                                                 float64\n",
      " 117  fxarm                                               float64\n",
      " 118  fxhip                                               float64\n",
      " 119  fxleg                                               float64\n",
      " 120  fxskullfac                                          float64\n",
      " 121  gangrene                                            float64\n",
      " 122  gasduoulcer                                         float64\n",
      " 123  gastritis                                           float64\n",
      " 124  gastroent                                           float64\n",
      " 125  giconganom                                          float64\n",
      " 126  gihemorrhag                                         float64\n",
      " 127  giperitcan                                          float64\n",
      " 128  glaucoma                                            float64\n",
      " 129  goutotcrys                                          float64\n",
      " 130  guconganom                                          float64\n",
      " 131  hdnckcancr                                          float64\n",
      " 132  headachemig                                         float64\n",
      " 133  hemmorhoids                                         float64\n",
      " 134  hemorrpreg                                          float64\n",
      " 135  hepatitis                                           float64\n",
      " 136  hivinfectn                                          float64\n",
      " 137  hodgkinsds                                          float64\n",
      " 138  hrtvalvedx                                          float64\n",
      " 139  htn                                                 float64\n",
      " 140  htncomplicn                                         float64\n",
      " 141  htninpreg                                           float64\n",
      " 142  hyperlipidem                                        float64\n",
      " 143  immunitydx                                          float64\n",
      " 144  immunizscrn                                         float64\n",
      " 145  impulsecontroldisordersnec                          float64\n",
      " 146  inducabortn                                         float64\n",
      " 147  infectarth                                          float64\n",
      " 148  influenza                                           float64\n",
      " 149  infmalegen                                          float64\n",
      " 150  intestinfct                                         float64\n",
      " 151  intobstruct                                         float64\n",
      " 152  intracrninj                                         float64\n",
      " 153  jointinjury                                         float64\n",
      " 154  kidnyrnlca                                          float64\n",
      " 155  lateeffcvd                                          float64\n",
      " 156  leukemias                                           float64\n",
      " 157  liveborn                                            float64\n",
      " 158  liveribdca                                          float64\n",
      " 159  longpregncy                                         float64\n",
      " 160  lowbirthwt                                          float64\n",
      " 161  lungexternl                                         float64\n",
      " 162  lymphenlarg                                         float64\n",
      " 163  maintchemr                                          float64\n",
      " 164  malgenitca                                          float64\n",
      " 165  maligneopls                                         float64\n",
      " 166  malposition                                         float64\n",
      " 167  meningitis                                          float64\n",
      " 168  menopausldx                                         float64\n",
      " 169  menstrualdx                                         float64\n",
      " 170  miscellaneousmentalhealthdisorders                  float64\n",
      " 171  mooddisorders                                       float64\n",
      " 172  mouthdx                                             float64\n",
      " 173  ms                                                  float64\n",
      " 174  multmyeloma                                         float64\n",
      " 175  mycoses                                             float64\n",
      " 176  nauseavomit                                         float64\n",
      " 177  neoplsmunsp                                         float64\n",
      " 178  nephritis                                           float64\n",
      " 179  nervcongan                                          float64\n",
      " 180  nonepithca                                          float64\n",
      " 181  nonhodglym                                          float64\n",
      " 182  nutritdefic                                         float64\n",
      " 183  obrelatedperintrauma                                float64\n",
      " 184  opnwndextr                                          float64\n",
      " 185  opnwndhead                                          float64\n",
      " 186  osteoarthros                                        float64\n",
      " 187  osteoporosis                                        float64\n",
      " 188  otacqdefor                                          float64\n",
      " 189  otaftercare                                         float64\n",
      " 190  otbnignneo                                          float64\n",
      " 191  otbonedx                                            float64\n",
      " 192  otcirculdx                                          float64\n",
      " 193  otcomplbir                                          float64\n",
      " 194  otconganom                                          float64\n",
      " 195  otconntiss                                          float64\n",
      " 196  otdxbladdr                                          float64\n",
      " 197  otdxkidney                                          float64\n",
      " 198  otdxstomch                                          float64\n",
      " 199  otendodsor                                          float64\n",
      " 200  otfemalgen                                          float64\n",
      " 201  othbactinf                                          float64\n",
      " 202  othcnsinfx                                          float64\n",
      " 203  othematldx                                          float64\n",
      " 204  othercvd                                            float64\n",
      " 205  othereardx                                          float64\n",
      " 206  otheredcns                                          float64\n",
      " 207  othereyedx                                          float64\n",
      " 208  othergidx                                           float64\n",
      " 209  othergudx                                           float64\n",
      " 210  otherinjury                                         float64\n",
      " 211  otherpregnancyanddeliveryincludingnormal            float64\n",
      " 212  otherscreen                                         float64\n",
      " 213  othfracture                                         float64\n",
      " 214  othheartdx                                          float64\n",
      " 215  othinfectns                                         float64\n",
      " 216  othliverdx                                          float64\n",
      " 217  othlowresp                                          float64\n",
      " 218  othmalegen                                          float64\n",
      " 219  othnervdx                                           float64\n",
      " 220  othskindx                                           float64\n",
      " 221  othveindx                                           float64\n",
      " 222  otinflskin                                          float64\n",
      " 223  otitismedia                                         float64\n",
      " 224  otjointdx                                           float64\n",
      " 225  otnutritdx                                          float64\n",
      " 226  otperintdx                                          float64\n",
      " 227  otpregcomp                                          float64\n",
      " 228  otprimryca                                          float64\n",
      " 229  otrespirca                                          float64\n",
      " 230  otupprresp                                          float64\n",
      " 231  otuprspin                                           float64\n",
      " 232  ovariancyst                                         float64\n",
      " 233  ovarycancer                                         float64\n",
      " 234  pancreascan                                         float64\n",
      " 235  pancreasdx                                          float64\n",
      " 236  paralysis                                           float64\n",
      " 237  parkinsons                                          float64\n",
      " 238  pathologfx                                          float64\n",
      " 239  pelvicobstr                                         float64\n",
      " 240  perintjaund                                         float64\n",
      " 241  peripathero                                         float64\n",
      " 242  peritonitis                                         float64\n",
      " 243  personalitydisorders                                float64\n",
      " 244  phlebitis                                           float64\n",
      " 245  pid                                                 float64\n",
      " 246  pleurisy                                            float64\n",
      " 247  pneumonia                                           float64\n",
      " 248  poisnnonmed                                         float64\n",
      " 249  poisnotmed                                          float64\n",
      " 250  poisonpsych                                         float64\n",
      " 251  precereoccl                                         float64\n",
      " 252  prevcsectn                                          float64\n",
      " 253  prolapse                                            float64\n",
      " 254  prostatecan                                         float64\n",
      " 255  pulmhartdx                                          float64\n",
      " 256  rctmanusca                                          float64\n",
      " 257  rehab                                               float64\n",
      " 258  respdistres                                         float64\n",
      " 259  retinaldx                                           float64\n",
      " 260  rheumarth                                           float64\n",
      " 261  schizophreniaandotherpsychoticdisorde               float64\n",
      " 262  screeningandhistoryofmentalhealthan                 float64\n",
      " 263  septicemia                                          float64\n",
      " 264  septicemiaexceptinlabor                             float64\n",
      " 265  sexualinfxs                                         float64\n",
      " 266  shock                                               float64\n",
      " 267  sicklecell                                          float64\n",
      " 268  skininfectn                                         float64\n",
      " 269  skinmelanom                                         float64\n",
      " 270  sle                                                 float64\n",
      " 271  socialadmin                                         float64\n",
      " 272  spincorinj                                          float64\n",
      " 273  spontabortn                                         float64\n",
      " 274  sprain                                              float64\n",
      " 275  stomchcancr                                         float64\n",
      " 276  substancerelateddisorders                           float64\n",
      " 277  suicideandintentionalselfinflictedin                float64\n",
      " 278  superficinj                                         float64\n",
      " 279  syncope                                             float64\n",
      " 280  teethdx                                             float64\n",
      " 281  testiscancr                                         float64\n",
      " 282  thyroidcncr                                         float64\n",
      " 283  thyroiddsor                                         float64\n",
      " 284  tia                                                 float64\n",
      " 285  tonsillitis                                         float64\n",
      " 286  tuberculosis                                        float64\n",
      " 287  ulceratcol                                          float64\n",
      " 288  ulcerskin                                           float64\n",
      " 289  umbilcord                                           float64\n",
      " 290  unclassified                                        float64\n",
      " 291  urinstone                                           float64\n",
      " 292  urinyorgca                                          float64\n",
      " 293  uteruscancr                                         float64\n",
      " 294  uti                                                 float64\n",
      " 295  varicosevn                                          float64\n",
      " 296  viralinfect                                         float64\n",
      " 297  whtblooddx                                          float64\n",
      " 298  n_edvisits                                          int64  \n",
      " 299  n_admissions                                        int64  \n",
      " 300  absolutelymphocytecount_last                        float64\n",
      " 301  acetonebld_last                                     float64\n",
      " 302  alanineaminotransferase(alt)_last                   float64\n",
      " 303  albumin_last                                        float64\n",
      " 304  alkphos_last                                        float64\n",
      " 305  anc(absneutrophilcount)_last                        float64\n",
      " 306  aniongap_last                                       float64\n",
      " 307  aspartateaminotransferase(ast)_last                 float64\n",
      " 308  b-typenatriureticpeptide,pro(probnp)_last           float64\n",
      " 309  baseexcess(poc)_last                                float64\n",
      " 310  baseexcess,venous(poc)_last                         float64\n",
      " 311  basos_last                                          float64\n",
      " 312  basosabs_last                                       float64\n",
      " 313  benzodiazepinesscreen,urine,noconf._last            float64\n",
      " 314  bilirubindirect_last                                float64\n",
      " 315  bilirubintotal_last                                 float64\n",
      " 316  bun_last                                            float64\n",
      " 317  bun/creatratio_last                                 float64\n",
      " 318  calcium_last                                        float64\n",
      " 319  calculatedco2(poc)_last                             float64\n",
      " 320  calculatedhco3(poc)i_last                           float64\n",
      " 321  calculatedo2saturation(poc)_last                    float64\n",
      " 322  chloride_last                                       float64\n",
      " 323  cktotal_last                                        float64\n",
      " 324  co2_last                                            float64\n",
      " 325  co2calculated,venous(poc)_last                      float64\n",
      " 326  co2,poc_last                                        float64\n",
      " 327  creatinine_last                                     float64\n",
      " 328  d-dimer_last                                        float64\n",
      " 329  egfr_last                                           float64\n",
      " 330  egfr(nonafricanamerican)_last                       float64\n",
      " 331  egfr(aframer)_last                                  float64\n",
      " 332  eos_last                                            float64\n",
      " 333  eosinoabs_last                                      float64\n",
      " 334  epithelialcells_last                                float64\n",
      " 335  globulin_last                                       float64\n",
      " 336  glucose_last                                        float64\n",
      " 337  glucose,meter_last                                  float64\n",
      " 338  hco3calculated,venous(poc)_last                     float64\n",
      " 339  hematocrit_last                                     float64\n",
      " 340  hemoglobin_last                                     float64\n",
      " 341  immaturegrans(abs)_last                             float64\n",
      " 342  immaturegranulocytes_last                           float64\n",
      " 343  inr_last                                            float64\n",
      " 344  lactate,poc_last                                    float64\n",
      " 345  lipase_last                                         float64\n",
      " 346  lymphs_last                                         float64\n",
      " 347  magnesium_last                                      float64\n",
      " 348  mch_last                                            float64\n",
      " 349  mchc_last                                           float64\n",
      " 350  mcv_last                                            float64\n",
      " 351  monocytes_last                                      float64\n",
      " 352  monosabs_last                                       float64\n",
      " 353  mpv_last                                            float64\n",
      " 354  neutrophils_last                                    float64\n",
      " 355  nrbc_last                                           float64\n",
      " 356  nrbcabsolute_last                                   float64\n",
      " 357  o2satcalculated,venous(poc)_last                    float64\n",
      " 358  pco2(poc)_last                                      float64\n",
      " 359  pco2,venous(poc)_last                               float64\n",
      " 360  ph,venous(poc)_last                                 float64\n",
      " 361  phencyclidine(pcp)screen,urine,noconf._last         float64\n",
      " 362  phosphorus_last                                     float64\n",
      " 363  platelets_last                                      float64\n",
      " 364  po2(poc)_last                                       float64\n",
      " 365  po2,venous(poc)_last                                float64\n",
      " 366  pocbun_last                                         float64\n",
      " 367  poccreatinine_last                                  float64\n",
      " 368  pocglucose_last                                     float64\n",
      " 369  pochematocrit_last                                  float64\n",
      " 370  pocionizedcalcium_last                              float64\n",
      " 371  pocph_last                                          float64\n",
      " 372  pocpotassium_last                                   float64\n",
      " 373  pocsodium_last                                      float64\n",
      " 374  poctroponini._last                                  float64\n",
      " 375  potassium_last                                      float64\n",
      " 376  proteintotal_last                                   float64\n",
      " 377  prothrombintime_last                                float64\n",
      " 378  ptt_last                                            float64\n",
      " 379  rbc_last                                            float64\n",
      " 380  rbc/hpf_last                                        float64\n",
      " 381  rdw_last                                            float64\n",
      " 382  sodium_last                                         float64\n",
      " 383  troponini(poc)_last                                 float64\n",
      " 384  troponint_last                                      float64\n",
      " 385  tsh_last                                            float64\n",
      " 386  wbc_last                                            float64\n",
      " 387  wbc/hpf_last                                        float64\n",
      " 388  absolutelymphocytecount_min                         float64\n",
      " 389  acetonebld_min                                      float64\n",
      " 390  alanineaminotransferase(alt)_min                    float64\n",
      " 391  albumin_min                                         float64\n",
      " 392  alkphos_min                                         float64\n",
      " 393  anc(absneutrophilcount)_min                         float64\n",
      " 394  aniongap_min                                        float64\n",
      " 395  aspartateaminotransferase(ast)_min                  float64\n",
      " 396  b-typenatriureticpeptide,pro(probnp)_min            float64\n",
      " 397  baseexcess(poc)_min                                 float64\n",
      " 398  baseexcess,venous(poc)_min                          float64\n",
      " 399  basos_min                                           float64\n",
      " 400  basosabs_min                                        float64\n",
      " 401  benzodiazepinesscreen,urine,noconf._min             float64\n",
      " 402  bilirubindirect_min                                 float64\n",
      " 403  bilirubintotal_min                                  float64\n",
      " 404  bun_min                                             float64\n",
      " 405  bun/creatratio_min                                  float64\n",
      " 406  calcium_min                                         float64\n",
      " 407  calculatedco2(poc)_min                              float64\n",
      " 408  calculatedhco3(poc)i_min                            float64\n",
      " 409  calculatedo2saturation(poc)_min                     float64\n",
      " 410  chloride_min                                        float64\n",
      " 411  cktotal_min                                         float64\n",
      " 412  co2_min                                             float64\n",
      " 413  co2calculated,venous(poc)_min                       float64\n",
      " 414  co2,poc_min                                         float64\n",
      " 415  creatinine_min                                      float64\n",
      " 416  d-dimer_min                                         float64\n",
      " 417  egfr_min                                            float64\n",
      " 418  egfr(nonafricanamerican)_min                        float64\n",
      " 419  egfr(aframer)_min                                   float64\n",
      " 420  eos_min                                             float64\n",
      " 421  eosinoabs_min                                       float64\n",
      " 422  epithelialcells_min                                 float64\n",
      " 423  globulin_min                                        float64\n",
      " 424  glucose_min                                         float64\n",
      " 425  glucose,meter_min                                   float64\n",
      " 426  hco3calculated,venous(poc)_min                      float64\n",
      " 427  hematocrit_min                                      float64\n",
      " 428  hemoglobin_min                                      float64\n",
      " 429  immaturegrans(abs)_min                              float64\n",
      " 430  immaturegranulocytes_min                            float64\n",
      " 431  inr_min                                             float64\n",
      " 432  lactate,poc_min                                     float64\n",
      " 433  lipase_min                                          float64\n",
      " 434  lymphs_min                                          float64\n",
      " 435  magnesium_min                                       float64\n",
      " 436  mch_min                                             float64\n",
      " 437  mchc_min                                            float64\n",
      " 438  mcv_min                                             float64\n",
      " 439  monocytes_min                                       float64\n",
      " 440  monosabs_min                                        float64\n",
      " 441  mpv_min                                             float64\n",
      " 442  neutrophils_min                                     float64\n",
      " 443  nrbc_min                                            float64\n",
      " 444  nrbcabsolute_min                                    float64\n",
      " 445  o2satcalculated,venous(poc)_min                     float64\n",
      " 446  pco2(poc)_min                                       float64\n",
      " 447  pco2,venous(poc)_min                                float64\n",
      " 448  ph,venous(poc)_min                                  float64\n",
      " 449  phencyclidine(pcp)screen,urine,noconf._min          float64\n",
      " 450  phosphorus_min                                      float64\n",
      " 451  platelets_min                                       float64\n",
      " 452  po2(poc)_min                                        float64\n",
      " 453  po2,venous(poc)_min                                 float64\n",
      " 454  pocbun_min                                          float64\n",
      " 455  poccreatinine_min                                   float64\n",
      " 456  pocglucose_min                                      float64\n",
      " 457  pochematocrit_min                                   float64\n",
      " 458  pocionizedcalcium_min                               float64\n",
      " 459  pocph_min                                           float64\n",
      " 460  pocpotassium_min                                    float64\n",
      " 461  pocsodium_min                                       float64\n",
      " 462  poctroponini._min                                   float64\n",
      " 463  potassium_min                                       float64\n",
      " 464  proteintotal_min                                    float64\n",
      " 465  prothrombintime_min                                 float64\n",
      " 466  ptt_min                                             float64\n",
      " 467  rbc_min                                             float64\n",
      " 468  rbc/hpf_min                                         float64\n",
      " 469  rdw_min                                             float64\n",
      " 470  sodium_min                                          float64\n",
      " 471  troponini(poc)_min                                  float64\n",
      " 472  troponint_min                                       float64\n",
      " 473  tsh_min                                             float64\n",
      " 474  wbc_min                                             float64\n",
      " 475  wbc/hpf_min                                         float64\n",
      " 476  absolutelymphocytecount_max                         float64\n",
      " 477  acetonebld_max                                      float64\n",
      " 478  alanineaminotransferase(alt)_max                    float64\n",
      " 479  albumin_max                                         float64\n",
      " 480  alkphos_max                                         float64\n",
      " 481  anc(absneutrophilcount)_max                         float64\n",
      " 482  aniongap_max                                        float64\n",
      " 483  aspartateaminotransferase(ast)_max                  float64\n",
      " 484  b-typenatriureticpeptide,pro(probnp)_max            float64\n",
      " 485  baseexcess(poc)_max                                 float64\n",
      " 486  baseexcess,venous(poc)_max                          float64\n",
      " 487  basos_max                                           float64\n",
      " 488  basosabs_max                                        float64\n",
      " 489  benzodiazepinesscreen,urine,noconf._max             float64\n",
      " 490  bilirubindirect_max                                 float64\n",
      " 491  bilirubintotal_max                                  float64\n",
      " 492  bun_max                                             float64\n",
      " 493  bun/creatratio_max                                  float64\n",
      " 494  calcium_max                                         float64\n",
      " 495  calculatedco2(poc)_max                              float64\n",
      " 496  calculatedhco3(poc)i_max                            float64\n",
      " 497  calculatedo2saturation(poc)_max                     float64\n",
      " 498  chloride_max                                        float64\n",
      " 499  cktotal_max                                         float64\n",
      " 500  co2_max                                             float64\n",
      " 501  co2calculated,venous(poc)_max                       float64\n",
      " 502  co2,poc_max                                         float64\n",
      " 503  creatinine_max                                      float64\n",
      " 504  d-dimer_max                                         float64\n",
      " 505  egfr_max                                            float64\n",
      " 506  egfr(nonafricanamerican)_max                        float64\n",
      " 507  egfr(aframer)_max                                   float64\n",
      " 508  eos_max                                             float64\n",
      " 509  eosinoabs_max                                       float64\n",
      " 510  epithelialcells_max                                 float64\n",
      " 511  globulin_max                                        float64\n",
      " 512  glucose_max                                         float64\n",
      " 513  glucose,meter_max                                   float64\n",
      " 514  hco3calculated,venous(poc)_max                      float64\n",
      " 515  hematocrit_max                                      float64\n",
      " 516  hemoglobin_max                                      float64\n",
      " 517  immaturegrans(abs)_max                              float64\n",
      " 518  immaturegranulocytes_max                            float64\n",
      " 519  inr_max                                             float64\n",
      " 520  lactate,poc_max                                     float64\n",
      " 521  lipase_max                                          float64\n",
      " 522  lymphs_max                                          float64\n",
      " 523  magnesium_max                                       float64\n",
      " 524  mch_max                                             float64\n",
      " 525  mchc_max                                            float64\n",
      " 526  mcv_max                                             float64\n",
      " 527  monocytes_max                                       float64\n",
      " 528  monosabs_max                                        float64\n",
      " 529  mpv_max                                             float64\n",
      " 530  neutrophils_max                                     float64\n",
      " 531  nrbc_max                                            float64\n",
      " 532  nrbcabsolute_max                                    float64\n",
      " 533  o2satcalculated,venous(poc)_max                     float64\n",
      " 534  pco2(poc)_max                                       float64\n",
      " 535  pco2,venous(poc)_max                                float64\n",
      " 536  ph,venous(poc)_max                                  float64\n",
      " 537  phencyclidine(pcp)screen,urine,noconf._max          float64\n",
      " 538  phosphorus_max                                      float64\n",
      " 539  platelets_max                                       float64\n",
      " 540  po2(poc)_max                                        float64\n",
      " 541  po2,venous(poc)_max                                 float64\n",
      " 542  pocbun_max                                          float64\n",
      " 543  poccreatinine_max                                   float64\n",
      " 544  pocglucose_max                                      float64\n",
      " 545  pochematocrit_max                                   float64\n",
      " 546  pocionizedcalcium_max                               float64\n",
      " 547  pocph_max                                           float64\n",
      " 548  pocpotassium_max                                    float64\n",
      " 549  pocsodium_max                                       float64\n",
      " 550  poctroponini._max                                   float64\n",
      " 551  potassium_max                                       float64\n",
      " 552  proteintotal_max                                    float64\n",
      " 553  prothrombintime_max                                 float64\n",
      " 554  ptt_max                                             float64\n",
      " 555  rbc_max                                             float64\n",
      " 556  rbc/hpf_max                                         float64\n",
      " 557  rdw_max                                             float64\n",
      " 558  sodium_max                                          float64\n",
      " 559  troponini(poc)_max                                  float64\n",
      " 560  troponint_max                                       float64\n",
      " 561  tsh_max                                             float64\n",
      " 562  wbc_max                                             float64\n",
      " 563  wbc/hpf_max                                         float64\n",
      " 564  absolutelymphocytecount_median                      float64\n",
      " 565  acetonebld_median                                   float64\n",
      " 566  alanineaminotransferase(alt)_median                 float64\n",
      " 567  albumin_median                                      float64\n",
      " 568  alkphos_median                                      float64\n",
      " 569  anc(absneutrophilcount)_median                      float64\n",
      " 570  aniongap_median                                     float64\n",
      " 571  aspartateaminotransferase(ast)_median               float64\n",
      " 572  b-typenatriureticpeptide,pro(probnp)_median         float64\n",
      " 573  baseexcess(poc)_median                              float64\n",
      " 574  baseexcess,venous(poc)_median                       float64\n",
      " 575  basos_median                                        float64\n",
      " 576  basosabs_median                                     float64\n",
      " 577  benzodiazepinesscreen,urine,noconf._median          float64\n",
      " 578  bilirubindirect_median                              float64\n",
      " 579  bilirubintotal_median                               float64\n",
      " 580  bun_median                                          float64\n",
      " 581  bun/creatratio_median                               float64\n",
      " 582  calcium_median                                      float64\n",
      " 583  calculatedco2(poc)_median                           float64\n",
      " 584  calculatedhco3(poc)i_median                         float64\n",
      " 585  calculatedo2saturation(poc)_median                  float64\n",
      " 586  chloride_median                                     float64\n",
      " 587  cktotal_median                                      float64\n",
      " 588  co2_median                                          float64\n",
      " 589  co2calculated,venous(poc)_median                    float64\n",
      " 590  co2,poc_median                                      float64\n",
      " 591  creatinine_median                                   float64\n",
      " 592  d-dimer_median                                      float64\n",
      " 593  egfr_median                                         float64\n",
      " 594  egfr(nonafricanamerican)_median                     float64\n",
      " 595  egfr(aframer)_median                                float64\n",
      " 596  eos_median                                          float64\n",
      " 597  eosinoabs_median                                    float64\n",
      " 598  epithelialcells_median                              float64\n",
      " 599  globulin_median                                     float64\n",
      " 600  glucose_median                                      float64\n",
      " 601  glucose,meter_median                                float64\n",
      " 602  hco3calculated,venous(poc)_median                   float64\n",
      " 603  hematocrit_median                                   float64\n",
      " 604  hemoglobin_median                                   float64\n",
      " 605  immaturegrans(abs)_median                           float64\n",
      " 606  immaturegranulocytes_median                         float64\n",
      " 607  inr_median                                          float64\n",
      " 608  lactate,poc_median                                  float64\n",
      " 609  lipase_median                                       float64\n",
      " 610  lymphs_median                                       float64\n",
      " 611  magnesium_median                                    float64\n",
      " 612  mch_median                                          float64\n",
      " 613  mchc_median                                         float64\n",
      " 614  mcv_median                                          float64\n",
      " 615  monocytes_median                                    float64\n",
      " 616  monosabs_median                                     float64\n",
      " 617  mpv_median                                          float64\n",
      " 618  neutrophils_median                                  float64\n",
      " 619  nrbc_median                                         float64\n",
      " 620  nrbcabsolute_median                                 float64\n",
      " 621  o2satcalculated,venous(poc)_median                  float64\n",
      " 622  pco2(poc)_median                                    float64\n",
      " 623  pco2,venous(poc)_median                             float64\n",
      " 624  ph,venous(poc)_median                               float64\n",
      " 625  phencyclidine(pcp)screen,urine,noconf._median       float64\n",
      " 626  phosphorus_median                                   float64\n",
      " 627  platelets_median                                    float64\n",
      " 628  po2(poc)_median                                     float64\n",
      " 629  po2,venous(poc)_median                              float64\n",
      " 630  pocbun_median                                       float64\n",
      " 631  poccreatinine_median                                float64\n",
      " 632  pocglucose_median                                   float64\n",
      " 633  pochematocrit_median                                float64\n",
      " 634  pocionizedcalcium_median                            float64\n",
      " 635  pocph_median                                        float64\n",
      " 636  pocpotassium_median                                 float64\n",
      " 637  pocsodium_median                                    float64\n",
      " 638  poctroponini._median                                float64\n",
      " 639  potassium_median                                    float64\n",
      " 640  proteintotal_median                                 float64\n",
      " 641  prothrombintime_median                              float64\n",
      " 642  ptt_median                                          float64\n",
      " 643  rbc_median                                          float64\n",
      " 644  rbc/hpf_median                                      float64\n",
      " 645  rdw_median                                          float64\n",
      " 646  sodium_median                                       float64\n",
      " 647  troponini(poc)_median                               float64\n",
      " 648  troponint_median                                    float64\n",
      " 649  tsh_median                                          float64\n",
      " 650  wbc_median                                          float64\n",
      " 651  wbc/hpf_median                                      float64\n",
      " 652  bloodua_last                                        float64\n",
      " 653  glucoseua_last                                      float64\n",
      " 654  ketonesua_last                                      float64\n",
      " 655  leukocytesua_last                                   float64\n",
      " 656  nitriteua_last                                      float64\n",
      " 657  pregtestur_last                                     float64\n",
      " 658  proteinua_last                                      float64\n",
      " 659  bloodculture,routine_last                           float64\n",
      " 660  urineculture,routine_last                           float64\n",
      " 661  bloodua_npos                                        float64\n",
      " 662  glucoseua_npos                                      float64\n",
      " 663  ketonesua_npos                                      float64\n",
      " 664  leukocytesua_npos                                   float64\n",
      " 665  nitriteua_npos                                      float64\n",
      " 666  pregtestur_npos                                     float64\n",
      " 667  proteinua_npos                                      float64\n",
      " 668  bloodculture,routine_npos                           float64\n",
      " 669  urineculture,routine_npos                           float64\n",
      " 670  bloodua_count                                       float64\n",
      " 671  glucoseua_count                                     float64\n",
      " 672  ketonesua_count                                     float64\n",
      " 673  leukocytesua_count                                  float64\n",
      " 674  nitriteua_count                                     float64\n",
      " 675  pregtestur_count                                    float64\n",
      " 676  proteinua_count                                     float64\n",
      " 677  bloodculture,routine_count                          float64\n",
      " 678  urineculture,routine_count                          float64\n",
      " 679  triage_vital_hr                                     float64\n",
      " 680  triage_vital_sbp                                    float64\n",
      " 681  triage_vital_dbp                                    float64\n",
      " 682  triage_vital_rr                                     float64\n",
      " 683  triage_vital_o2                                     float64\n",
      " 684  triage_vital_o2_device                              float64\n",
      " 685  triage_vital_temp                                   float64\n",
      " 686  pulse_last                                          float64\n",
      " 687  resp_last                                           float64\n",
      " 688  spo2_last                                           float64\n",
      " 689  temp_last                                           float64\n",
      " 690  sbp_last                                            float64\n",
      " 691  dbp_last                                            float64\n",
      " 692  o2_device_last                                      float64\n",
      " 693  pulse_min                                           float64\n",
      " 694  resp_min                                            float64\n",
      " 695  spo2_min                                            float64\n",
      " 696  temp_min                                            float64\n",
      " 697  sbp_min                                             float64\n",
      " 698  dbp_min                                             float64\n",
      " 699  o2_device_min                                       float64\n",
      " 700  pulse_max                                           float64\n",
      " 701  resp_max                                            float64\n",
      " 702  spo2_max                                            float64\n",
      " 703  temp_max                                            float64\n",
      " 704  sbp_max                                             float64\n",
      " 705  dbp_max                                             float64\n",
      " 706  o2_device_max                                       float64\n",
      " 707  pulse_median                                        float64\n",
      " 708  resp_median                                         float64\n",
      " 709  spo2_median                                         float64\n",
      " 710  temp_median                                         float64\n",
      " 711  sbp_median                                          float64\n",
      " 712  dbp_median                                          float64\n",
      " 713  o2_device_median                                    float64\n",
      " 714  cxr_count                                           float64\n",
      " 715  echo_count                                          float64\n",
      " 716  ekg_count                                           float64\n",
      " 717  headct_count                                        float64\n",
      " 718  mri_count                                           float64\n",
      " 719  otherct_count                                       float64\n",
      " 720  otherimg_count                                      float64\n",
      " 721  otherus_count                                       float64\n",
      " 722  otherxr_count                                       float64\n",
      " 723  meds_analgesicandantihistaminecombination           float64\n",
      " 724  meds_analgesics                                     float64\n",
      " 725  meds_anesthetics                                    float64\n",
      " 726  meds_anti-obesitydrugs                              float64\n",
      " 727  meds_antiallergy                                    float64\n",
      " 728  meds_antiarthritics                                 float64\n",
      " 729  meds_antiasthmatics                                 float64\n",
      " 730  meds_antibiotics                                    float64\n",
      " 731  meds_anticoagulants                                 float64\n",
      " 732  meds_antidotes                                      float64\n",
      " 733  meds_antifungals                                    float64\n",
      " 734  meds_antihistamineanddecongestantcombination        float64\n",
      " 735  meds_antihistamines                                 float64\n",
      " 736  meds_antihyperglycemics                             float64\n",
      " 737  meds_antiinfectives                                 float64\n",
      " 738  meds_antiinfectives/miscellaneous                   float64\n",
      " 739  meds_antineoplastics                                float64\n",
      " 740  meds_antiparkinsondrugs                             float64\n",
      " 741  meds_antiplateletdrugs                              float64\n",
      " 742  meds_antivirals                                     float64\n",
      " 743  meds_autonomicdrugs                                 float64\n",
      " 744  meds_biologicals                                    float64\n",
      " 745  meds_blood                                          float64\n",
      " 746  meds_cardiacdrugs                                   float64\n",
      " 747  meds_cardiovascular                                 float64\n",
      " 748  meds_cnsdrugs                                       float64\n",
      " 749  meds_colonystimulatingfactors                       float64\n",
      " 750  meds_contraceptives                                 float64\n",
      " 751  meds_cough/coldpreparations                         float64\n",
      " 752  meds_diagnostic                                     float64\n",
      " 753  meds_diuretics                                      float64\n",
      " 754  meds_eentpreps                                      float64\n",
      " 755  meds_elect/caloric/h2o                              float64\n",
      " 756  meds_gastrointestinal                               float64\n",
      " 757  meds_herbals                                        float64\n",
      " 758  meds_hormones                                       float64\n",
      " 759  meds_immunosuppressants                             float64\n",
      " 760  meds_investigational                                float64\n",
      " 761  meds_miscellaneousmedicalsupplies,devices,non-drug  float64\n",
      " 762  meds_musclerelaxants                                float64\n",
      " 763  meds_pre-natalvitamins                              float64\n",
      " 764  meds_psychotherapeuticdrugs                         float64\n",
      " 765  meds_sedative/hypnotics                             float64\n",
      " 766  meds_skinpreps                                      float64\n",
      " 767  meds_smokingdeterrents                              float64\n",
      " 768  meds_thyroidpreps                                   float64\n",
      " 769  meds_unclassifieddrugproducts                       float64\n",
      " 770  meds_vitamins                                       float64\n",
      " 771  n_surgeries                                         float64\n",
      " 772  cc_abdominalcramping                                float64\n",
      " 773  cc_abdominaldistention                              float64\n",
      " 774  cc_abdominalpain                                    float64\n",
      " 775  cc_abdominalpainpregnant                            float64\n",
      " 776  cc_abnormallab                                      float64\n",
      " 777  cc_abscess                                          float64\n",
      " 778  cc_addictionproblem                                 float64\n",
      " 779  cc_agitation                                        float64\n",
      " 780  cc_alcoholintoxication                              float64\n",
      " 781  cc_alcoholproblem                                   float64\n",
      " 782  cc_allergicreaction                                 float64\n",
      " 783  cc_alteredmentalstatus                              float64\n",
      " 784  cc_animalbite                                       float64\n",
      " 785  cc_ankleinjury                                      float64\n",
      " 786  cc_anklepain                                        float64\n",
      " 787  cc_anxiety                                          float64\n",
      " 788  cc_arminjury                                        float64\n",
      " 789  cc_armpain                                          float64\n",
      " 790  cc_armswelling                                      float64\n",
      " 791  cc_assaultvictim                                    float64\n",
      " 792  cc_asthma                                           float64\n",
      " 793  cc_backpain                                         float64\n",
      " 794  cc_bleeding/bruising                                float64\n",
      " 795  cc_blurredvision                                    float64\n",
      " 796  cc_bodyfluidexposure                                float64\n",
      " 797  cc_breastpain                                       float64\n",
      " 798  cc_breathingdifficulty                              float64\n",
      " 799  cc_breathingproblem                                 float64\n",
      " 800  cc_burn                                             float64\n",
      " 801  cc_cardiacarrest                                    float64\n",
      " 802  cc_cellulitis                                       float64\n",
      " 803  cc_chestpain                                        float64\n",
      " 804  cc_chesttightness                                   float64\n",
      " 805  cc_chills                                           float64\n",
      " 806  cc_coldlikesymptoms                                 float64\n",
      " 807  cc_confusion                                        float64\n",
      " 808  cc_conjunctivitis                                   float64\n",
      " 809  cc_constipation                                     float64\n",
      " 810  cc_cough                                            float64\n",
      " 811  cc_cyst                                             float64\n",
      " 812  cc_decreasedbloodsugar-symptomatic                  float64\n",
      " 813  cc_dehydration                                      float64\n",
      " 814  cc_dentalpain                                       float64\n",
      " 815  cc_depression                                       float64\n",
      " 816  cc_detoxevaluation                                  float64\n",
      " 817  cc_diarrhea                                         float64\n",
      " 818  cc_dizziness                                        float64\n",
      " 819  cc_drug/alcoholassessment                           float64\n",
      " 820  cc_drugproblem                                      float64\n",
      " 821  cc_dyspnea                                          float64\n",
      " 822  cc_dysuria                                          float64\n",
      " 823  cc_earpain                                          float64\n",
      " 824  cc_earproblem                                       float64\n",
      " 825  cc_edema                                            float64\n",
      " 826  cc_elbowpain                                        float64\n",
      " 827  cc_elevatedbloodsugar-nosymptoms                    float64\n",
      " 828  cc_elevatedbloodsugar-symptomatic                   float64\n",
      " 829  cc_emesis                                           float64\n",
      " 830  cc_epigastricpain                                   float64\n",
      " 831  cc_epistaxis                                        float64\n",
      " 832  cc_exposuretostd                                    float64\n",
      " 833  cc_extremitylaceration                              float64\n",
      " 834  cc_extremityweakness                                float64\n",
      " 835  cc_eyeinjury                                        float64\n",
      " 836  cc_eyepain                                          float64\n",
      " 837  cc_eyeproblem                                       float64\n",
      " 838  cc_eyeredness                                       float64\n",
      " 839  cc_facialinjury                                     float64\n",
      " 840  cc_faciallaceration                                 float64\n",
      " 841  cc_facialpain                                       float64\n",
      " 842  cc_facialswelling                                   float64\n",
      " 843  cc_fall                                             float64\n",
      " 844  cc_fall>65                                          float64\n",
      " 845  cc_fatigue                                          float64\n",
      " 846  cc_femaleguproblem                                  float64\n",
      " 847  cc_fever                                            float64\n",
      " 848  cc_fever-75yearsorolder                             float64\n",
      " 849  cc_fever-9weeksto74years                            float64\n",
      " 850  cc_feverimmunocompromised                           float64\n",
      " 851  cc_fingerinjury                                     float64\n",
      " 852  cc_fingerpain                                       float64\n",
      " 853  cc_fingerswelling                                   float64\n",
      " 854  cc_flankpain                                        float64\n",
      " 855  cc_follow-upcellulitis                              float64\n",
      " 856  cc_footinjury                                       float64\n",
      " 857  cc_footpain                                         float64\n",
      " 858  cc_footswelling                                     float64\n",
      " 859  cc_foreignbodyineye                                 float64\n",
      " 860  cc_fulltrauma                                       float64\n",
      " 861  cc_generalizedbodyaches                             float64\n",
      " 862  cc_gibleeding                                       float64\n",
      " 863  cc_giproblem                                        float64\n",
      " 864  cc_groinpain                                        float64\n",
      " 865  cc_hallucinations                                   float64\n",
      " 866  cc_handinjury                                       float64\n",
      " 867  cc_handpain                                         float64\n",
      " 868  cc_headache                                         float64\n",
      " 869  cc_headache-newonsetornewsymptoms                   float64\n",
      " 870  cc_headache-recurrentorknowndxmigraines             float64\n",
      " 871  cc_headachere-evaluation                            float64\n",
      " 872  cc_headinjury                                       float64\n",
      " 873  cc_headlaceration                                   float64\n",
      " 874  cc_hematuria                                        float64\n",
      " 875  cc_hemoptysis                                       float64\n",
      " 876  cc_hippain                                          float64\n",
      " 877  cc_homicidal                                        float64\n",
      " 878  cc_hyperglycemia                                    float64\n",
      " 879  cc_hypertension                                     float64\n",
      " 880  cc_hypotension                                      float64\n",
      " 881  cc_influenza                                        float64\n",
      " 882  cc_ingestion                                        float64\n",
      " 883  cc_insectbite                                       float64\n",
      " 884  cc_irregularheartbeat                               float64\n",
      " 885  cc_jawpain                                          float64\n",
      " 886  cc_jointswelling                                    float64\n",
      " 887  cc_kneeinjury                                       float64\n",
      " 888  cc_kneepain                                         float64\n",
      " 889  cc_laceration                                       float64\n",
      " 890  cc_leginjury                                        float64\n",
      " 891  cc_legpain                                          float64\n",
      " 892  cc_legswelling                                      float64\n",
      " 893  cc_lethargy                                         float64\n",
      " 894  cc_lossofconsciousness                              float64\n",
      " 895  cc_maleguproblem                                    float64\n",
      " 896  cc_mass                                             float64\n",
      " 897  cc_medicalproblem                                   float64\n",
      " 898  cc_medicalscreening                                 float64\n",
      " 899  cc_medicationproblem                                float64\n",
      " 900  cc_medicationrefill                                 float64\n",
      " 901  cc_migraine                                         float64\n",
      " 902  cc_modifiedtrauma                                   float64\n",
      " 903  cc_motorcyclecrash                                  float64\n",
      " 904  cc_motorvehiclecrash                                float64\n",
      " 905  cc_multiplefalls                                    float64\n",
      " 906  cc_nasalcongestion                                  float64\n",
      " 907  cc_nausea                                           float64\n",
      " 908  cc_nearsyncope                                      float64\n",
      " 909  cc_neckpain                                         float64\n",
      " 910  cc_neurologicproblem                                float64\n",
      " 911  cc_numbness                                         float64\n",
      " 912  cc_oralswelling                                     float64\n",
      " 913  cc_otalgia                                          float64\n",
      " 914  cc_other                                            float64\n",
      " 915  cc_overdose-accidental                              float64\n",
      " 916  cc_overdose-intentional                             float64\n",
      " 917  cc_pain                                             float64\n",
      " 918  cc_palpitations                                     float64\n",
      " 919  cc_panicattack                                      float64\n",
      " 920  cc_pelvicpain                                       float64\n",
      " 921  cc_poisoning                                        float64\n",
      " 922  cc_post-opproblem                                   float64\n",
      " 923  cc_psychiatricevaluation                            float64\n",
      " 924  cc_psychoticsymptoms                                float64\n",
      " 925  cc_rapidheartrate                                   float64\n",
      " 926  cc_rash                                             float64\n",
      " 927  cc_rectalbleeding                                   float64\n",
      " 928  cc_rectalpain                                       float64\n",
      " 929  cc_respiratorydistress                              float64\n",
      " 930  cc_ribinjury                                        float64\n",
      " 931  cc_ribpain                                          float64\n",
      " 932  cc_seizure-newonset                                 float64\n",
      " 933  cc_seizure-priorhxof                                float64\n",
      " 934  cc_seizures                                         float64\n",
      " 935  cc_shortnessofbreath                                float64\n",
      " 936  cc_shoulderinjury                                   float64\n",
      " 937  cc_shoulderpain                                     float64\n",
      " 938  cc_sicklecellpain                                   float64\n",
      " 939  cc_sinusproblem                                     float64\n",
      " 940  cc_skinirritation                                   float64\n",
      " 941  cc_skinproblem                                      float64\n",
      " 942  cc_sorethroat                                       float64\n",
      " 943  cc_stdcheck                                         float64\n",
      " 944  cc_strokealert                                      float64\n",
      " 945  cc_suicidal                                         float64\n",
      " 946  cc_suture/stapleremoval                             float64\n",
      " 947  cc_swallowedforeignbody                             float64\n",
      " 948  cc_syncope                                          float64\n",
      " 949  cc_tachycardia                                      float64\n",
      " 950  cc_testiclepain                                     float64\n",
      " 951  cc_thumbinjury                                      float64\n",
      " 952  cc_tickremoval                                      float64\n",
      " 953  cc_toeinjury                                        float64\n",
      " 954  cc_toepain                                          float64\n",
      " 955  cc_trauma                                           float64\n",
      " 956  cc_unresponsive                                     float64\n",
      " 957  cc_uri                                              float64\n",
      " 958  cc_urinaryfrequency                                 float64\n",
      " 959  cc_urinaryretention                                 float64\n",
      " 960  cc_urinarytractinfection                            float64\n",
      " 961  cc_vaginalbleeding                                  float64\n",
      " 962  cc_vaginaldischarge                                 float64\n",
      " 963  cc_vaginalpain                                      float64\n",
      " 964  cc_weakness                                         float64\n",
      " 965  cc_wheezing                                         float64\n",
      " 966  cc_withdrawal-alcohol                               float64\n",
      " 967  cc_woundcheck                                       float64\n",
      " 968  cc_woundinfection                                   float64\n",
      " 969  cc_woundre-evaluation                               float64\n",
      " 970  cc_wristinjury                                      float64\n",
      " 971  cc_wristpain                                        float64\n",
      "dtypes: float64(955), int64(2), object(15)\n",
      "memory usage: 4.1+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1714a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dep_name</th>\n",
       "      <th>esi</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>lang</th>\n",
       "      <th>religion</th>\n",
       "      <th>maritalstatus</th>\n",
       "      <th>employstatus</th>\n",
       "      <th>...</th>\n",
       "      <th>cc_vaginaldischarge</th>\n",
       "      <th>cc_vaginalpain</th>\n",
       "      <th>cc_weakness</th>\n",
       "      <th>cc_wheezing</th>\n",
       "      <th>cc_withdrawal-alcohol</th>\n",
       "      <th>cc_woundcheck</th>\n",
       "      <th>cc_woundinfection</th>\n",
       "      <th>cc_woundre-evaluation</th>\n",
       "      <th>cc_wristinjury</th>\n",
       "      <th>cc_wristpain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>White or Caucasian</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>English</td>\n",
       "      <td>Pentecostal</td>\n",
       "      <td>Married</td>\n",
       "      <td>Not Employed</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>English</td>\n",
       "      <td>Pentecostal</td>\n",
       "      <td>Married</td>\n",
       "      <td>Not Employed</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>Native Hawaiian or Other Pacific Islander</td>\n",
       "      <td>English</td>\n",
       "      <td>Pentecostal</td>\n",
       "      <td>Married</td>\n",
       "      <td>Not Employed</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>3.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Pentecostal</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Retired</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 972 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dep_name  esi   age  gender           ethnicity   \n",
       "0        B  4.0  40.0    Male  Hispanic or Latino  \\\n",
       "1        B  4.0  66.0    Male  Hispanic or Latino   \n",
       "2        B  2.0  66.0    Male  Hispanic or Latino   \n",
       "3        A  2.0  66.0    Male  Hispanic or Latino   \n",
       "4        A  3.0  84.0  Female  Hispanic or Latino   \n",
       "\n",
       "                                        race     lang     religion   \n",
       "0                         White or Caucasian  English          NaN  \\\n",
       "1  Native Hawaiian or Other Pacific Islander  English  Pentecostal   \n",
       "2  Native Hawaiian or Other Pacific Islander  English  Pentecostal   \n",
       "3  Native Hawaiian or Other Pacific Islander  English  Pentecostal   \n",
       "4                                      Other    Other  Pentecostal   \n",
       "\n",
       "  maritalstatus  employstatus  ... cc_vaginaldischarge cc_vaginalpain   \n",
       "0        Single     Full Time  ...                 0.0            0.0  \\\n",
       "1       Married  Not Employed  ...                 0.0            0.0   \n",
       "2       Married  Not Employed  ...                 0.0            0.0   \n",
       "3       Married  Not Employed  ...                 0.0            0.0   \n",
       "4       Widowed       Retired  ...                 0.0            0.0   \n",
       "\n",
       "  cc_weakness cc_wheezing cc_withdrawal-alcohol cc_woundcheck   \n",
       "0         0.0         0.0                   0.0           0.0  \\\n",
       "1         0.0         0.0                   0.0           0.0   \n",
       "2         0.0         0.0                   0.0           0.0   \n",
       "3         0.0         0.0                   0.0           0.0   \n",
       "4         0.0         0.0                   0.0           0.0   \n",
       "\n",
       "  cc_woundinfection  cc_woundre-evaluation  cc_wristinjury  cc_wristpain  \n",
       "0               0.0                    0.0             0.0           0.0  \n",
       "1               0.0                    0.0             0.0           0.0  \n",
       "2               0.0                    0.0             0.0           0.0  \n",
       "3               0.0                    0.0             0.0           0.0  \n",
       "4               0.0                    0.0             0.0           0.0  \n",
       "\n",
       "[5 rows x 972 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73f075e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disposition\n",
       "Discharge    393848\n",
       "Admit        166638\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.disposition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498c8d5",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b11aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38424ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset\n",
    "class TriageDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, file_path = \"Hospital Triage and Patient History.csv\"):\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Clean data (convert categorical to numeric, remove department name)\n",
    "        df.drop(columns=['dep_name'], inplace=True)\n",
    "        for col in df:\n",
    "            dt = df[col].dtype \n",
    "            if dt == int or dt == float:\n",
    "                df[col].fillna(0, inplace=True)\n",
    "            else:\n",
    "                df[col].fillna(\"\", inplace=True)\n",
    "        \n",
    "        categorical = []\n",
    "        for (key, value) in df.dtypes.items():\n",
    "            if (value == 'object'):\n",
    "                df[key] = df[key].astype('category')\n",
    "                categorical.append(key)\n",
    "                category_num = f'{key}_num'\n",
    "                df[category_num] = df[key].cat.codes.astype('float64')\n",
    "            elif (value == 'int64'):\n",
    "                df[key] = df[key].astype('float64')\n",
    "        \n",
    "        df = df.drop(columns=categorical)\n",
    "        label_ind = df.columns.get_loc('disposition_num')\n",
    "        features_ind = [i for i in range(df.shape[1])]\n",
    "        features_ind.remove(label_ind)\n",
    "        \n",
    "        # Set features and label\n",
    "        self.data = df\n",
    "        self.features = df.iloc[:, features_ind]\n",
    "        self.label = df.iloc[:, label_ind]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.features.iloc[index].values\n",
    "        y = self.label.iloc[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e897978a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create train and test datasets\n",
    "triage_dataset = TriageDataset()\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(triage_dataset, [0.8, 0.2], generator=generator)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d358f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SmallModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(970, 300) # 972 - 2 features\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(300, 100)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(100, 30)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.linear4 = torch.nn.Linear(30, 10)\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        self.linear5 = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.linear4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6276b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = SmallModel().to(device)\n",
    "# optimizer = torch.optim.Adam(small_model.parameters(), lr= 0.005, weight_decay= 0.001)\n",
    "# optimizer = torch.optim.Adam(small_model.parameters(), lr= 0.01, weight_decay= 0.005)\n",
    "optimizer = torch.optim.Adam(small_model.parameters(), lr = 0.001, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e86bfc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "def trainer(model, train_loader, test_loader, num_epochs, optimizer):\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = small_model(features)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 1 == 0:\n",
    "                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in train_loader:\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "        train_accuracy = correct/total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.4f}')\n",
    "        train_accs.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_accuracy = correct/total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {test_accuracy:.4f}')\n",
    "        test_accs.append(test_accuracy)\n",
    "    \n",
    "    return train_accs, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7201fbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/3504], Loss: 2.6212\n",
      "Epoch [1/1], Step [2/3504], Loss: 2.1728\n",
      "Epoch [1/1], Step [3/3504], Loss: 1.3842\n",
      "Epoch [1/1], Step [4/3504], Loss: 1.2248\n",
      "Epoch [1/1], Step [5/3504], Loss: 1.0694\n",
      "Epoch [1/1], Step [6/3504], Loss: 0.9944\n",
      "Epoch [1/1], Step [7/3504], Loss: 1.3315\n",
      "Epoch [1/1], Step [8/3504], Loss: 0.8157\n",
      "Epoch [1/1], Step [9/3504], Loss: 2.3966\n",
      "Epoch [1/1], Step [10/3504], Loss: 0.7969\n",
      "Epoch [1/1], Step [11/3504], Loss: 0.6869\n",
      "Epoch [1/1], Step [12/3504], Loss: 0.7931\n",
      "Epoch [1/1], Step [13/3504], Loss: 0.7105\n",
      "Epoch [1/1], Step [14/3504], Loss: 0.6695\n",
      "Epoch [1/1], Step [15/3504], Loss: 0.7308\n",
      "Epoch [1/1], Step [16/3504], Loss: 0.6474\n",
      "Epoch [1/1], Step [17/3504], Loss: 0.6508\n",
      "Epoch [1/1], Step [18/3504], Loss: 0.7155\n",
      "Epoch [1/1], Step [19/3504], Loss: 0.6535\n",
      "Epoch [1/1], Step [20/3504], Loss: 0.6344\n",
      "Epoch [1/1], Step [21/3504], Loss: 0.6652\n",
      "Epoch [1/1], Step [22/3504], Loss: 0.5740\n",
      "Epoch [1/1], Step [23/3504], Loss: 0.6306\n",
      "Epoch [1/1], Step [24/3504], Loss: 0.5899\n",
      "Epoch [1/1], Step [25/3504], Loss: 0.5651\n",
      "Epoch [1/1], Step [26/3504], Loss: 0.6307\n",
      "Epoch [1/1], Step [27/3504], Loss: 2.6415\n",
      "Epoch [1/1], Step [28/3504], Loss: 1.3130\n",
      "Epoch [1/1], Step [29/3504], Loss: 0.5876\n",
      "Epoch [1/1], Step [30/3504], Loss: 0.6701\n",
      "Epoch [1/1], Step [31/3504], Loss: 0.5575\n",
      "Epoch [1/1], Step [32/3504], Loss: 0.6555\n",
      "Epoch [1/1], Step [33/3504], Loss: 0.6622\n",
      "Epoch [1/1], Step [34/3504], Loss: 0.6557\n",
      "Epoch [1/1], Step [35/3504], Loss: 0.5632\n",
      "Epoch [1/1], Step [36/3504], Loss: 0.5709\n",
      "Epoch [1/1], Step [37/3504], Loss: 0.5897\n",
      "Epoch [1/1], Step [38/3504], Loss: 0.5100\n",
      "Epoch [1/1], Step [39/3504], Loss: 0.6133\n",
      "Epoch [1/1], Step [40/3504], Loss: 0.5517\n",
      "Epoch [1/1], Step [41/3504], Loss: 0.4962\n",
      "Epoch [1/1], Step [42/3504], Loss: 0.5294\n",
      "Epoch [1/1], Step [43/3504], Loss: 0.4917\n",
      "Epoch [1/1], Step [44/3504], Loss: 0.6063\n",
      "Epoch [1/1], Step [45/3504], Loss: 0.5336\n",
      "Epoch [1/1], Step [46/3504], Loss: 0.7266\n",
      "Epoch [1/1], Step [47/3504], Loss: 0.6215\n",
      "Epoch [1/1], Step [48/3504], Loss: 0.4914\n",
      "Epoch [1/1], Step [49/3504], Loss: 0.6839\n",
      "Epoch [1/1], Step [50/3504], Loss: 0.5784\n",
      "Epoch [1/1], Step [51/3504], Loss: 0.5136\n",
      "Epoch [1/1], Step [52/3504], Loss: 0.5031\n",
      "Epoch [1/1], Step [53/3504], Loss: 0.5744\n",
      "Epoch [1/1], Step [54/3504], Loss: 0.5341\n",
      "Epoch [1/1], Step [55/3504], Loss: 0.6642\n",
      "Epoch [1/1], Step [56/3504], Loss: 0.5598\n",
      "Epoch [1/1], Step [57/3504], Loss: 0.5479\n",
      "Epoch [1/1], Step [58/3504], Loss: 0.4794\n",
      "Epoch [1/1], Step [59/3504], Loss: 0.6403\n",
      "Epoch [1/1], Step [60/3504], Loss: 0.5030\n",
      "Epoch [1/1], Step [61/3504], Loss: 0.5514\n",
      "Epoch [1/1], Step [62/3504], Loss: 0.5207\n",
      "Epoch [1/1], Step [63/3504], Loss: 0.6112\n",
      "Epoch [1/1], Step [64/3504], Loss: 0.5999\n",
      "Epoch [1/1], Step [65/3504], Loss: 0.5523\n",
      "Epoch [1/1], Step [66/3504], Loss: 0.6303\n",
      "Epoch [1/1], Step [67/3504], Loss: 0.5571\n",
      "Epoch [1/1], Step [68/3504], Loss: 0.5233\n",
      "Epoch [1/1], Step [69/3504], Loss: 0.5520\n",
      "Epoch [1/1], Step [70/3504], Loss: 0.5975\n",
      "Epoch [1/1], Step [71/3504], Loss: 0.5292\n",
      "Epoch [1/1], Step [72/3504], Loss: 0.6130\n",
      "Epoch [1/1], Step [73/3504], Loss: 0.5821\n",
      "Epoch [1/1], Step [74/3504], Loss: 0.5516\n",
      "Epoch [1/1], Step [75/3504], Loss: 0.6772\n",
      "Epoch [1/1], Step [76/3504], Loss: 0.5721\n",
      "Epoch [1/1], Step [77/3504], Loss: 0.5360\n",
      "Epoch [1/1], Step [78/3504], Loss: 0.5633\n",
      "Epoch [1/1], Step [79/3504], Loss: 0.6143\n",
      "Epoch [1/1], Step [80/3504], Loss: 0.6489\n",
      "Epoch [1/1], Step [81/3504], Loss: 0.5894\n",
      "Epoch [1/1], Step [82/3504], Loss: 0.7066\n",
      "Epoch [1/1], Step [83/3504], Loss: 1.1571\n",
      "Epoch [1/1], Step [84/3504], Loss: 0.5841\n",
      "Epoch [1/1], Step [85/3504], Loss: 0.5280\n",
      "Epoch [1/1], Step [86/3504], Loss: 0.5909\n",
      "Epoch [1/1], Step [87/3504], Loss: 0.5680\n",
      "Epoch [1/1], Step [88/3504], Loss: 0.5662\n",
      "Epoch [1/1], Step [89/3504], Loss: 0.5407\n",
      "Epoch [1/1], Step [90/3504], Loss: 0.5482\n",
      "Epoch [1/1], Step [91/3504], Loss: 0.5675\n",
      "Epoch [1/1], Step [92/3504], Loss: 0.6559\n",
      "Epoch [1/1], Step [93/3504], Loss: 0.5875\n",
      "Epoch [1/1], Step [94/3504], Loss: 0.5309\n",
      "Epoch [1/1], Step [95/3504], Loss: 0.4748\n",
      "Epoch [1/1], Step [96/3504], Loss: 0.5387\n",
      "Epoch [1/1], Step [97/3504], Loss: 0.4562\n",
      "Epoch [1/1], Step [98/3504], Loss: 0.6648\n",
      "Epoch [1/1], Step [99/3504], Loss: 0.5863\n",
      "Epoch [1/1], Step [100/3504], Loss: 0.5457\n",
      "Epoch [1/1], Step [101/3504], Loss: 0.5830\n",
      "Epoch [1/1], Step [102/3504], Loss: 0.5802\n",
      "Epoch [1/1], Step [103/3504], Loss: 0.9371\n",
      "Epoch [1/1], Step [104/3504], Loss: 0.5571\n",
      "Epoch [1/1], Step [105/3504], Loss: 0.5990\n",
      "Epoch [1/1], Step [106/3504], Loss: 1.0032\n",
      "Epoch [1/1], Step [107/3504], Loss: 0.6295\n",
      "Epoch [1/1], Step [108/3504], Loss: 0.5352\n",
      "Epoch [1/1], Step [109/3504], Loss: 0.5877\n",
      "Epoch [1/1], Step [110/3504], Loss: 0.5610\n",
      "Epoch [1/1], Step [111/3504], Loss: 0.5552\n",
      "Epoch [1/1], Step [112/3504], Loss: 0.4983\n",
      "Epoch [1/1], Step [113/3504], Loss: 0.5116\n",
      "Epoch [1/1], Step [114/3504], Loss: 0.5110\n",
      "Epoch [1/1], Step [115/3504], Loss: 0.4591\n",
      "Epoch [1/1], Step [116/3504], Loss: 0.5044\n",
      "Epoch [1/1], Step [117/3504], Loss: 0.5212\n",
      "Epoch [1/1], Step [118/3504], Loss: 0.5666\n",
      "Epoch [1/1], Step [119/3504], Loss: 0.4776\n",
      "Epoch [1/1], Step [120/3504], Loss: 0.5136\n",
      "Epoch [1/1], Step [121/3504], Loss: 0.5171\n",
      "Epoch [1/1], Step [122/3504], Loss: 0.4611\n",
      "Epoch [1/1], Step [123/3504], Loss: 0.5008\n",
      "Epoch [1/1], Step [124/3504], Loss: 0.5875\n",
      "Epoch [1/1], Step [125/3504], Loss: 0.4780\n",
      "Epoch [1/1], Step [126/3504], Loss: 0.5215\n",
      "Epoch [1/1], Step [127/3504], Loss: 0.5090\n",
      "Epoch [1/1], Step [128/3504], Loss: 0.4116\n",
      "Epoch [1/1], Step [129/3504], Loss: 0.6465\n",
      "Epoch [1/1], Step [130/3504], Loss: 0.5257\n",
      "Epoch [1/1], Step [131/3504], Loss: 0.5288\n",
      "Epoch [1/1], Step [132/3504], Loss: 0.5554\n",
      "Epoch [1/1], Step [133/3504], Loss: 0.5182\n",
      "Epoch [1/1], Step [134/3504], Loss: 0.5354\n",
      "Epoch [1/1], Step [135/3504], Loss: 0.5653\n",
      "Epoch [1/1], Step [136/3504], Loss: 0.4934\n",
      "Epoch [1/1], Step [137/3504], Loss: 0.4628\n",
      "Epoch [1/1], Step [138/3504], Loss: 0.4512\n",
      "Epoch [1/1], Step [139/3504], Loss: 0.5573\n",
      "Epoch [1/1], Step [140/3504], Loss: 0.4455\n",
      "Epoch [1/1], Step [141/3504], Loss: 0.4533\n",
      "Epoch [1/1], Step [142/3504], Loss: 0.4430\n",
      "Epoch [1/1], Step [143/3504], Loss: 0.4640\n",
      "Epoch [1/1], Step [144/3504], Loss: 0.4656\n",
      "Epoch [1/1], Step [145/3504], Loss: 0.5631\n",
      "Epoch [1/1], Step [146/3504], Loss: 0.4525\n",
      "Epoch [1/1], Step [147/3504], Loss: 0.4687\n",
      "Epoch [1/1], Step [148/3504], Loss: 0.4960\n",
      "Epoch [1/1], Step [149/3504], Loss: 0.5278\n",
      "Epoch [1/1], Step [150/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [151/3504], Loss: 0.5231\n",
      "Epoch [1/1], Step [152/3504], Loss: 0.4887\n",
      "Epoch [1/1], Step [153/3504], Loss: 0.5140\n",
      "Epoch [1/1], Step [154/3504], Loss: 0.4992\n",
      "Epoch [1/1], Step [155/3504], Loss: 0.6052\n",
      "Epoch [1/1], Step [156/3504], Loss: 0.4565\n",
      "Epoch [1/1], Step [157/3504], Loss: 0.4411\n",
      "Epoch [1/1], Step [158/3504], Loss: 0.4657\n",
      "Epoch [1/1], Step [159/3504], Loss: 0.4813\n",
      "Epoch [1/1], Step [160/3504], Loss: 0.4723\n",
      "Epoch [1/1], Step [161/3504], Loss: 0.4847\n",
      "Epoch [1/1], Step [162/3504], Loss: 1.8669\n",
      "Epoch [1/1], Step [163/3504], Loss: 0.4951\n",
      "Epoch [1/1], Step [164/3504], Loss: 0.5208\n",
      "Epoch [1/1], Step [165/3504], Loss: 0.5252\n",
      "Epoch [1/1], Step [166/3504], Loss: 0.4489\n",
      "Epoch [1/1], Step [167/3504], Loss: 0.5017\n",
      "Epoch [1/1], Step [168/3504], Loss: 0.4610\n",
      "Epoch [1/1], Step [169/3504], Loss: 0.5005\n",
      "Epoch [1/1], Step [170/3504], Loss: 0.6507\n",
      "Epoch [1/1], Step [171/3504], Loss: 0.5959\n",
      "Epoch [1/1], Step [172/3504], Loss: 0.8777\n",
      "Epoch [1/1], Step [173/3504], Loss: 0.4356\n",
      "Epoch [1/1], Step [174/3504], Loss: 0.5073\n",
      "Epoch [1/1], Step [175/3504], Loss: 0.4302\n",
      "Epoch [1/1], Step [176/3504], Loss: 0.4465\n",
      "Epoch [1/1], Step [177/3504], Loss: 0.5226\n",
      "Epoch [1/1], Step [178/3504], Loss: 0.5489\n",
      "Epoch [1/1], Step [179/3504], Loss: 0.4205\n",
      "Epoch [1/1], Step [180/3504], Loss: 0.5612\n",
      "Epoch [1/1], Step [181/3504], Loss: 0.4126\n",
      "Epoch [1/1], Step [182/3504], Loss: 0.3888\n",
      "Epoch [1/1], Step [183/3504], Loss: 0.4980\n",
      "Epoch [1/1], Step [184/3504], Loss: 0.4327\n",
      "Epoch [1/1], Step [185/3504], Loss: 0.5145\n",
      "Epoch [1/1], Step [186/3504], Loss: 0.4476\n",
      "Epoch [1/1], Step [187/3504], Loss: 0.5816\n",
      "Epoch [1/1], Step [188/3504], Loss: 0.5309\n",
      "Epoch [1/1], Step [189/3504], Loss: 0.4474\n",
      "Epoch [1/1], Step [190/3504], Loss: 0.5529\n",
      "Epoch [1/1], Step [191/3504], Loss: 0.4642\n",
      "Epoch [1/1], Step [192/3504], Loss: 0.5050\n",
      "Epoch [1/1], Step [193/3504], Loss: 0.5485\n",
      "Epoch [1/1], Step [194/3504], Loss: 0.4768\n",
      "Epoch [1/1], Step [195/3504], Loss: 0.4542\n",
      "Epoch [1/1], Step [196/3504], Loss: 0.4608\n",
      "Epoch [1/1], Step [197/3504], Loss: 0.5192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [198/3504], Loss: 0.4555\n",
      "Epoch [1/1], Step [199/3504], Loss: 0.4570\n",
      "Epoch [1/1], Step [200/3504], Loss: 0.4781\n",
      "Epoch [1/1], Step [201/3504], Loss: 0.4249\n",
      "Epoch [1/1], Step [202/3504], Loss: 0.4739\n",
      "Epoch [1/1], Step [203/3504], Loss: 0.7029\n",
      "Epoch [1/1], Step [204/3504], Loss: 0.4884\n",
      "Epoch [1/1], Step [205/3504], Loss: 0.5022\n",
      "Epoch [1/1], Step [206/3504], Loss: 0.5092\n",
      "Epoch [1/1], Step [207/3504], Loss: 1.0405\n",
      "Epoch [1/1], Step [208/3504], Loss: 0.4838\n",
      "Epoch [1/1], Step [209/3504], Loss: 0.5027\n",
      "Epoch [1/1], Step [210/3504], Loss: 0.4396\n",
      "Epoch [1/1], Step [211/3504], Loss: 0.4418\n",
      "Epoch [1/1], Step [212/3504], Loss: 0.4169\n",
      "Epoch [1/1], Step [213/3504], Loss: 0.5066\n",
      "Epoch [1/1], Step [214/3504], Loss: 0.4669\n",
      "Epoch [1/1], Step [215/3504], Loss: 0.4938\n",
      "Epoch [1/1], Step [216/3504], Loss: 0.4641\n",
      "Epoch [1/1], Step [217/3504], Loss: 0.6008\n",
      "Epoch [1/1], Step [218/3504], Loss: 0.4720\n",
      "Epoch [1/1], Step [219/3504], Loss: 0.4152\n",
      "Epoch [1/1], Step [220/3504], Loss: 0.4369\n",
      "Epoch [1/1], Step [221/3504], Loss: 0.4871\n",
      "Epoch [1/1], Step [222/3504], Loss: 0.9775\n",
      "Epoch [1/1], Step [223/3504], Loss: 0.4451\n",
      "Epoch [1/1], Step [224/3504], Loss: 0.9150\n",
      "Epoch [1/1], Step [225/3504], Loss: 0.4776\n",
      "Epoch [1/1], Step [226/3504], Loss: 0.4062\n",
      "Epoch [1/1], Step [227/3504], Loss: 0.4121\n",
      "Epoch [1/1], Step [228/3504], Loss: 0.5067\n",
      "Epoch [1/1], Step [229/3504], Loss: 0.5231\n",
      "Epoch [1/1], Step [230/3504], Loss: 0.5691\n",
      "Epoch [1/1], Step [231/3504], Loss: 0.4604\n",
      "Epoch [1/1], Step [232/3504], Loss: 0.4164\n",
      "Epoch [1/1], Step [233/3504], Loss: 0.5041\n",
      "Epoch [1/1], Step [234/3504], Loss: 0.7572\n",
      "Epoch [1/1], Step [235/3504], Loss: 0.6240\n",
      "Epoch [1/1], Step [236/3504], Loss: 0.4430\n",
      "Epoch [1/1], Step [237/3504], Loss: 0.5057\n",
      "Epoch [1/1], Step [238/3504], Loss: 0.5085\n",
      "Epoch [1/1], Step [239/3504], Loss: 0.4474\n",
      "Epoch [1/1], Step [240/3504], Loss: 0.5471\n",
      "Epoch [1/1], Step [241/3504], Loss: 0.4567\n",
      "Epoch [1/1], Step [242/3504], Loss: 0.5473\n",
      "Epoch [1/1], Step [243/3504], Loss: 0.3912\n",
      "Epoch [1/1], Step [244/3504], Loss: 0.4901\n",
      "Epoch [1/1], Step [245/3504], Loss: 0.4619\n",
      "Epoch [1/1], Step [246/3504], Loss: 0.4432\n",
      "Epoch [1/1], Step [247/3504], Loss: 0.4486\n",
      "Epoch [1/1], Step [248/3504], Loss: 0.4481\n",
      "Epoch [1/1], Step [249/3504], Loss: 0.4451\n",
      "Epoch [1/1], Step [250/3504], Loss: 0.4693\n",
      "Epoch [1/1], Step [251/3504], Loss: 0.4198\n",
      "Epoch [1/1], Step [252/3504], Loss: 0.5909\n",
      "Epoch [1/1], Step [253/3504], Loss: 0.5474\n",
      "Epoch [1/1], Step [254/3504], Loss: 0.4512\n",
      "Epoch [1/1], Step [255/3504], Loss: 0.4562\n",
      "Epoch [1/1], Step [256/3504], Loss: 0.4392\n",
      "Epoch [1/1], Step [257/3504], Loss: 0.5243\n",
      "Epoch [1/1], Step [258/3504], Loss: 0.4818\n",
      "Epoch [1/1], Step [259/3504], Loss: 0.7515\n",
      "Epoch [1/1], Step [260/3504], Loss: 0.4501\n",
      "Epoch [1/1], Step [261/3504], Loss: 0.5202\n",
      "Epoch [1/1], Step [262/3504], Loss: 0.4696\n",
      "Epoch [1/1], Step [263/3504], Loss: 0.4439\n",
      "Epoch [1/1], Step [264/3504], Loss: 0.3609\n",
      "Epoch [1/1], Step [265/3504], Loss: 0.4254\n",
      "Epoch [1/1], Step [266/3504], Loss: 0.4031\n",
      "Epoch [1/1], Step [267/3504], Loss: 0.4421\n",
      "Epoch [1/1], Step [268/3504], Loss: 0.4301\n",
      "Epoch [1/1], Step [269/3504], Loss: 0.4230\n",
      "Epoch [1/1], Step [270/3504], Loss: 0.4838\n",
      "Epoch [1/1], Step [271/3504], Loss: 0.5100\n",
      "Epoch [1/1], Step [272/3504], Loss: 0.4447\n",
      "Epoch [1/1], Step [273/3504], Loss: 0.4384\n",
      "Epoch [1/1], Step [274/3504], Loss: 0.8713\n",
      "Epoch [1/1], Step [275/3504], Loss: 0.4906\n",
      "Epoch [1/1], Step [276/3504], Loss: 0.5157\n",
      "Epoch [1/1], Step [277/3504], Loss: 0.4645\n",
      "Epoch [1/1], Step [278/3504], Loss: 0.4002\n",
      "Epoch [1/1], Step [279/3504], Loss: 0.3570\n",
      "Epoch [1/1], Step [280/3504], Loss: 0.4414\n",
      "Epoch [1/1], Step [281/3504], Loss: 0.4702\n",
      "Epoch [1/1], Step [282/3504], Loss: 0.7177\n",
      "Epoch [1/1], Step [283/3504], Loss: 0.5422\n",
      "Epoch [1/1], Step [284/3504], Loss: 0.4483\n",
      "Epoch [1/1], Step [285/3504], Loss: 0.3636\n",
      "Epoch [1/1], Step [286/3504], Loss: 0.4351\n",
      "Epoch [1/1], Step [287/3504], Loss: 0.2368\n",
      "Epoch [1/1], Step [288/3504], Loss: 0.6368\n",
      "Epoch [1/1], Step [289/3504], Loss: 0.3543\n",
      "Epoch [1/1], Step [290/3504], Loss: 0.4719\n",
      "Epoch [1/1], Step [291/3504], Loss: 0.4888\n",
      "Epoch [1/1], Step [292/3504], Loss: 0.4307\n",
      "Epoch [1/1], Step [293/3504], Loss: 0.5374\n",
      "Epoch [1/1], Step [294/3504], Loss: 0.5437\n",
      "Epoch [1/1], Step [295/3504], Loss: 0.4636\n",
      "Epoch [1/1], Step [296/3504], Loss: 0.5348\n",
      "Epoch [1/1], Step [297/3504], Loss: 0.5397\n",
      "Epoch [1/1], Step [298/3504], Loss: 0.4825\n",
      "Epoch [1/1], Step [299/3504], Loss: 0.4424\n",
      "Epoch [1/1], Step [300/3504], Loss: 0.4023\n",
      "Epoch [1/1], Step [301/3504], Loss: 0.4011\n",
      "Epoch [1/1], Step [302/3504], Loss: 0.5760\n",
      "Epoch [1/1], Step [303/3504], Loss: 0.5072\n",
      "Epoch [1/1], Step [304/3504], Loss: 0.4670\n",
      "Epoch [1/1], Step [305/3504], Loss: 0.4460\n",
      "Epoch [1/1], Step [306/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [307/3504], Loss: 1.8510\n",
      "Epoch [1/1], Step [308/3504], Loss: 0.4965\n",
      "Epoch [1/1], Step [309/3504], Loss: 0.4466\n",
      "Epoch [1/1], Step [310/3504], Loss: 0.4327\n",
      "Epoch [1/1], Step [311/3504], Loss: 0.5406\n",
      "Epoch [1/1], Step [312/3504], Loss: 0.5284\n",
      "Epoch [1/1], Step [313/3504], Loss: 0.5004\n",
      "Epoch [1/1], Step [314/3504], Loss: 0.6591\n",
      "Epoch [1/1], Step [315/3504], Loss: 0.4969\n",
      "Epoch [1/1], Step [316/3504], Loss: 0.5583\n",
      "Epoch [1/1], Step [317/3504], Loss: 0.5113\n",
      "Epoch [1/1], Step [318/3504], Loss: 0.5802\n",
      "Epoch [1/1], Step [319/3504], Loss: 0.4176\n",
      "Epoch [1/1], Step [320/3504], Loss: 0.6556\n",
      "Epoch [1/1], Step [321/3504], Loss: 0.4624\n",
      "Epoch [1/1], Step [322/3504], Loss: 0.5171\n",
      "Epoch [1/1], Step [323/3504], Loss: 0.4271\n",
      "Epoch [1/1], Step [324/3504], Loss: 0.4413\n",
      "Epoch [1/1], Step [325/3504], Loss: 0.4574\n",
      "Epoch [1/1], Step [326/3504], Loss: 0.4324\n",
      "Epoch [1/1], Step [327/3504], Loss: 0.5093\n",
      "Epoch [1/1], Step [328/3504], Loss: 0.7735\n",
      "Epoch [1/1], Step [329/3504], Loss: 0.4892\n",
      "Epoch [1/1], Step [330/3504], Loss: 0.5065\n",
      "Epoch [1/1], Step [331/3504], Loss: 0.4543\n",
      "Epoch [1/1], Step [332/3504], Loss: 0.6023\n",
      "Epoch [1/1], Step [333/3504], Loss: 0.4919\n",
      "Epoch [1/1], Step [334/3504], Loss: 0.4684\n",
      "Epoch [1/1], Step [335/3504], Loss: 0.4142\n",
      "Epoch [1/1], Step [336/3504], Loss: 0.4616\n",
      "Epoch [1/1], Step [337/3504], Loss: 0.4557\n",
      "Epoch [1/1], Step [338/3504], Loss: 0.3780\n",
      "Epoch [1/1], Step [339/3504], Loss: 0.5221\n",
      "Epoch [1/1], Step [340/3504], Loss: 0.4871\n",
      "Epoch [1/1], Step [341/3504], Loss: 0.5279\n",
      "Epoch [1/1], Step [342/3504], Loss: 0.4847\n",
      "Epoch [1/1], Step [343/3504], Loss: 0.3950\n",
      "Epoch [1/1], Step [344/3504], Loss: 0.4715\n",
      "Epoch [1/1], Step [345/3504], Loss: 0.5566\n",
      "Epoch [1/1], Step [346/3504], Loss: 0.4304\n",
      "Epoch [1/1], Step [347/3504], Loss: 0.4679\n",
      "Epoch [1/1], Step [348/3504], Loss: 0.5175\n",
      "Epoch [1/1], Step [349/3504], Loss: 0.5016\n",
      "Epoch [1/1], Step [350/3504], Loss: 0.4860\n",
      "Epoch [1/1], Step [351/3504], Loss: 0.5110\n",
      "Epoch [1/1], Step [352/3504], Loss: 0.3994\n",
      "Epoch [1/1], Step [353/3504], Loss: 0.4974\n",
      "Epoch [1/1], Step [354/3504], Loss: 0.5380\n",
      "Epoch [1/1], Step [355/3504], Loss: 0.3447\n",
      "Epoch [1/1], Step [356/3504], Loss: 0.4410\n",
      "Epoch [1/1], Step [357/3504], Loss: 0.4735\n",
      "Epoch [1/1], Step [358/3504], Loss: 0.5381\n",
      "Epoch [1/1], Step [359/3504], Loss: 0.4460\n",
      "Epoch [1/1], Step [360/3504], Loss: 0.4191\n",
      "Epoch [1/1], Step [361/3504], Loss: 0.9998\n",
      "Epoch [1/1], Step [362/3504], Loss: 0.4148\n",
      "Epoch [1/1], Step [363/3504], Loss: 0.4404\n",
      "Epoch [1/1], Step [364/3504], Loss: 1.8191\n",
      "Epoch [1/1], Step [365/3504], Loss: 0.4382\n",
      "Epoch [1/1], Step [366/3504], Loss: 0.4566\n",
      "Epoch [1/1], Step [367/3504], Loss: 0.4848\n",
      "Epoch [1/1], Step [368/3504], Loss: 0.4124\n",
      "Epoch [1/1], Step [369/3504], Loss: 0.4100\n",
      "Epoch [1/1], Step [370/3504], Loss: 0.4509\n",
      "Epoch [1/1], Step [371/3504], Loss: 0.3961\n",
      "Epoch [1/1], Step [372/3504], Loss: 0.4896\n",
      "Epoch [1/1], Step [373/3504], Loss: 0.5282\n",
      "Epoch [1/1], Step [374/3504], Loss: 0.4709\n",
      "Epoch [1/1], Step [375/3504], Loss: 0.4304\n",
      "Epoch [1/1], Step [376/3504], Loss: 0.5107\n",
      "Epoch [1/1], Step [377/3504], Loss: 0.4350\n",
      "Epoch [1/1], Step [378/3504], Loss: 0.4395\n",
      "Epoch [1/1], Step [379/3504], Loss: 0.4916\n",
      "Epoch [1/1], Step [380/3504], Loss: 0.4656\n",
      "Epoch [1/1], Step [381/3504], Loss: 0.4351\n",
      "Epoch [1/1], Step [382/3504], Loss: 0.4417\n",
      "Epoch [1/1], Step [383/3504], Loss: 0.5117\n",
      "Epoch [1/1], Step [384/3504], Loss: 0.3728\n",
      "Epoch [1/1], Step [385/3504], Loss: 0.4716\n",
      "Epoch [1/1], Step [386/3504], Loss: 0.4884\n",
      "Epoch [1/1], Step [387/3504], Loss: 0.4141\n",
      "Epoch [1/1], Step [388/3504], Loss: 0.5717\n",
      "Epoch [1/1], Step [389/3504], Loss: 0.4326\n",
      "Epoch [1/1], Step [390/3504], Loss: 0.4536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [391/3504], Loss: 0.5444\n",
      "Epoch [1/1], Step [392/3504], Loss: 0.4265\n",
      "Epoch [1/1], Step [393/3504], Loss: 0.4710\n",
      "Epoch [1/1], Step [394/3504], Loss: 0.5018\n",
      "Epoch [1/1], Step [395/3504], Loss: 0.5162\n",
      "Epoch [1/1], Step [396/3504], Loss: 0.4332\n",
      "Epoch [1/1], Step [397/3504], Loss: 0.5098\n",
      "Epoch [1/1], Step [398/3504], Loss: 0.4802\n",
      "Epoch [1/1], Step [399/3504], Loss: 0.4385\n",
      "Epoch [1/1], Step [400/3504], Loss: 0.3997\n",
      "Epoch [1/1], Step [401/3504], Loss: 0.4402\n",
      "Epoch [1/1], Step [402/3504], Loss: 0.5228\n",
      "Epoch [1/1], Step [403/3504], Loss: 0.4977\n",
      "Epoch [1/1], Step [404/3504], Loss: 0.4868\n",
      "Epoch [1/1], Step [405/3504], Loss: 0.4234\n",
      "Epoch [1/1], Step [406/3504], Loss: 0.4195\n",
      "Epoch [1/1], Step [407/3504], Loss: 0.4548\n",
      "Epoch [1/1], Step [408/3504], Loss: 0.3440\n",
      "Epoch [1/1], Step [409/3504], Loss: 0.5010\n",
      "Epoch [1/1], Step [410/3504], Loss: 0.4782\n",
      "Epoch [1/1], Step [411/3504], Loss: 0.3787\n",
      "Epoch [1/1], Step [412/3504], Loss: 0.3593\n",
      "Epoch [1/1], Step [413/3504], Loss: 0.4431\n",
      "Epoch [1/1], Step [414/3504], Loss: 0.3894\n",
      "Epoch [1/1], Step [415/3504], Loss: 0.4247\n",
      "Epoch [1/1], Step [416/3504], Loss: 0.3346\n",
      "Epoch [1/1], Step [417/3504], Loss: 0.4753\n",
      "Epoch [1/1], Step [418/3504], Loss: 0.3971\n",
      "Epoch [1/1], Step [419/3504], Loss: 0.4559\n",
      "Epoch [1/1], Step [420/3504], Loss: 0.3764\n",
      "Epoch [1/1], Step [421/3504], Loss: 0.4701\n",
      "Epoch [1/1], Step [422/3504], Loss: 0.5403\n",
      "Epoch [1/1], Step [423/3504], Loss: 0.4704\n",
      "Epoch [1/1], Step [424/3504], Loss: 0.4780\n",
      "Epoch [1/1], Step [425/3504], Loss: 0.4967\n",
      "Epoch [1/1], Step [426/3504], Loss: 0.4855\n",
      "Epoch [1/1], Step [427/3504], Loss: 0.5953\n",
      "Epoch [1/1], Step [428/3504], Loss: 0.4635\n",
      "Epoch [1/1], Step [429/3504], Loss: 0.4615\n",
      "Epoch [1/1], Step [430/3504], Loss: 0.4705\n",
      "Epoch [1/1], Step [431/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [432/3504], Loss: 0.4942\n",
      "Epoch [1/1], Step [433/3504], Loss: 0.4217\n",
      "Epoch [1/1], Step [434/3504], Loss: 0.4735\n",
      "Epoch [1/1], Step [435/3504], Loss: 0.5275\n",
      "Epoch [1/1], Step [436/3504], Loss: 0.4776\n",
      "Epoch [1/1], Step [437/3504], Loss: 0.3870\n",
      "Epoch [1/1], Step [438/3504], Loss: 0.6921\n",
      "Epoch [1/1], Step [439/3504], Loss: 0.7169\n",
      "Epoch [1/1], Step [440/3504], Loss: 0.4797\n",
      "Epoch [1/1], Step [441/3504], Loss: 0.4238\n",
      "Epoch [1/1], Step [442/3504], Loss: 0.4430\n",
      "Epoch [1/1], Step [443/3504], Loss: 0.6942\n",
      "Epoch [1/1], Step [444/3504], Loss: 0.3385\n",
      "Epoch [1/1], Step [445/3504], Loss: 0.3687\n",
      "Epoch [1/1], Step [446/3504], Loss: 0.4633\n",
      "Epoch [1/1], Step [447/3504], Loss: 0.4463\n",
      "Epoch [1/1], Step [448/3504], Loss: 0.3337\n",
      "Epoch [1/1], Step [449/3504], Loss: 0.4807\n",
      "Epoch [1/1], Step [450/3504], Loss: 0.4501\n",
      "Epoch [1/1], Step [451/3504], Loss: 0.4229\n",
      "Epoch [1/1], Step [452/3504], Loss: 0.4395\n",
      "Epoch [1/1], Step [453/3504], Loss: 0.3935\n",
      "Epoch [1/1], Step [454/3504], Loss: 0.4329\n",
      "Epoch [1/1], Step [455/3504], Loss: 0.3737\n",
      "Epoch [1/1], Step [456/3504], Loss: 0.6961\n",
      "Epoch [1/1], Step [457/3504], Loss: 0.3343\n",
      "Epoch [1/1], Step [458/3504], Loss: 0.5991\n",
      "Epoch [1/1], Step [459/3504], Loss: 0.5360\n",
      "Epoch [1/1], Step [460/3504], Loss: 0.4036\n",
      "Epoch [1/1], Step [461/3504], Loss: 0.5019\n",
      "Epoch [1/1], Step [462/3504], Loss: 0.6713\n",
      "Epoch [1/1], Step [463/3504], Loss: 0.4594\n",
      "Epoch [1/1], Step [464/3504], Loss: 0.3782\n",
      "Epoch [1/1], Step [465/3504], Loss: 0.5399\n",
      "Epoch [1/1], Step [466/3504], Loss: 0.8603\n",
      "Epoch [1/1], Step [467/3504], Loss: 0.4268\n",
      "Epoch [1/1], Step [468/3504], Loss: 0.4219\n",
      "Epoch [1/1], Step [469/3504], Loss: 0.4823\n",
      "Epoch [1/1], Step [470/3504], Loss: 0.4764\n",
      "Epoch [1/1], Step [471/3504], Loss: 0.4424\n",
      "Epoch [1/1], Step [472/3504], Loss: 0.3563\n",
      "Epoch [1/1], Step [473/3504], Loss: 0.3899\n",
      "Epoch [1/1], Step [474/3504], Loss: 0.3670\n",
      "Epoch [1/1], Step [475/3504], Loss: 0.4544\n",
      "Epoch [1/1], Step [476/3504], Loss: 0.4309\n",
      "Epoch [1/1], Step [477/3504], Loss: 0.5402\n",
      "Epoch [1/1], Step [478/3504], Loss: 0.4369\n",
      "Epoch [1/1], Step [479/3504], Loss: 0.5365\n",
      "Epoch [1/1], Step [480/3504], Loss: 0.4495\n",
      "Epoch [1/1], Step [481/3504], Loss: 0.4470\n",
      "Epoch [1/1], Step [482/3504], Loss: 0.4685\n",
      "Epoch [1/1], Step [483/3504], Loss: 0.4147\n",
      "Epoch [1/1], Step [484/3504], Loss: 0.4227\n",
      "Epoch [1/1], Step [485/3504], Loss: 0.4278\n",
      "Epoch [1/1], Step [486/3504], Loss: 0.5020\n",
      "Epoch [1/1], Step [487/3504], Loss: 0.6037\n",
      "Epoch [1/1], Step [488/3504], Loss: 0.5137\n",
      "Epoch [1/1], Step [489/3504], Loss: 0.3876\n",
      "Epoch [1/1], Step [490/3504], Loss: 0.6005\n",
      "Epoch [1/1], Step [491/3504], Loss: 0.4744\n",
      "Epoch [1/1], Step [492/3504], Loss: 0.4255\n",
      "Epoch [1/1], Step [493/3504], Loss: 0.4377\n",
      "Epoch [1/1], Step [494/3504], Loss: 0.4109\n",
      "Epoch [1/1], Step [495/3504], Loss: 0.4293\n",
      "Epoch [1/1], Step [496/3504], Loss: 0.3950\n",
      "Epoch [1/1], Step [497/3504], Loss: 0.4269\n",
      "Epoch [1/1], Step [498/3504], Loss: 0.4115\n",
      "Epoch [1/1], Step [499/3504], Loss: 0.3855\n",
      "Epoch [1/1], Step [500/3504], Loss: 0.3904\n",
      "Epoch [1/1], Step [501/3504], Loss: 0.4823\n",
      "Epoch [1/1], Step [502/3504], Loss: 0.4438\n",
      "Epoch [1/1], Step [503/3504], Loss: 0.4969\n",
      "Epoch [1/1], Step [504/3504], Loss: 0.4251\n",
      "Epoch [1/1], Step [505/3504], Loss: 0.3129\n",
      "Epoch [1/1], Step [506/3504], Loss: 0.3726\n",
      "Epoch [1/1], Step [507/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [508/3504], Loss: 0.4119\n",
      "Epoch [1/1], Step [509/3504], Loss: 0.5305\n",
      "Epoch [1/1], Step [510/3504], Loss: 0.4936\n",
      "Epoch [1/1], Step [511/3504], Loss: 0.4041\n",
      "Epoch [1/1], Step [512/3504], Loss: 0.4078\n",
      "Epoch [1/1], Step [513/3504], Loss: 0.4602\n",
      "Epoch [1/1], Step [514/3504], Loss: 0.4282\n",
      "Epoch [1/1], Step [515/3504], Loss: 0.4948\n",
      "Epoch [1/1], Step [516/3504], Loss: 0.4113\n",
      "Epoch [1/1], Step [517/3504], Loss: 0.3724\n",
      "Epoch [1/1], Step [518/3504], Loss: 0.4389\n",
      "Epoch [1/1], Step [519/3504], Loss: 0.3913\n",
      "Epoch [1/1], Step [520/3504], Loss: 0.3666\n",
      "Epoch [1/1], Step [521/3504], Loss: 0.4155\n",
      "Epoch [1/1], Step [522/3504], Loss: 0.4136\n",
      "Epoch [1/1], Step [523/3504], Loss: 0.4301\n",
      "Epoch [1/1], Step [524/3504], Loss: 0.4329\n",
      "Epoch [1/1], Step [525/3504], Loss: 0.5100\n",
      "Epoch [1/1], Step [526/3504], Loss: 0.4354\n",
      "Epoch [1/1], Step [527/3504], Loss: 0.3780\n",
      "Epoch [1/1], Step [528/3504], Loss: 0.4669\n",
      "Epoch [1/1], Step [529/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [530/3504], Loss: 0.4355\n",
      "Epoch [1/1], Step [531/3504], Loss: 0.3590\n",
      "Epoch [1/1], Step [532/3504], Loss: 0.4770\n",
      "Epoch [1/1], Step [533/3504], Loss: 0.4093\n",
      "Epoch [1/1], Step [534/3504], Loss: 0.4110\n",
      "Epoch [1/1], Step [535/3504], Loss: 0.3503\n",
      "Epoch [1/1], Step [536/3504], Loss: 0.4254\n",
      "Epoch [1/1], Step [537/3504], Loss: 0.3911\n",
      "Epoch [1/1], Step [538/3504], Loss: 0.4280\n",
      "Epoch [1/1], Step [539/3504], Loss: 0.4161\n",
      "Epoch [1/1], Step [540/3504], Loss: 0.5749\n",
      "Epoch [1/1], Step [541/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [542/3504], Loss: 0.3878\n",
      "Epoch [1/1], Step [543/3504], Loss: 0.4445\n",
      "Epoch [1/1], Step [544/3504], Loss: 0.4219\n",
      "Epoch [1/1], Step [545/3504], Loss: 0.4355\n",
      "Epoch [1/1], Step [546/3504], Loss: 0.5155\n",
      "Epoch [1/1], Step [547/3504], Loss: 0.8674\n",
      "Epoch [1/1], Step [548/3504], Loss: 0.4550\n",
      "Epoch [1/1], Step [549/3504], Loss: 0.4210\n",
      "Epoch [1/1], Step [550/3504], Loss: 0.4363\n",
      "Epoch [1/1], Step [551/3504], Loss: 0.4143\n",
      "Epoch [1/1], Step [552/3504], Loss: 0.9942\n",
      "Epoch [1/1], Step [553/3504], Loss: 0.3693\n",
      "Epoch [1/1], Step [554/3504], Loss: 0.3806\n",
      "Epoch [1/1], Step [555/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [556/3504], Loss: 0.4211\n",
      "Epoch [1/1], Step [557/3504], Loss: 0.4899\n",
      "Epoch [1/1], Step [558/3504], Loss: 0.3712\n",
      "Epoch [1/1], Step [559/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [560/3504], Loss: 0.3835\n",
      "Epoch [1/1], Step [561/3504], Loss: 0.3406\n",
      "Epoch [1/1], Step [562/3504], Loss: 0.3892\n",
      "Epoch [1/1], Step [563/3504], Loss: 0.4873\n",
      "Epoch [1/1], Step [564/3504], Loss: 0.4829\n",
      "Epoch [1/1], Step [565/3504], Loss: 0.4948\n",
      "Epoch [1/1], Step [566/3504], Loss: 0.3979\n",
      "Epoch [1/1], Step [567/3504], Loss: 0.4564\n",
      "Epoch [1/1], Step [568/3504], Loss: 0.4660\n",
      "Epoch [1/1], Step [569/3504], Loss: 0.4676\n",
      "Epoch [1/1], Step [570/3504], Loss: 0.3849\n",
      "Epoch [1/1], Step [571/3504], Loss: 0.4382\n",
      "Epoch [1/1], Step [572/3504], Loss: 0.4681\n",
      "Epoch [1/1], Step [573/3504], Loss: 0.4281\n",
      "Epoch [1/1], Step [574/3504], Loss: 0.3921\n",
      "Epoch [1/1], Step [575/3504], Loss: 0.4073\n",
      "Epoch [1/1], Step [576/3504], Loss: 0.4465\n",
      "Epoch [1/1], Step [577/3504], Loss: 0.3871\n",
      "Epoch [1/1], Step [578/3504], Loss: 0.3820\n",
      "Epoch [1/1], Step [579/3504], Loss: 0.4308\n",
      "Epoch [1/1], Step [580/3504], Loss: 0.4205\n",
      "Epoch [1/1], Step [581/3504], Loss: 0.4617\n",
      "Epoch [1/1], Step [582/3504], Loss: 0.4431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [583/3504], Loss: 0.5548\n",
      "Epoch [1/1], Step [584/3504], Loss: 0.4177\n",
      "Epoch [1/1], Step [585/3504], Loss: 0.4655\n",
      "Epoch [1/1], Step [586/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [587/3504], Loss: 0.3564\n",
      "Epoch [1/1], Step [588/3504], Loss: 0.5069\n",
      "Epoch [1/1], Step [589/3504], Loss: 0.4612\n",
      "Epoch [1/1], Step [590/3504], Loss: 0.3995\n",
      "Epoch [1/1], Step [591/3504], Loss: 0.3600\n",
      "Epoch [1/1], Step [592/3504], Loss: 0.4493\n",
      "Epoch [1/1], Step [593/3504], Loss: 0.4664\n",
      "Epoch [1/1], Step [594/3504], Loss: 0.4521\n",
      "Epoch [1/1], Step [595/3504], Loss: 0.4846\n",
      "Epoch [1/1], Step [596/3504], Loss: 0.4362\n",
      "Epoch [1/1], Step [597/3504], Loss: 0.4495\n",
      "Epoch [1/1], Step [598/3504], Loss: 0.4351\n",
      "Epoch [1/1], Step [599/3504], Loss: 0.4164\n",
      "Epoch [1/1], Step [600/3504], Loss: 0.4348\n",
      "Epoch [1/1], Step [601/3504], Loss: 1.2799\n",
      "Epoch [1/1], Step [602/3504], Loss: 0.4419\n",
      "Epoch [1/1], Step [603/3504], Loss: 0.4089\n",
      "Epoch [1/1], Step [604/3504], Loss: 0.3952\n",
      "Epoch [1/1], Step [605/3504], Loss: 0.4183\n",
      "Epoch [1/1], Step [606/3504], Loss: 0.3669\n",
      "Epoch [1/1], Step [607/3504], Loss: 0.3033\n",
      "Epoch [1/1], Step [608/3504], Loss: 0.3890\n",
      "Epoch [1/1], Step [609/3504], Loss: 0.4264\n",
      "Epoch [1/1], Step [610/3504], Loss: 0.4152\n",
      "Epoch [1/1], Step [611/3504], Loss: 0.4023\n",
      "Epoch [1/1], Step [612/3504], Loss: 0.4220\n",
      "Epoch [1/1], Step [613/3504], Loss: 0.3414\n",
      "Epoch [1/1], Step [614/3504], Loss: 0.4090\n",
      "Epoch [1/1], Step [615/3504], Loss: 0.4876\n",
      "Epoch [1/1], Step [616/3504], Loss: 0.4830\n",
      "Epoch [1/1], Step [617/3504], Loss: 0.4362\n",
      "Epoch [1/1], Step [618/3504], Loss: 0.5111\n",
      "Epoch [1/1], Step [619/3504], Loss: 0.3584\n",
      "Epoch [1/1], Step [620/3504], Loss: 0.4092\n",
      "Epoch [1/1], Step [621/3504], Loss: 0.4207\n",
      "Epoch [1/1], Step [622/3504], Loss: 0.4259\n",
      "Epoch [1/1], Step [623/3504], Loss: 0.3772\n",
      "Epoch [1/1], Step [624/3504], Loss: 0.4160\n",
      "Epoch [1/1], Step [625/3504], Loss: 0.4571\n",
      "Epoch [1/1], Step [626/3504], Loss: 0.3495\n",
      "Epoch [1/1], Step [627/3504], Loss: 0.3178\n",
      "Epoch [1/1], Step [628/3504], Loss: 0.4581\n",
      "Epoch [1/1], Step [629/3504], Loss: 0.3698\n",
      "Epoch [1/1], Step [630/3504], Loss: 0.4364\n",
      "Epoch [1/1], Step [631/3504], Loss: 0.3810\n",
      "Epoch [1/1], Step [632/3504], Loss: 0.4592\n",
      "Epoch [1/1], Step [633/3504], Loss: 0.5488\n",
      "Epoch [1/1], Step [634/3504], Loss: 0.4509\n",
      "Epoch [1/1], Step [635/3504], Loss: 0.4024\n",
      "Epoch [1/1], Step [636/3504], Loss: 0.4409\n",
      "Epoch [1/1], Step [637/3504], Loss: 0.3845\n",
      "Epoch [1/1], Step [638/3504], Loss: 0.5102\n",
      "Epoch [1/1], Step [639/3504], Loss: 0.5086\n",
      "Epoch [1/1], Step [640/3504], Loss: 0.3533\n",
      "Epoch [1/1], Step [641/3504], Loss: 0.4060\n",
      "Epoch [1/1], Step [642/3504], Loss: 0.5092\n",
      "Epoch [1/1], Step [643/3504], Loss: 0.4212\n",
      "Epoch [1/1], Step [644/3504], Loss: 0.4616\n",
      "Epoch [1/1], Step [645/3504], Loss: 0.5229\n",
      "Epoch [1/1], Step [646/3504], Loss: 0.7209\n",
      "Epoch [1/1], Step [647/3504], Loss: 0.4191\n",
      "Epoch [1/1], Step [648/3504], Loss: 0.3601\n",
      "Epoch [1/1], Step [649/3504], Loss: 0.3974\n",
      "Epoch [1/1], Step [650/3504], Loss: 0.4888\n",
      "Epoch [1/1], Step [651/3504], Loss: 0.4998\n",
      "Epoch [1/1], Step [652/3504], Loss: 0.4596\n",
      "Epoch [1/1], Step [653/3504], Loss: 0.4638\n",
      "Epoch [1/1], Step [654/3504], Loss: 0.4358\n",
      "Epoch [1/1], Step [655/3504], Loss: 0.6772\n",
      "Epoch [1/1], Step [656/3504], Loss: 0.4715\n",
      "Epoch [1/1], Step [657/3504], Loss: 0.4056\n",
      "Epoch [1/1], Step [658/3504], Loss: 0.3791\n",
      "Epoch [1/1], Step [659/3504], Loss: 0.4154\n",
      "Epoch [1/1], Step [660/3504], Loss: 0.4600\n",
      "Epoch [1/1], Step [661/3504], Loss: 0.5538\n",
      "Epoch [1/1], Step [662/3504], Loss: 0.3444\n",
      "Epoch [1/1], Step [663/3504], Loss: 0.4522\n",
      "Epoch [1/1], Step [664/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [665/3504], Loss: 0.3872\n",
      "Epoch [1/1], Step [666/3504], Loss: 0.4612\n",
      "Epoch [1/1], Step [667/3504], Loss: 0.3774\n",
      "Epoch [1/1], Step [668/3504], Loss: 0.4095\n",
      "Epoch [1/1], Step [669/3504], Loss: 0.3535\n",
      "Epoch [1/1], Step [670/3504], Loss: 0.3723\n",
      "Epoch [1/1], Step [671/3504], Loss: 0.3745\n",
      "Epoch [1/1], Step [672/3504], Loss: 0.3632\n",
      "Epoch [1/1], Step [673/3504], Loss: 0.4582\n",
      "Epoch [1/1], Step [674/3504], Loss: 0.3724\n",
      "Epoch [1/1], Step [675/3504], Loss: 0.3142\n",
      "Epoch [1/1], Step [676/3504], Loss: 0.3937\n",
      "Epoch [1/1], Step [677/3504], Loss: 0.5256\n",
      "Epoch [1/1], Step [678/3504], Loss: 0.3871\n",
      "Epoch [1/1], Step [679/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [680/3504], Loss: 0.4041\n",
      "Epoch [1/1], Step [681/3504], Loss: 0.4080\n",
      "Epoch [1/1], Step [682/3504], Loss: 0.4561\n",
      "Epoch [1/1], Step [683/3504], Loss: 0.5107\n",
      "Epoch [1/1], Step [684/3504], Loss: 0.4217\n",
      "Epoch [1/1], Step [685/3504], Loss: 0.3557\n",
      "Epoch [1/1], Step [686/3504], Loss: 0.4415\n",
      "Epoch [1/1], Step [687/3504], Loss: 0.5236\n",
      "Epoch [1/1], Step [688/3504], Loss: 0.3864\n",
      "Epoch [1/1], Step [689/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [690/3504], Loss: 0.8237\n",
      "Epoch [1/1], Step [691/3504], Loss: 0.5503\n",
      "Epoch [1/1], Step [692/3504], Loss: 0.3002\n",
      "Epoch [1/1], Step [693/3504], Loss: 0.4148\n",
      "Epoch [1/1], Step [694/3504], Loss: 0.3493\n",
      "Epoch [1/1], Step [695/3504], Loss: 0.5074\n",
      "Epoch [1/1], Step [696/3504], Loss: 0.7344\n",
      "Epoch [1/1], Step [697/3504], Loss: 0.3390\n",
      "Epoch [1/1], Step [698/3504], Loss: 0.4104\n",
      "Epoch [1/1], Step [699/3504], Loss: 0.5194\n",
      "Epoch [1/1], Step [700/3504], Loss: 0.4976\n",
      "Epoch [1/1], Step [701/3504], Loss: 0.4210\n",
      "Epoch [1/1], Step [702/3504], Loss: 0.7232\n",
      "Epoch [1/1], Step [703/3504], Loss: 0.4723\n",
      "Epoch [1/1], Step [704/3504], Loss: 0.3976\n",
      "Epoch [1/1], Step [705/3504], Loss: 0.3877\n",
      "Epoch [1/1], Step [706/3504], Loss: 0.3854\n",
      "Epoch [1/1], Step [707/3504], Loss: 0.4274\n",
      "Epoch [1/1], Step [708/3504], Loss: 0.3466\n",
      "Epoch [1/1], Step [709/3504], Loss: 0.3233\n",
      "Epoch [1/1], Step [710/3504], Loss: 0.2760\n",
      "Epoch [1/1], Step [711/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [712/3504], Loss: 0.3561\n",
      "Epoch [1/1], Step [713/3504], Loss: 0.3662\n",
      "Epoch [1/1], Step [714/3504], Loss: 0.4215\n",
      "Epoch [1/1], Step [715/3504], Loss: 0.4902\n",
      "Epoch [1/1], Step [716/3504], Loss: 0.3515\n",
      "Epoch [1/1], Step [717/3504], Loss: 0.5927\n",
      "Epoch [1/1], Step [718/3504], Loss: 0.3476\n",
      "Epoch [1/1], Step [719/3504], Loss: 0.4339\n",
      "Epoch [1/1], Step [720/3504], Loss: 0.4338\n",
      "Epoch [1/1], Step [721/3504], Loss: 0.4217\n",
      "Epoch [1/1], Step [722/3504], Loss: 0.5160\n",
      "Epoch [1/1], Step [723/3504], Loss: 0.4432\n",
      "Epoch [1/1], Step [724/3504], Loss: 0.4418\n",
      "Epoch [1/1], Step [725/3504], Loss: 0.2918\n",
      "Epoch [1/1], Step [726/3504], Loss: 0.4111\n",
      "Epoch [1/1], Step [727/3504], Loss: 0.4365\n",
      "Epoch [1/1], Step [728/3504], Loss: 0.3991\n",
      "Epoch [1/1], Step [729/3504], Loss: 0.4692\n",
      "Epoch [1/1], Step [730/3504], Loss: 0.4063\n",
      "Epoch [1/1], Step [731/3504], Loss: 0.3603\n",
      "Epoch [1/1], Step [732/3504], Loss: 0.4013\n",
      "Epoch [1/1], Step [733/3504], Loss: 0.3695\n",
      "Epoch [1/1], Step [734/3504], Loss: 0.4623\n",
      "Epoch [1/1], Step [735/3504], Loss: 0.3798\n",
      "Epoch [1/1], Step [736/3504], Loss: 0.4136\n",
      "Epoch [1/1], Step [737/3504], Loss: 0.4718\n",
      "Epoch [1/1], Step [738/3504], Loss: 0.3049\n",
      "Epoch [1/1], Step [739/3504], Loss: 0.4358\n",
      "Epoch [1/1], Step [740/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [741/3504], Loss: 1.0181\n",
      "Epoch [1/1], Step [742/3504], Loss: 0.3812\n",
      "Epoch [1/1], Step [743/3504], Loss: 0.4286\n",
      "Epoch [1/1], Step [744/3504], Loss: 0.4655\n",
      "Epoch [1/1], Step [745/3504], Loss: 0.4386\n",
      "Epoch [1/1], Step [746/3504], Loss: 0.3624\n",
      "Epoch [1/1], Step [747/3504], Loss: 0.3513\n",
      "Epoch [1/1], Step [748/3504], Loss: 0.4745\n",
      "Epoch [1/1], Step [749/3504], Loss: 0.4106\n",
      "Epoch [1/1], Step [750/3504], Loss: 0.4821\n",
      "Epoch [1/1], Step [751/3504], Loss: 0.4760\n",
      "Epoch [1/1], Step [752/3504], Loss: 0.3818\n",
      "Epoch [1/1], Step [753/3504], Loss: 0.3409\n",
      "Epoch [1/1], Step [754/3504], Loss: 0.3811\n",
      "Epoch [1/1], Step [755/3504], Loss: 0.6541\n",
      "Epoch [1/1], Step [756/3504], Loss: 0.5628\n",
      "Epoch [1/1], Step [757/3504], Loss: 0.4909\n",
      "Epoch [1/1], Step [758/3504], Loss: 0.4116\n",
      "Epoch [1/1], Step [759/3504], Loss: 0.5790\n",
      "Epoch [1/1], Step [760/3504], Loss: 0.3548\n",
      "Epoch [1/1], Step [761/3504], Loss: 0.4599\n",
      "Epoch [1/1], Step [762/3504], Loss: 0.3102\n",
      "Epoch [1/1], Step [763/3504], Loss: 0.4337\n",
      "Epoch [1/1], Step [764/3504], Loss: 0.5227\n",
      "Epoch [1/1], Step [765/3504], Loss: 0.3727\n",
      "Epoch [1/1], Step [766/3504], Loss: 0.4167\n",
      "Epoch [1/1], Step [767/3504], Loss: 0.4654\n",
      "Epoch [1/1], Step [768/3504], Loss: 0.4260\n",
      "Epoch [1/1], Step [769/3504], Loss: 0.4749\n",
      "Epoch [1/1], Step [770/3504], Loss: 0.3139\n",
      "Epoch [1/1], Step [771/3504], Loss: 0.3303\n",
      "Epoch [1/1], Step [772/3504], Loss: 0.4082\n",
      "Epoch [1/1], Step [773/3504], Loss: 0.3703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [774/3504], Loss: 0.3850\n",
      "Epoch [1/1], Step [775/3504], Loss: 0.4145\n",
      "Epoch [1/1], Step [776/3504], Loss: 0.4307\n",
      "Epoch [1/1], Step [777/3504], Loss: 0.3389\n",
      "Epoch [1/1], Step [778/3504], Loss: 0.3864\n",
      "Epoch [1/1], Step [779/3504], Loss: 0.3543\n",
      "Epoch [1/1], Step [780/3504], Loss: 0.5019\n",
      "Epoch [1/1], Step [781/3504], Loss: 0.3743\n",
      "Epoch [1/1], Step [782/3504], Loss: 0.5393\n",
      "Epoch [1/1], Step [783/3504], Loss: 0.2995\n",
      "Epoch [1/1], Step [784/3504], Loss: 0.3521\n",
      "Epoch [1/1], Step [785/3504], Loss: 0.4444\n",
      "Epoch [1/1], Step [786/3504], Loss: 0.4248\n",
      "Epoch [1/1], Step [787/3504], Loss: 0.4851\n",
      "Epoch [1/1], Step [788/3504], Loss: 0.3891\n",
      "Epoch [1/1], Step [789/3504], Loss: 0.4828\n",
      "Epoch [1/1], Step [790/3504], Loss: 0.3785\n",
      "Epoch [1/1], Step [791/3504], Loss: 0.3658\n",
      "Epoch [1/1], Step [792/3504], Loss: 0.4253\n",
      "Epoch [1/1], Step [793/3504], Loss: 0.4029\n",
      "Epoch [1/1], Step [794/3504], Loss: 0.3687\n",
      "Epoch [1/1], Step [795/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [796/3504], Loss: 0.4127\n",
      "Epoch [1/1], Step [797/3504], Loss: 0.3781\n",
      "Epoch [1/1], Step [798/3504], Loss: 0.4189\n",
      "Epoch [1/1], Step [799/3504], Loss: 0.3547\n",
      "Epoch [1/1], Step [800/3504], Loss: 0.4558\n",
      "Epoch [1/1], Step [801/3504], Loss: 0.4222\n",
      "Epoch [1/1], Step [802/3504], Loss: 0.4600\n",
      "Epoch [1/1], Step [803/3504], Loss: 0.3641\n",
      "Epoch [1/1], Step [804/3504], Loss: 0.4109\n",
      "Epoch [1/1], Step [805/3504], Loss: 0.4609\n",
      "Epoch [1/1], Step [806/3504], Loss: 0.4288\n",
      "Epoch [1/1], Step [807/3504], Loss: 0.3969\n",
      "Epoch [1/1], Step [808/3504], Loss: 0.4568\n",
      "Epoch [1/1], Step [809/3504], Loss: 0.4392\n",
      "Epoch [1/1], Step [810/3504], Loss: 0.3498\n",
      "Epoch [1/1], Step [811/3504], Loss: 0.4596\n",
      "Epoch [1/1], Step [812/3504], Loss: 0.4080\n",
      "Epoch [1/1], Step [813/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [814/3504], Loss: 0.4309\n",
      "Epoch [1/1], Step [815/3504], Loss: 0.4054\n",
      "Epoch [1/1], Step [816/3504], Loss: 0.3651\n",
      "Epoch [1/1], Step [817/3504], Loss: 0.4029\n",
      "Epoch [1/1], Step [818/3504], Loss: 0.4205\n",
      "Epoch [1/1], Step [819/3504], Loss: 0.3648\n",
      "Epoch [1/1], Step [820/3504], Loss: 0.4605\n",
      "Epoch [1/1], Step [821/3504], Loss: 0.3312\n",
      "Epoch [1/1], Step [822/3504], Loss: 0.3849\n",
      "Epoch [1/1], Step [823/3504], Loss: 0.3710\n",
      "Epoch [1/1], Step [824/3504], Loss: 0.3990\n",
      "Epoch [1/1], Step [825/3504], Loss: 0.3563\n",
      "Epoch [1/1], Step [826/3504], Loss: 0.4892\n",
      "Epoch [1/1], Step [827/3504], Loss: 0.4345\n",
      "Epoch [1/1], Step [828/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [829/3504], Loss: 0.4053\n",
      "Epoch [1/1], Step [830/3504], Loss: 0.4462\n",
      "Epoch [1/1], Step [831/3504], Loss: 0.4068\n",
      "Epoch [1/1], Step [832/3504], Loss: 0.3495\n",
      "Epoch [1/1], Step [833/3504], Loss: 0.3353\n",
      "Epoch [1/1], Step [834/3504], Loss: 0.4335\n",
      "Epoch [1/1], Step [835/3504], Loss: 0.5299\n",
      "Epoch [1/1], Step [836/3504], Loss: 0.4643\n",
      "Epoch [1/1], Step [837/3504], Loss: 0.5632\n",
      "Epoch [1/1], Step [838/3504], Loss: 0.3517\n",
      "Epoch [1/1], Step [839/3504], Loss: 0.4242\n",
      "Epoch [1/1], Step [840/3504], Loss: 0.4876\n",
      "Epoch [1/1], Step [841/3504], Loss: 0.3860\n",
      "Epoch [1/1], Step [842/3504], Loss: 0.3377\n",
      "Epoch [1/1], Step [843/3504], Loss: 0.3566\n",
      "Epoch [1/1], Step [844/3504], Loss: 0.3176\n",
      "Epoch [1/1], Step [845/3504], Loss: 0.3747\n",
      "Epoch [1/1], Step [846/3504], Loss: 0.4123\n",
      "Epoch [1/1], Step [847/3504], Loss: 0.3987\n",
      "Epoch [1/1], Step [848/3504], Loss: 0.3979\n",
      "Epoch [1/1], Step [849/3504], Loss: 0.3115\n",
      "Epoch [1/1], Step [850/3504], Loss: 0.5450\n",
      "Epoch [1/1], Step [851/3504], Loss: 0.4257\n",
      "Epoch [1/1], Step [852/3504], Loss: 0.3750\n",
      "Epoch [1/1], Step [853/3504], Loss: 0.4612\n",
      "Epoch [1/1], Step [854/3504], Loss: 0.4831\n",
      "Epoch [1/1], Step [855/3504], Loss: 0.4129\n",
      "Epoch [1/1], Step [856/3504], Loss: 0.3591\n",
      "Epoch [1/1], Step [857/3504], Loss: 0.3494\n",
      "Epoch [1/1], Step [858/3504], Loss: 0.3201\n",
      "Epoch [1/1], Step [859/3504], Loss: 0.4478\n",
      "Epoch [1/1], Step [860/3504], Loss: 0.4703\n",
      "Epoch [1/1], Step [861/3504], Loss: 0.3184\n",
      "Epoch [1/1], Step [862/3504], Loss: 0.3830\n",
      "Epoch [1/1], Step [863/3504], Loss: 0.4161\n",
      "Epoch [1/1], Step [864/3504], Loss: 0.4553\n",
      "Epoch [1/1], Step [865/3504], Loss: 0.4250\n",
      "Epoch [1/1], Step [866/3504], Loss: 0.4259\n",
      "Epoch [1/1], Step [867/3504], Loss: 0.4091\n",
      "Epoch [1/1], Step [868/3504], Loss: 0.4318\n",
      "Epoch [1/1], Step [869/3504], Loss: 0.4579\n",
      "Epoch [1/1], Step [870/3504], Loss: 0.4279\n",
      "Epoch [1/1], Step [871/3504], Loss: 0.4352\n",
      "Epoch [1/1], Step [872/3504], Loss: 0.3738\n",
      "Epoch [1/1], Step [873/3504], Loss: 0.3668\n",
      "Epoch [1/1], Step [874/3504], Loss: 0.3599\n",
      "Epoch [1/1], Step [875/3504], Loss: 0.3825\n",
      "Epoch [1/1], Step [876/3504], Loss: 0.4631\n",
      "Epoch [1/1], Step [877/3504], Loss: 0.4737\n",
      "Epoch [1/1], Step [878/3504], Loss: 0.3550\n",
      "Epoch [1/1], Step [879/3504], Loss: 0.4954\n",
      "Epoch [1/1], Step [880/3504], Loss: 0.3774\n",
      "Epoch [1/1], Step [881/3504], Loss: 0.4082\n",
      "Epoch [1/1], Step [882/3504], Loss: 0.4429\n",
      "Epoch [1/1], Step [883/3504], Loss: 0.3884\n",
      "Epoch [1/1], Step [884/3504], Loss: 0.3503\n",
      "Epoch [1/1], Step [885/3504], Loss: 0.3912\n",
      "Epoch [1/1], Step [886/3504], Loss: 0.4381\n",
      "Epoch [1/1], Step [887/3504], Loss: 0.3531\n",
      "Epoch [1/1], Step [888/3504], Loss: 0.4104\n",
      "Epoch [1/1], Step [889/3504], Loss: 0.5643\n",
      "Epoch [1/1], Step [890/3504], Loss: 0.3311\n",
      "Epoch [1/1], Step [891/3504], Loss: 0.3940\n",
      "Epoch [1/1], Step [892/3504], Loss: 0.4395\n",
      "Epoch [1/1], Step [893/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [894/3504], Loss: 0.3362\n",
      "Epoch [1/1], Step [895/3504], Loss: 0.5038\n",
      "Epoch [1/1], Step [896/3504], Loss: 0.4118\n",
      "Epoch [1/1], Step [897/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [898/3504], Loss: 0.4065\n",
      "Epoch [1/1], Step [899/3504], Loss: 0.3146\n",
      "Epoch [1/1], Step [900/3504], Loss: 0.3637\n",
      "Epoch [1/1], Step [901/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [902/3504], Loss: 0.3126\n",
      "Epoch [1/1], Step [903/3504], Loss: 0.4272\n",
      "Epoch [1/1], Step [904/3504], Loss: 0.3533\n",
      "Epoch [1/1], Step [905/3504], Loss: 0.4409\n",
      "Epoch [1/1], Step [906/3504], Loss: 0.4351\n",
      "Epoch [1/1], Step [907/3504], Loss: 0.4544\n",
      "Epoch [1/1], Step [908/3504], Loss: 0.3345\n",
      "Epoch [1/1], Step [909/3504], Loss: 0.3161\n",
      "Epoch [1/1], Step [910/3504], Loss: 0.3838\n",
      "Epoch [1/1], Step [911/3504], Loss: 0.5068\n",
      "Epoch [1/1], Step [912/3504], Loss: 0.3305\n",
      "Epoch [1/1], Step [913/3504], Loss: 0.4381\n",
      "Epoch [1/1], Step [914/3504], Loss: 0.4835\n",
      "Epoch [1/1], Step [915/3504], Loss: 0.4431\n",
      "Epoch [1/1], Step [916/3504], Loss: 0.3147\n",
      "Epoch [1/1], Step [917/3504], Loss: 0.5352\n",
      "Epoch [1/1], Step [918/3504], Loss: 0.4283\n",
      "Epoch [1/1], Step [919/3504], Loss: 0.3272\n",
      "Epoch [1/1], Step [920/3504], Loss: 0.4477\n",
      "Epoch [1/1], Step [921/3504], Loss: 0.4658\n",
      "Epoch [1/1], Step [922/3504], Loss: 0.3756\n",
      "Epoch [1/1], Step [923/3504], Loss: 0.4950\n",
      "Epoch [1/1], Step [924/3504], Loss: 0.3131\n",
      "Epoch [1/1], Step [925/3504], Loss: 0.3846\n",
      "Epoch [1/1], Step [926/3504], Loss: 0.4728\n",
      "Epoch [1/1], Step [927/3504], Loss: 0.3636\n",
      "Epoch [1/1], Step [928/3504], Loss: 0.3693\n",
      "Epoch [1/1], Step [929/3504], Loss: 0.4117\n",
      "Epoch [1/1], Step [930/3504], Loss: 0.5115\n",
      "Epoch [1/1], Step [931/3504], Loss: 0.4384\n",
      "Epoch [1/1], Step [932/3504], Loss: 0.3699\n",
      "Epoch [1/1], Step [933/3504], Loss: 0.2806\n",
      "Epoch [1/1], Step [934/3504], Loss: 0.4700\n",
      "Epoch [1/1], Step [935/3504], Loss: 0.7571\n",
      "Epoch [1/1], Step [936/3504], Loss: 0.5428\n",
      "Epoch [1/1], Step [937/3504], Loss: 0.3124\n",
      "Epoch [1/1], Step [938/3504], Loss: 0.4068\n",
      "Epoch [1/1], Step [939/3504], Loss: 0.5669\n",
      "Epoch [1/1], Step [940/3504], Loss: 0.4789\n",
      "Epoch [1/1], Step [941/3504], Loss: 0.4066\n",
      "Epoch [1/1], Step [942/3504], Loss: 0.3543\n",
      "Epoch [1/1], Step [943/3504], Loss: 0.3995\n",
      "Epoch [1/1], Step [944/3504], Loss: 0.3704\n",
      "Epoch [1/1], Step [945/3504], Loss: 0.4490\n",
      "Epoch [1/1], Step [946/3504], Loss: 0.4077\n",
      "Epoch [1/1], Step [947/3504], Loss: 0.3749\n",
      "Epoch [1/1], Step [948/3504], Loss: 0.4010\n",
      "Epoch [1/1], Step [949/3504], Loss: 0.3744\n",
      "Epoch [1/1], Step [950/3504], Loss: 0.3787\n",
      "Epoch [1/1], Step [951/3504], Loss: 0.4145\n",
      "Epoch [1/1], Step [952/3504], Loss: 0.3685\n",
      "Epoch [1/1], Step [953/3504], Loss: 0.4347\n",
      "Epoch [1/1], Step [954/3504], Loss: 0.3173\n",
      "Epoch [1/1], Step [955/3504], Loss: 0.4284\n",
      "Epoch [1/1], Step [956/3504], Loss: 0.4875\n",
      "Epoch [1/1], Step [957/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [958/3504], Loss: 0.3407\n",
      "Epoch [1/1], Step [959/3504], Loss: 0.4215\n",
      "Epoch [1/1], Step [960/3504], Loss: 0.4046\n",
      "Epoch [1/1], Step [961/3504], Loss: 0.3307\n",
      "Epoch [1/1], Step [962/3504], Loss: 0.3844\n",
      "Epoch [1/1], Step [963/3504], Loss: 0.3367\n",
      "Epoch [1/1], Step [964/3504], Loss: 0.3480\n",
      "Epoch [1/1], Step [965/3504], Loss: 0.3536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [966/3504], Loss: 0.2276\n",
      "Epoch [1/1], Step [967/3504], Loss: 0.4343\n",
      "Epoch [1/1], Step [968/3504], Loss: 0.4804\n",
      "Epoch [1/1], Step [969/3504], Loss: 0.4379\n",
      "Epoch [1/1], Step [970/3504], Loss: 0.3901\n",
      "Epoch [1/1], Step [971/3504], Loss: 0.3430\n",
      "Epoch [1/1], Step [972/3504], Loss: 0.3913\n",
      "Epoch [1/1], Step [973/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [974/3504], Loss: 0.4272\n",
      "Epoch [1/1], Step [975/3504], Loss: 0.3736\n",
      "Epoch [1/1], Step [976/3504], Loss: 0.3567\n",
      "Epoch [1/1], Step [977/3504], Loss: 0.3989\n",
      "Epoch [1/1], Step [978/3504], Loss: 0.3672\n",
      "Epoch [1/1], Step [979/3504], Loss: 0.3336\n",
      "Epoch [1/1], Step [980/3504], Loss: 0.3307\n",
      "Epoch [1/1], Step [981/3504], Loss: 0.4043\n",
      "Epoch [1/1], Step [982/3504], Loss: 0.3562\n",
      "Epoch [1/1], Step [983/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [984/3504], Loss: 0.3090\n",
      "Epoch [1/1], Step [985/3504], Loss: 0.3294\n",
      "Epoch [1/1], Step [986/3504], Loss: 0.3840\n",
      "Epoch [1/1], Step [987/3504], Loss: 0.4309\n",
      "Epoch [1/1], Step [988/3504], Loss: 0.3346\n",
      "Epoch [1/1], Step [989/3504], Loss: 0.3860\n",
      "Epoch [1/1], Step [990/3504], Loss: 0.3480\n",
      "Epoch [1/1], Step [991/3504], Loss: 0.4945\n",
      "Epoch [1/1], Step [992/3504], Loss: 0.3961\n",
      "Epoch [1/1], Step [993/3504], Loss: 0.3807\n",
      "Epoch [1/1], Step [994/3504], Loss: 0.3812\n",
      "Epoch [1/1], Step [995/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [996/3504], Loss: 0.3617\n",
      "Epoch [1/1], Step [997/3504], Loss: 0.3945\n",
      "Epoch [1/1], Step [998/3504], Loss: 0.4074\n",
      "Epoch [1/1], Step [999/3504], Loss: 0.4438\n",
      "Epoch [1/1], Step [1000/3504], Loss: 0.3272\n",
      "Epoch [1/1], Step [1001/3504], Loss: 0.3416\n",
      "Epoch [1/1], Step [1002/3504], Loss: 0.4979\n",
      "Epoch [1/1], Step [1003/3504], Loss: 0.3055\n",
      "Epoch [1/1], Step [1004/3504], Loss: 0.4783\n",
      "Epoch [1/1], Step [1005/3504], Loss: 0.3873\n",
      "Epoch [1/1], Step [1006/3504], Loss: 0.3440\n",
      "Epoch [1/1], Step [1007/3504], Loss: 0.3982\n",
      "Epoch [1/1], Step [1008/3504], Loss: 0.3046\n",
      "Epoch [1/1], Step [1009/3504], Loss: 0.3340\n",
      "Epoch [1/1], Step [1010/3504], Loss: 0.3637\n",
      "Epoch [1/1], Step [1011/3504], Loss: 0.3529\n",
      "Epoch [1/1], Step [1012/3504], Loss: 0.4016\n",
      "Epoch [1/1], Step [1013/3504], Loss: 0.5342\n",
      "Epoch [1/1], Step [1014/3504], Loss: 0.4000\n",
      "Epoch [1/1], Step [1015/3504], Loss: 0.3567\n",
      "Epoch [1/1], Step [1016/3504], Loss: 0.3901\n",
      "Epoch [1/1], Step [1017/3504], Loss: 0.3764\n",
      "Epoch [1/1], Step [1018/3504], Loss: 0.3470\n",
      "Epoch [1/1], Step [1019/3504], Loss: 0.3645\n",
      "Epoch [1/1], Step [1020/3504], Loss: 0.3995\n",
      "Epoch [1/1], Step [1021/3504], Loss: 0.4213\n",
      "Epoch [1/1], Step [1022/3504], Loss: 0.4120\n",
      "Epoch [1/1], Step [1023/3504], Loss: 0.4500\n",
      "Epoch [1/1], Step [1024/3504], Loss: 0.3302\n",
      "Epoch [1/1], Step [1025/3504], Loss: 0.3907\n",
      "Epoch [1/1], Step [1026/3504], Loss: 0.3940\n",
      "Epoch [1/1], Step [1027/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [1028/3504], Loss: 0.5353\n",
      "Epoch [1/1], Step [1029/3504], Loss: 0.3703\n",
      "Epoch [1/1], Step [1030/3504], Loss: 0.4786\n",
      "Epoch [1/1], Step [1031/3504], Loss: 0.3276\n",
      "Epoch [1/1], Step [1032/3504], Loss: 0.2925\n",
      "Epoch [1/1], Step [1033/3504], Loss: 0.4081\n",
      "Epoch [1/1], Step [1034/3504], Loss: 0.3312\n",
      "Epoch [1/1], Step [1035/3504], Loss: 0.3754\n",
      "Epoch [1/1], Step [1036/3504], Loss: 0.3722\n",
      "Epoch [1/1], Step [1037/3504], Loss: 0.4722\n",
      "Epoch [1/1], Step [1038/3504], Loss: 0.4003\n",
      "Epoch [1/1], Step [1039/3504], Loss: 0.3444\n",
      "Epoch [1/1], Step [1040/3504], Loss: 0.4004\n",
      "Epoch [1/1], Step [1041/3504], Loss: 0.3663\n",
      "Epoch [1/1], Step [1042/3504], Loss: 0.3613\n",
      "Epoch [1/1], Step [1043/3504], Loss: 0.3671\n",
      "Epoch [1/1], Step [1044/3504], Loss: 0.3915\n",
      "Epoch [1/1], Step [1045/3504], Loss: 0.4017\n",
      "Epoch [1/1], Step [1046/3504], Loss: 0.3421\n",
      "Epoch [1/1], Step [1047/3504], Loss: 0.4047\n",
      "Epoch [1/1], Step [1048/3504], Loss: 0.3717\n",
      "Epoch [1/1], Step [1049/3504], Loss: 0.4553\n",
      "Epoch [1/1], Step [1050/3504], Loss: 0.4048\n",
      "Epoch [1/1], Step [1051/3504], Loss: 0.4415\n",
      "Epoch [1/1], Step [1052/3504], Loss: 0.4354\n",
      "Epoch [1/1], Step [1053/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [1054/3504], Loss: 0.3453\n",
      "Epoch [1/1], Step [1055/3504], Loss: 0.4527\n",
      "Epoch [1/1], Step [1056/3504], Loss: 0.4225\n",
      "Epoch [1/1], Step [1057/3504], Loss: 0.3959\n",
      "Epoch [1/1], Step [1058/3504], Loss: 0.3892\n",
      "Epoch [1/1], Step [1059/3504], Loss: 1.2132\n",
      "Epoch [1/1], Step [1060/3504], Loss: 0.3568\n",
      "Epoch [1/1], Step [1061/3504], Loss: 0.3856\n",
      "Epoch [1/1], Step [1062/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [1063/3504], Loss: 0.3953\n",
      "Epoch [1/1], Step [1064/3504], Loss: 0.4152\n",
      "Epoch [1/1], Step [1065/3504], Loss: 0.3580\n",
      "Epoch [1/1], Step [1066/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [1067/3504], Loss: 0.4454\n",
      "Epoch [1/1], Step [1068/3504], Loss: 0.4556\n",
      "Epoch [1/1], Step [1069/3504], Loss: 0.4425\n",
      "Epoch [1/1], Step [1070/3504], Loss: 0.3650\n",
      "Epoch [1/1], Step [1071/3504], Loss: 0.3216\n",
      "Epoch [1/1], Step [1072/3504], Loss: 0.5109\n",
      "Epoch [1/1], Step [1073/3504], Loss: 0.4808\n",
      "Epoch [1/1], Step [1074/3504], Loss: 0.4111\n",
      "Epoch [1/1], Step [1075/3504], Loss: 0.3632\n",
      "Epoch [1/1], Step [1076/3504], Loss: 0.4917\n",
      "Epoch [1/1], Step [1077/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [1078/3504], Loss: 0.3561\n",
      "Epoch [1/1], Step [1079/3504], Loss: 0.4148\n",
      "Epoch [1/1], Step [1080/3504], Loss: 0.3694\n",
      "Epoch [1/1], Step [1081/3504], Loss: 0.3478\n",
      "Epoch [1/1], Step [1082/3504], Loss: 0.4048\n",
      "Epoch [1/1], Step [1083/3504], Loss: 0.3845\n",
      "Epoch [1/1], Step [1084/3504], Loss: 0.2828\n",
      "Epoch [1/1], Step [1085/3504], Loss: 0.4018\n",
      "Epoch [1/1], Step [1086/3504], Loss: 0.4138\n",
      "Epoch [1/1], Step [1087/3504], Loss: 0.4863\n",
      "Epoch [1/1], Step [1088/3504], Loss: 0.3733\n",
      "Epoch [1/1], Step [1089/3504], Loss: 0.3694\n",
      "Epoch [1/1], Step [1090/3504], Loss: 0.3879\n",
      "Epoch [1/1], Step [1091/3504], Loss: 0.4422\n",
      "Epoch [1/1], Step [1092/3504], Loss: 0.4410\n",
      "Epoch [1/1], Step [1093/3504], Loss: 0.4289\n",
      "Epoch [1/1], Step [1094/3504], Loss: 0.4109\n",
      "Epoch [1/1], Step [1095/3504], Loss: 0.3598\n",
      "Epoch [1/1], Step [1096/3504], Loss: 0.3779\n",
      "Epoch [1/1], Step [1097/3504], Loss: 0.3555\n",
      "Epoch [1/1], Step [1098/3504], Loss: 0.3114\n",
      "Epoch [1/1], Step [1099/3504], Loss: 0.3241\n",
      "Epoch [1/1], Step [1100/3504], Loss: 0.4254\n",
      "Epoch [1/1], Step [1101/3504], Loss: 0.3462\n",
      "Epoch [1/1], Step [1102/3504], Loss: 0.4588\n",
      "Epoch [1/1], Step [1103/3504], Loss: 0.4475\n",
      "Epoch [1/1], Step [1104/3504], Loss: 0.3451\n",
      "Epoch [1/1], Step [1105/3504], Loss: 0.3659\n",
      "Epoch [1/1], Step [1106/3504], Loss: 0.3649\n",
      "Epoch [1/1], Step [1107/3504], Loss: 0.4505\n",
      "Epoch [1/1], Step [1108/3504], Loss: 0.3797\n",
      "Epoch [1/1], Step [1109/3504], Loss: 0.3971\n",
      "Epoch [1/1], Step [1110/3504], Loss: 0.2999\n",
      "Epoch [1/1], Step [1111/3504], Loss: 0.3493\n",
      "Epoch [1/1], Step [1112/3504], Loss: 0.3112\n",
      "Epoch [1/1], Step [1113/3504], Loss: 0.3192\n",
      "Epoch [1/1], Step [1114/3504], Loss: 0.4589\n",
      "Epoch [1/1], Step [1115/3504], Loss: 0.3377\n",
      "Epoch [1/1], Step [1116/3504], Loss: 0.3969\n",
      "Epoch [1/1], Step [1117/3504], Loss: 0.3825\n",
      "Epoch [1/1], Step [1118/3504], Loss: 0.3409\n",
      "Epoch [1/1], Step [1119/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [1120/3504], Loss: 0.3971\n",
      "Epoch [1/1], Step [1121/3504], Loss: 0.4035\n",
      "Epoch [1/1], Step [1122/3504], Loss: 0.4232\n",
      "Epoch [1/1], Step [1123/3504], Loss: 0.3772\n",
      "Epoch [1/1], Step [1124/3504], Loss: 0.3488\n",
      "Epoch [1/1], Step [1125/3504], Loss: 0.2837\n",
      "Epoch [1/1], Step [1126/3504], Loss: 0.3637\n",
      "Epoch [1/1], Step [1127/3504], Loss: 0.4098\n",
      "Epoch [1/1], Step [1128/3504], Loss: 0.3464\n",
      "Epoch [1/1], Step [1129/3504], Loss: 0.3874\n",
      "Epoch [1/1], Step [1130/3504], Loss: 0.3781\n",
      "Epoch [1/1], Step [1131/3504], Loss: 0.3951\n",
      "Epoch [1/1], Step [1132/3504], Loss: 0.3917\n",
      "Epoch [1/1], Step [1133/3504], Loss: 0.3622\n",
      "Epoch [1/1], Step [1134/3504], Loss: 0.3964\n",
      "Epoch [1/1], Step [1135/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [1136/3504], Loss: 0.3502\n",
      "Epoch [1/1], Step [1137/3504], Loss: 0.3324\n",
      "Epoch [1/1], Step [1138/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [1139/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [1140/3504], Loss: 0.4392\n",
      "Epoch [1/1], Step [1141/3504], Loss: 0.5009\n",
      "Epoch [1/1], Step [1142/3504], Loss: 0.5084\n",
      "Epoch [1/1], Step [1143/3504], Loss: 0.2909\n",
      "Epoch [1/1], Step [1144/3504], Loss: 0.3111\n",
      "Epoch [1/1], Step [1145/3504], Loss: 0.3985\n",
      "Epoch [1/1], Step [1146/3504], Loss: 0.4237\n",
      "Epoch [1/1], Step [1147/3504], Loss: 0.3151\n",
      "Epoch [1/1], Step [1148/3504], Loss: 0.4761\n",
      "Epoch [1/1], Step [1149/3504], Loss: 0.4059\n",
      "Epoch [1/1], Step [1150/3504], Loss: 0.3727\n",
      "Epoch [1/1], Step [1151/3504], Loss: 0.3916\n",
      "Epoch [1/1], Step [1152/3504], Loss: 0.3297\n",
      "Epoch [1/1], Step [1153/3504], Loss: 0.4423\n",
      "Epoch [1/1], Step [1154/3504], Loss: 0.4980\n",
      "Epoch [1/1], Step [1155/3504], Loss: 0.4212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1156/3504], Loss: 0.5337\n",
      "Epoch [1/1], Step [1157/3504], Loss: 0.4195\n",
      "Epoch [1/1], Step [1158/3504], Loss: 0.4283\n",
      "Epoch [1/1], Step [1159/3504], Loss: 0.4573\n",
      "Epoch [1/1], Step [1160/3504], Loss: 0.4583\n",
      "Epoch [1/1], Step [1161/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [1162/3504], Loss: 0.3912\n",
      "Epoch [1/1], Step [1163/3504], Loss: 0.4821\n",
      "Epoch [1/1], Step [1164/3504], Loss: 0.3974\n",
      "Epoch [1/1], Step [1165/3504], Loss: 0.4185\n",
      "Epoch [1/1], Step [1166/3504], Loss: 0.3387\n",
      "Epoch [1/1], Step [1167/3504], Loss: 0.4597\n",
      "Epoch [1/1], Step [1168/3504], Loss: 0.4358\n",
      "Epoch [1/1], Step [1169/3504], Loss: 0.4292\n",
      "Epoch [1/1], Step [1170/3504], Loss: 0.3672\n",
      "Epoch [1/1], Step [1171/3504], Loss: 0.3897\n",
      "Epoch [1/1], Step [1172/3504], Loss: 0.4027\n",
      "Epoch [1/1], Step [1173/3504], Loss: 0.4192\n",
      "Epoch [1/1], Step [1174/3504], Loss: 0.3048\n",
      "Epoch [1/1], Step [1175/3504], Loss: 0.3775\n",
      "Epoch [1/1], Step [1176/3504], Loss: 0.3610\n",
      "Epoch [1/1], Step [1177/3504], Loss: 0.3744\n",
      "Epoch [1/1], Step [1178/3504], Loss: 0.3934\n",
      "Epoch [1/1], Step [1179/3504], Loss: 0.3441\n",
      "Epoch [1/1], Step [1180/3504], Loss: 0.3712\n",
      "Epoch [1/1], Step [1181/3504], Loss: 0.3524\n",
      "Epoch [1/1], Step [1182/3504], Loss: 0.3327\n",
      "Epoch [1/1], Step [1183/3504], Loss: 0.3048\n",
      "Epoch [1/1], Step [1184/3504], Loss: 0.3975\n",
      "Epoch [1/1], Step [1185/3504], Loss: 0.3666\n",
      "Epoch [1/1], Step [1186/3504], Loss: 0.3431\n",
      "Epoch [1/1], Step [1187/3504], Loss: 0.3864\n",
      "Epoch [1/1], Step [1188/3504], Loss: 0.4011\n",
      "Epoch [1/1], Step [1189/3504], Loss: 0.4325\n",
      "Epoch [1/1], Step [1190/3504], Loss: 0.3539\n",
      "Epoch [1/1], Step [1191/3504], Loss: 0.4243\n",
      "Epoch [1/1], Step [1192/3504], Loss: 0.3648\n",
      "Epoch [1/1], Step [1193/3504], Loss: 0.3886\n",
      "Epoch [1/1], Step [1194/3504], Loss: 0.4107\n",
      "Epoch [1/1], Step [1195/3504], Loss: 0.3278\n",
      "Epoch [1/1], Step [1196/3504], Loss: 0.3344\n",
      "Epoch [1/1], Step [1197/3504], Loss: 0.2889\n",
      "Epoch [1/1], Step [1198/3504], Loss: 0.4366\n",
      "Epoch [1/1], Step [1199/3504], Loss: 0.4308\n",
      "Epoch [1/1], Step [1200/3504], Loss: 0.4235\n",
      "Epoch [1/1], Step [1201/3504], Loss: 0.3873\n",
      "Epoch [1/1], Step [1202/3504], Loss: 0.2763\n",
      "Epoch [1/1], Step [1203/3504], Loss: 0.3673\n",
      "Epoch [1/1], Step [1204/3504], Loss: 0.3816\n",
      "Epoch [1/1], Step [1205/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [1206/3504], Loss: 0.5274\n",
      "Epoch [1/1], Step [1207/3504], Loss: 0.3572\n",
      "Epoch [1/1], Step [1208/3504], Loss: 0.3476\n",
      "Epoch [1/1], Step [1209/3504], Loss: 0.3410\n",
      "Epoch [1/1], Step [1210/3504], Loss: 0.3811\n",
      "Epoch [1/1], Step [1211/3504], Loss: 0.3327\n",
      "Epoch [1/1], Step [1212/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [1213/3504], Loss: 0.3474\n",
      "Epoch [1/1], Step [1214/3504], Loss: 0.3810\n",
      "Epoch [1/1], Step [1215/3504], Loss: 0.4032\n",
      "Epoch [1/1], Step [1216/3504], Loss: 0.3690\n",
      "Epoch [1/1], Step [1217/3504], Loss: 0.3155\n",
      "Epoch [1/1], Step [1218/3504], Loss: 0.4379\n",
      "Epoch [1/1], Step [1219/3504], Loss: 0.3684\n",
      "Epoch [1/1], Step [1220/3504], Loss: 0.3639\n",
      "Epoch [1/1], Step [1221/3504], Loss: 0.3557\n",
      "Epoch [1/1], Step [1222/3504], Loss: 0.4052\n",
      "Epoch [1/1], Step [1223/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [1224/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [1225/3504], Loss: 0.4189\n",
      "Epoch [1/1], Step [1226/3504], Loss: 0.4920\n",
      "Epoch [1/1], Step [1227/3504], Loss: 0.4051\n",
      "Epoch [1/1], Step [1228/3504], Loss: 0.3831\n",
      "Epoch [1/1], Step [1229/3504], Loss: 0.3165\n",
      "Epoch [1/1], Step [1230/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [1231/3504], Loss: 0.3615\n",
      "Epoch [1/1], Step [1232/3504], Loss: 0.4827\n",
      "Epoch [1/1], Step [1233/3504], Loss: 0.3255\n",
      "Epoch [1/1], Step [1234/3504], Loss: 0.4756\n",
      "Epoch [1/1], Step [1235/3504], Loss: 0.4093\n",
      "Epoch [1/1], Step [1236/3504], Loss: 0.3892\n",
      "Epoch [1/1], Step [1237/3504], Loss: 0.3811\n",
      "Epoch [1/1], Step [1238/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [1239/3504], Loss: 0.3453\n",
      "Epoch [1/1], Step [1240/3504], Loss: 0.3250\n",
      "Epoch [1/1], Step [1241/3504], Loss: 0.3689\n",
      "Epoch [1/1], Step [1242/3504], Loss: 0.3290\n",
      "Epoch [1/1], Step [1243/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [1244/3504], Loss: 0.3282\n",
      "Epoch [1/1], Step [1245/3504], Loss: 0.3258\n",
      "Epoch [1/1], Step [1246/3504], Loss: 0.5086\n",
      "Epoch [1/1], Step [1247/3504], Loss: 0.3442\n",
      "Epoch [1/1], Step [1248/3504], Loss: 0.4074\n",
      "Epoch [1/1], Step [1249/3504], Loss: 0.3177\n",
      "Epoch [1/1], Step [1250/3504], Loss: 0.2962\n",
      "Epoch [1/1], Step [1251/3504], Loss: 0.3646\n",
      "Epoch [1/1], Step [1252/3504], Loss: 0.3923\n",
      "Epoch [1/1], Step [1253/3504], Loss: 0.3201\n",
      "Epoch [1/1], Step [1254/3504], Loss: 0.4526\n",
      "Epoch [1/1], Step [1255/3504], Loss: 0.3099\n",
      "Epoch [1/1], Step [1256/3504], Loss: 0.4528\n",
      "Epoch [1/1], Step [1257/3504], Loss: 0.6393\n",
      "Epoch [1/1], Step [1258/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [1259/3504], Loss: 0.3637\n",
      "Epoch [1/1], Step [1260/3504], Loss: 0.2878\n",
      "Epoch [1/1], Step [1261/3504], Loss: 0.3266\n",
      "Epoch [1/1], Step [1262/3504], Loss: 0.4051\n",
      "Epoch [1/1], Step [1263/3504], Loss: 0.3051\n",
      "Epoch [1/1], Step [1264/3504], Loss: 0.3837\n",
      "Epoch [1/1], Step [1265/3504], Loss: 0.3848\n",
      "Epoch [1/1], Step [1266/3504], Loss: 0.3519\n",
      "Epoch [1/1], Step [1267/3504], Loss: 0.3563\n",
      "Epoch [1/1], Step [1268/3504], Loss: 0.4012\n",
      "Epoch [1/1], Step [1269/3504], Loss: 0.4071\n",
      "Epoch [1/1], Step [1270/3504], Loss: 0.3709\n",
      "Epoch [1/1], Step [1271/3504], Loss: 0.2909\n",
      "Epoch [1/1], Step [1272/3504], Loss: 0.3164\n",
      "Epoch [1/1], Step [1273/3504], Loss: 0.4036\n",
      "Epoch [1/1], Step [1274/3504], Loss: 0.3664\n",
      "Epoch [1/1], Step [1275/3504], Loss: 0.3812\n",
      "Epoch [1/1], Step [1276/3504], Loss: 0.3897\n",
      "Epoch [1/1], Step [1277/3504], Loss: 0.4259\n",
      "Epoch [1/1], Step [1278/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [1279/3504], Loss: 0.2998\n",
      "Epoch [1/1], Step [1280/3504], Loss: 0.4080\n",
      "Epoch [1/1], Step [1281/3504], Loss: 0.3407\n",
      "Epoch [1/1], Step [1282/3504], Loss: 0.4123\n",
      "Epoch [1/1], Step [1283/3504], Loss: 0.3322\n",
      "Epoch [1/1], Step [1284/3504], Loss: 0.4488\n",
      "Epoch [1/1], Step [1285/3504], Loss: 0.3562\n",
      "Epoch [1/1], Step [1286/3504], Loss: 0.3552\n",
      "Epoch [1/1], Step [1287/3504], Loss: 0.3814\n",
      "Epoch [1/1], Step [1288/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [1289/3504], Loss: 0.3258\n",
      "Epoch [1/1], Step [1290/3504], Loss: 0.4156\n",
      "Epoch [1/1], Step [1291/3504], Loss: 0.4594\n",
      "Epoch [1/1], Step [1292/3504], Loss: 0.3536\n",
      "Epoch [1/1], Step [1293/3504], Loss: 0.3925\n",
      "Epoch [1/1], Step [1294/3504], Loss: 0.3113\n",
      "Epoch [1/1], Step [1295/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [1296/3504], Loss: 0.3915\n",
      "Epoch [1/1], Step [1297/3504], Loss: 0.3631\n",
      "Epoch [1/1], Step [1298/3504], Loss: 0.4301\n",
      "Epoch [1/1], Step [1299/3504], Loss: 0.4008\n",
      "Epoch [1/1], Step [1300/3504], Loss: 0.4262\n",
      "Epoch [1/1], Step [1301/3504], Loss: 0.4021\n",
      "Epoch [1/1], Step [1302/3504], Loss: 0.4324\n",
      "Epoch [1/1], Step [1303/3504], Loss: 0.5002\n",
      "Epoch [1/1], Step [1304/3504], Loss: 0.3631\n",
      "Epoch [1/1], Step [1305/3504], Loss: 0.3761\n",
      "Epoch [1/1], Step [1306/3504], Loss: 0.3437\n",
      "Epoch [1/1], Step [1307/3504], Loss: 0.4135\n",
      "Epoch [1/1], Step [1308/3504], Loss: 0.3907\n",
      "Epoch [1/1], Step [1309/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [1310/3504], Loss: 0.3681\n",
      "Epoch [1/1], Step [1311/3504], Loss: 0.3375\n",
      "Epoch [1/1], Step [1312/3504], Loss: 0.3326\n",
      "Epoch [1/1], Step [1313/3504], Loss: 0.3877\n",
      "Epoch [1/1], Step [1314/3504], Loss: 0.3767\n",
      "Epoch [1/1], Step [1315/3504], Loss: 0.4154\n",
      "Epoch [1/1], Step [1316/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [1317/3504], Loss: 0.3754\n",
      "Epoch [1/1], Step [1318/3504], Loss: 0.3646\n",
      "Epoch [1/1], Step [1319/3504], Loss: 0.3781\n",
      "Epoch [1/1], Step [1320/3504], Loss: 0.4313\n",
      "Epoch [1/1], Step [1321/3504], Loss: 0.3299\n",
      "Epoch [1/1], Step [1322/3504], Loss: 0.3658\n",
      "Epoch [1/1], Step [1323/3504], Loss: 0.3914\n",
      "Epoch [1/1], Step [1324/3504], Loss: 0.3830\n",
      "Epoch [1/1], Step [1325/3504], Loss: 0.3703\n",
      "Epoch [1/1], Step [1326/3504], Loss: 0.4585\n",
      "Epoch [1/1], Step [1327/3504], Loss: 0.3736\n",
      "Epoch [1/1], Step [1328/3504], Loss: 0.3479\n",
      "Epoch [1/1], Step [1329/3504], Loss: 0.4318\n",
      "Epoch [1/1], Step [1330/3504], Loss: 0.3495\n",
      "Epoch [1/1], Step [1331/3504], Loss: 0.3889\n",
      "Epoch [1/1], Step [1332/3504], Loss: 0.3091\n",
      "Epoch [1/1], Step [1333/3504], Loss: 0.3778\n",
      "Epoch [1/1], Step [1334/3504], Loss: 0.4245\n",
      "Epoch [1/1], Step [1335/3504], Loss: 0.4778\n",
      "Epoch [1/1], Step [1336/3504], Loss: 0.3595\n",
      "Epoch [1/1], Step [1337/3504], Loss: 0.3080\n",
      "Epoch [1/1], Step [1338/3504], Loss: 0.4594\n",
      "Epoch [1/1], Step [1339/3504], Loss: 0.4381\n",
      "Epoch [1/1], Step [1340/3504], Loss: 0.3537\n",
      "Epoch [1/1], Step [1341/3504], Loss: 0.4452\n",
      "Epoch [1/1], Step [1342/3504], Loss: 0.3757\n",
      "Epoch [1/1], Step [1343/3504], Loss: 0.4380\n",
      "Epoch [1/1], Step [1344/3504], Loss: 0.4191\n",
      "Epoch [1/1], Step [1345/3504], Loss: 0.3542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1346/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [1347/3504], Loss: 0.3364\n",
      "Epoch [1/1], Step [1348/3504], Loss: 0.3584\n",
      "Epoch [1/1], Step [1349/3504], Loss: 0.4040\n",
      "Epoch [1/1], Step [1350/3504], Loss: 0.2816\n",
      "Epoch [1/1], Step [1351/3504], Loss: 0.4082\n",
      "Epoch [1/1], Step [1352/3504], Loss: 0.3340\n",
      "Epoch [1/1], Step [1353/3504], Loss: 0.4024\n",
      "Epoch [1/1], Step [1354/3504], Loss: 0.2821\n",
      "Epoch [1/1], Step [1355/3504], Loss: 0.3791\n",
      "Epoch [1/1], Step [1356/3504], Loss: 0.3284\n",
      "Epoch [1/1], Step [1357/3504], Loss: 0.3426\n",
      "Epoch [1/1], Step [1358/3504], Loss: 0.4043\n",
      "Epoch [1/1], Step [1359/3504], Loss: 0.4245\n",
      "Epoch [1/1], Step [1360/3504], Loss: 0.2850\n",
      "Epoch [1/1], Step [1361/3504], Loss: 0.4322\n",
      "Epoch [1/1], Step [1362/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [1363/3504], Loss: 0.3681\n",
      "Epoch [1/1], Step [1364/3504], Loss: 0.4146\n",
      "Epoch [1/1], Step [1365/3504], Loss: 0.4173\n",
      "Epoch [1/1], Step [1366/3504], Loss: 0.4502\n",
      "Epoch [1/1], Step [1367/3504], Loss: 0.4136\n",
      "Epoch [1/1], Step [1368/3504], Loss: 0.3255\n",
      "Epoch [1/1], Step [1369/3504], Loss: 0.3028\n",
      "Epoch [1/1], Step [1370/3504], Loss: 0.3671\n",
      "Epoch [1/1], Step [1371/3504], Loss: 0.2870\n",
      "Epoch [1/1], Step [1372/3504], Loss: 0.3685\n",
      "Epoch [1/1], Step [1373/3504], Loss: 0.4333\n",
      "Epoch [1/1], Step [1374/3504], Loss: 0.4638\n",
      "Epoch [1/1], Step [1375/3504], Loss: 0.3288\n",
      "Epoch [1/1], Step [1376/3504], Loss: 0.3163\n",
      "Epoch [1/1], Step [1377/3504], Loss: 0.4379\n",
      "Epoch [1/1], Step [1378/3504], Loss: 0.3744\n",
      "Epoch [1/1], Step [1379/3504], Loss: 0.3070\n",
      "Epoch [1/1], Step [1380/3504], Loss: 0.3480\n",
      "Epoch [1/1], Step [1381/3504], Loss: 0.4063\n",
      "Epoch [1/1], Step [1382/3504], Loss: 0.3552\n",
      "Epoch [1/1], Step [1383/3504], Loss: 0.4935\n",
      "Epoch [1/1], Step [1384/3504], Loss: 0.3078\n",
      "Epoch [1/1], Step [1385/3504], Loss: 0.3641\n",
      "Epoch [1/1], Step [1386/3504], Loss: 0.4250\n",
      "Epoch [1/1], Step [1387/3504], Loss: 0.3580\n",
      "Epoch [1/1], Step [1388/3504], Loss: 0.3641\n",
      "Epoch [1/1], Step [1389/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [1390/3504], Loss: 0.3264\n",
      "Epoch [1/1], Step [1391/3504], Loss: 0.4977\n",
      "Epoch [1/1], Step [1392/3504], Loss: 0.3965\n",
      "Epoch [1/1], Step [1393/3504], Loss: 0.3297\n",
      "Epoch [1/1], Step [1394/3504], Loss: 0.3471\n",
      "Epoch [1/1], Step [1395/3504], Loss: 0.3813\n",
      "Epoch [1/1], Step [1396/3504], Loss: 0.4414\n",
      "Epoch [1/1], Step [1397/3504], Loss: 0.3489\n",
      "Epoch [1/1], Step [1398/3504], Loss: 0.5236\n",
      "Epoch [1/1], Step [1399/3504], Loss: 0.4390\n",
      "Epoch [1/1], Step [1400/3504], Loss: 0.3186\n",
      "Epoch [1/1], Step [1401/3504], Loss: 0.3986\n",
      "Epoch [1/1], Step [1402/3504], Loss: 0.3191\n",
      "Epoch [1/1], Step [1403/3504], Loss: 0.3210\n",
      "Epoch [1/1], Step [1404/3504], Loss: 0.3958\n",
      "Epoch [1/1], Step [1405/3504], Loss: 0.3413\n",
      "Epoch [1/1], Step [1406/3504], Loss: 0.4527\n",
      "Epoch [1/1], Step [1407/3504], Loss: 0.3640\n",
      "Epoch [1/1], Step [1408/3504], Loss: 0.4667\n",
      "Epoch [1/1], Step [1409/3504], Loss: 0.3511\n",
      "Epoch [1/1], Step [1410/3504], Loss: 0.3393\n",
      "Epoch [1/1], Step [1411/3504], Loss: 0.4130\n",
      "Epoch [1/1], Step [1412/3504], Loss: 0.4436\n",
      "Epoch [1/1], Step [1413/3504], Loss: 0.3795\n",
      "Epoch [1/1], Step [1414/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [1415/3504], Loss: 0.4585\n",
      "Epoch [1/1], Step [1416/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [1417/3504], Loss: 0.4164\n",
      "Epoch [1/1], Step [1418/3504], Loss: 0.3679\n",
      "Epoch [1/1], Step [1419/3504], Loss: 0.4036\n",
      "Epoch [1/1], Step [1420/3504], Loss: 0.3827\n",
      "Epoch [1/1], Step [1421/3504], Loss: 0.4337\n",
      "Epoch [1/1], Step [1422/3504], Loss: 0.4097\n",
      "Epoch [1/1], Step [1423/3504], Loss: 0.3742\n",
      "Epoch [1/1], Step [1424/3504], Loss: 0.3502\n",
      "Epoch [1/1], Step [1425/3504], Loss: 0.3664\n",
      "Epoch [1/1], Step [1426/3504], Loss: 0.3793\n",
      "Epoch [1/1], Step [1427/3504], Loss: 0.3856\n",
      "Epoch [1/1], Step [1428/3504], Loss: 0.3570\n",
      "Epoch [1/1], Step [1429/3504], Loss: 0.4019\n",
      "Epoch [1/1], Step [1430/3504], Loss: 0.3351\n",
      "Epoch [1/1], Step [1431/3504], Loss: 0.3665\n",
      "Epoch [1/1], Step [1432/3504], Loss: 0.4177\n",
      "Epoch [1/1], Step [1433/3504], Loss: 0.4374\n",
      "Epoch [1/1], Step [1434/3504], Loss: 0.3397\n",
      "Epoch [1/1], Step [1435/3504], Loss: 0.3356\n",
      "Epoch [1/1], Step [1436/3504], Loss: 0.3196\n",
      "Epoch [1/1], Step [1437/3504], Loss: 0.3792\n",
      "Epoch [1/1], Step [1438/3504], Loss: 0.3848\n",
      "Epoch [1/1], Step [1439/3504], Loss: 0.3819\n",
      "Epoch [1/1], Step [1440/3504], Loss: 0.3381\n",
      "Epoch [1/1], Step [1441/3504], Loss: 0.4151\n",
      "Epoch [1/1], Step [1442/3504], Loss: 0.3600\n",
      "Epoch [1/1], Step [1443/3504], Loss: 0.3304\n",
      "Epoch [1/1], Step [1444/3504], Loss: 0.3865\n",
      "Epoch [1/1], Step [1445/3504], Loss: 0.4023\n",
      "Epoch [1/1], Step [1446/3504], Loss: 0.3988\n",
      "Epoch [1/1], Step [1447/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [1448/3504], Loss: 0.3269\n",
      "Epoch [1/1], Step [1449/3504], Loss: 0.3603\n",
      "Epoch [1/1], Step [1450/3504], Loss: 0.3350\n",
      "Epoch [1/1], Step [1451/3504], Loss: 0.2829\n",
      "Epoch [1/1], Step [1452/3504], Loss: 0.3795\n",
      "Epoch [1/1], Step [1453/3504], Loss: 0.4275\n",
      "Epoch [1/1], Step [1454/3504], Loss: 0.4229\n",
      "Epoch [1/1], Step [1455/3504], Loss: 0.3114\n",
      "Epoch [1/1], Step [1456/3504], Loss: 0.3813\n",
      "Epoch [1/1], Step [1457/3504], Loss: 0.2887\n",
      "Epoch [1/1], Step [1458/3504], Loss: 0.3197\n",
      "Epoch [1/1], Step [1459/3504], Loss: 0.3878\n",
      "Epoch [1/1], Step [1460/3504], Loss: 0.3856\n",
      "Epoch [1/1], Step [1461/3504], Loss: 0.2504\n",
      "Epoch [1/1], Step [1462/3504], Loss: 0.3851\n",
      "Epoch [1/1], Step [1463/3504], Loss: 0.4576\n",
      "Epoch [1/1], Step [1464/3504], Loss: 0.3406\n",
      "Epoch [1/1], Step [1465/3504], Loss: 0.4061\n",
      "Epoch [1/1], Step [1466/3504], Loss: 0.4003\n",
      "Epoch [1/1], Step [1467/3504], Loss: 0.4744\n",
      "Epoch [1/1], Step [1468/3504], Loss: 0.3589\n",
      "Epoch [1/1], Step [1469/3504], Loss: 0.3057\n",
      "Epoch [1/1], Step [1470/3504], Loss: 0.3984\n",
      "Epoch [1/1], Step [1471/3504], Loss: 0.3512\n",
      "Epoch [1/1], Step [1472/3504], Loss: 0.3850\n",
      "Epoch [1/1], Step [1473/3504], Loss: 0.4510\n",
      "Epoch [1/1], Step [1474/3504], Loss: 0.3957\n",
      "Epoch [1/1], Step [1475/3504], Loss: 0.4589\n",
      "Epoch [1/1], Step [1476/3504], Loss: 0.4106\n",
      "Epoch [1/1], Step [1477/3504], Loss: 0.3198\n",
      "Epoch [1/1], Step [1478/3504], Loss: 0.3990\n",
      "Epoch [1/1], Step [1479/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [1480/3504], Loss: 0.4245\n",
      "Epoch [1/1], Step [1481/3504], Loss: 0.3021\n",
      "Epoch [1/1], Step [1482/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [1483/3504], Loss: 0.3823\n",
      "Epoch [1/1], Step [1484/3504], Loss: 0.3807\n",
      "Epoch [1/1], Step [1485/3504], Loss: 0.3504\n",
      "Epoch [1/1], Step [1486/3504], Loss: 0.3811\n",
      "Epoch [1/1], Step [1487/3504], Loss: 0.3648\n",
      "Epoch [1/1], Step [1488/3504], Loss: 0.4216\n",
      "Epoch [1/1], Step [1489/3504], Loss: 0.3510\n",
      "Epoch [1/1], Step [1490/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [1491/3504], Loss: 0.3670\n",
      "Epoch [1/1], Step [1492/3504], Loss: 0.4037\n",
      "Epoch [1/1], Step [1493/3504], Loss: 0.3477\n",
      "Epoch [1/1], Step [1494/3504], Loss: 0.3259\n",
      "Epoch [1/1], Step [1495/3504], Loss: 0.4271\n",
      "Epoch [1/1], Step [1496/3504], Loss: 0.4253\n",
      "Epoch [1/1], Step [1497/3504], Loss: 0.4341\n",
      "Epoch [1/1], Step [1498/3504], Loss: 0.4649\n",
      "Epoch [1/1], Step [1499/3504], Loss: 0.3577\n",
      "Epoch [1/1], Step [1500/3504], Loss: 0.3349\n",
      "Epoch [1/1], Step [1501/3504], Loss: 0.3991\n",
      "Epoch [1/1], Step [1502/3504], Loss: 0.4000\n",
      "Epoch [1/1], Step [1503/3504], Loss: 0.3498\n",
      "Epoch [1/1], Step [1504/3504], Loss: 0.3119\n",
      "Epoch [1/1], Step [1505/3504], Loss: 0.3857\n",
      "Epoch [1/1], Step [1506/3504], Loss: 0.3717\n",
      "Epoch [1/1], Step [1507/3504], Loss: 0.4296\n",
      "Epoch [1/1], Step [1508/3504], Loss: 0.3717\n",
      "Epoch [1/1], Step [1509/3504], Loss: 0.2930\n",
      "Epoch [1/1], Step [1510/3504], Loss: 0.4581\n",
      "Epoch [1/1], Step [1511/3504], Loss: 0.4286\n",
      "Epoch [1/1], Step [1512/3504], Loss: 0.4567\n",
      "Epoch [1/1], Step [1513/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [1514/3504], Loss: 0.3870\n",
      "Epoch [1/1], Step [1515/3504], Loss: 0.3411\n",
      "Epoch [1/1], Step [1516/3504], Loss: 0.4081\n",
      "Epoch [1/1], Step [1517/3504], Loss: 0.4596\n",
      "Epoch [1/1], Step [1518/3504], Loss: 0.3668\n",
      "Epoch [1/1], Step [1519/3504], Loss: 0.3758\n",
      "Epoch [1/1], Step [1520/3504], Loss: 0.3940\n",
      "Epoch [1/1], Step [1521/3504], Loss: 0.3554\n",
      "Epoch [1/1], Step [1522/3504], Loss: 0.3432\n",
      "Epoch [1/1], Step [1523/3504], Loss: 0.3435\n",
      "Epoch [1/1], Step [1524/3504], Loss: 0.2971\n",
      "Epoch [1/1], Step [1525/3504], Loss: 0.4329\n",
      "Epoch [1/1], Step [1526/3504], Loss: 0.2924\n",
      "Epoch [1/1], Step [1527/3504], Loss: 0.4691\n",
      "Epoch [1/1], Step [1528/3504], Loss: 0.4074\n",
      "Epoch [1/1], Step [1529/3504], Loss: 0.3606\n",
      "Epoch [1/1], Step [1530/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [1531/3504], Loss: 0.3280\n",
      "Epoch [1/1], Step [1532/3504], Loss: 0.4192\n",
      "Epoch [1/1], Step [1533/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [1534/3504], Loss: 0.4347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1535/3504], Loss: 0.4055\n",
      "Epoch [1/1], Step [1536/3504], Loss: 0.3629\n",
      "Epoch [1/1], Step [1537/3504], Loss: 0.4158\n",
      "Epoch [1/1], Step [1538/3504], Loss: 0.3485\n",
      "Epoch [1/1], Step [1539/3504], Loss: 0.3459\n",
      "Epoch [1/1], Step [1540/3504], Loss: 0.3993\n",
      "Epoch [1/1], Step [1541/3504], Loss: 0.4641\n",
      "Epoch [1/1], Step [1542/3504], Loss: 0.3330\n",
      "Epoch [1/1], Step [1543/3504], Loss: 0.3749\n",
      "Epoch [1/1], Step [1544/3504], Loss: 0.3873\n",
      "Epoch [1/1], Step [1545/3504], Loss: 0.3618\n",
      "Epoch [1/1], Step [1546/3504], Loss: 0.3214\n",
      "Epoch [1/1], Step [1547/3504], Loss: 0.3489\n",
      "Epoch [1/1], Step [1548/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [1549/3504], Loss: 0.3204\n",
      "Epoch [1/1], Step [1550/3504], Loss: 0.4265\n",
      "Epoch [1/1], Step [1551/3504], Loss: 0.4145\n",
      "Epoch [1/1], Step [1552/3504], Loss: 0.3679\n",
      "Epoch [1/1], Step [1553/3504], Loss: 0.2861\n",
      "Epoch [1/1], Step [1554/3504], Loss: 0.4202\n",
      "Epoch [1/1], Step [1555/3504], Loss: 0.3995\n",
      "Epoch [1/1], Step [1556/3504], Loss: 0.2896\n",
      "Epoch [1/1], Step [1557/3504], Loss: 0.4322\n",
      "Epoch [1/1], Step [1558/3504], Loss: 0.3226\n",
      "Epoch [1/1], Step [1559/3504], Loss: 0.3189\n",
      "Epoch [1/1], Step [1560/3504], Loss: 0.3897\n",
      "Epoch [1/1], Step [1561/3504], Loss: 0.3471\n",
      "Epoch [1/1], Step [1562/3504], Loss: 0.3483\n",
      "Epoch [1/1], Step [1563/3504], Loss: 0.4006\n",
      "Epoch [1/1], Step [1564/3504], Loss: 0.4306\n",
      "Epoch [1/1], Step [1565/3504], Loss: 0.4280\n",
      "Epoch [1/1], Step [1566/3504], Loss: 0.4362\n",
      "Epoch [1/1], Step [1567/3504], Loss: 0.3866\n",
      "Epoch [1/1], Step [1568/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [1569/3504], Loss: 0.3477\n",
      "Epoch [1/1], Step [1570/3504], Loss: 0.2849\n",
      "Epoch [1/1], Step [1571/3504], Loss: 0.3359\n",
      "Epoch [1/1], Step [1572/3504], Loss: 0.4321\n",
      "Epoch [1/1], Step [1573/3504], Loss: 0.2789\n",
      "Epoch [1/1], Step [1574/3504], Loss: 0.2610\n",
      "Epoch [1/1], Step [1575/3504], Loss: 0.3842\n",
      "Epoch [1/1], Step [1576/3504], Loss: 0.4787\n",
      "Epoch [1/1], Step [1577/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [1578/3504], Loss: 0.4374\n",
      "Epoch [1/1], Step [1579/3504], Loss: 0.3492\n",
      "Epoch [1/1], Step [1580/3504], Loss: 0.4296\n",
      "Epoch [1/1], Step [1581/3504], Loss: 0.4105\n",
      "Epoch [1/1], Step [1582/3504], Loss: 0.3194\n",
      "Epoch [1/1], Step [1583/3504], Loss: 0.3640\n",
      "Epoch [1/1], Step [1584/3504], Loss: 0.3117\n",
      "Epoch [1/1], Step [1585/3504], Loss: 0.4038\n",
      "Epoch [1/1], Step [1586/3504], Loss: 0.3480\n",
      "Epoch [1/1], Step [1587/3504], Loss: 0.3598\n",
      "Epoch [1/1], Step [1588/3504], Loss: 0.3400\n",
      "Epoch [1/1], Step [1589/3504], Loss: 0.4428\n",
      "Epoch [1/1], Step [1590/3504], Loss: 0.3919\n",
      "Epoch [1/1], Step [1591/3504], Loss: 0.3367\n",
      "Epoch [1/1], Step [1592/3504], Loss: 0.3828\n",
      "Epoch [1/1], Step [1593/3504], Loss: 0.3188\n",
      "Epoch [1/1], Step [1594/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [1595/3504], Loss: 0.2634\n",
      "Epoch [1/1], Step [1596/3504], Loss: 0.3562\n",
      "Epoch [1/1], Step [1597/3504], Loss: 0.4047\n",
      "Epoch [1/1], Step [1598/3504], Loss: 0.3220\n",
      "Epoch [1/1], Step [1599/3504], Loss: 0.3675\n",
      "Epoch [1/1], Step [1600/3504], Loss: 0.5162\n",
      "Epoch [1/1], Step [1601/3504], Loss: 0.3970\n",
      "Epoch [1/1], Step [1602/3504], Loss: 0.3565\n",
      "Epoch [1/1], Step [1603/3504], Loss: 0.3922\n",
      "Epoch [1/1], Step [1604/3504], Loss: 0.4424\n",
      "Epoch [1/1], Step [1605/3504], Loss: 0.5037\n",
      "Epoch [1/1], Step [1606/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [1607/3504], Loss: 0.3873\n",
      "Epoch [1/1], Step [1608/3504], Loss: 0.3664\n",
      "Epoch [1/1], Step [1609/3504], Loss: 0.3414\n",
      "Epoch [1/1], Step [1610/3504], Loss: 0.4110\n",
      "Epoch [1/1], Step [1611/3504], Loss: 0.3871\n",
      "Epoch [1/1], Step [1612/3504], Loss: 0.3739\n",
      "Epoch [1/1], Step [1613/3504], Loss: 0.2995\n",
      "Epoch [1/1], Step [1614/3504], Loss: 0.4463\n",
      "Epoch [1/1], Step [1615/3504], Loss: 0.4320\n",
      "Epoch [1/1], Step [1616/3504], Loss: 0.3630\n",
      "Epoch [1/1], Step [1617/3504], Loss: 0.3684\n",
      "Epoch [1/1], Step [1618/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [1619/3504], Loss: 0.3609\n",
      "Epoch [1/1], Step [1620/3504], Loss: 0.3462\n",
      "Epoch [1/1], Step [1621/3504], Loss: 0.3213\n",
      "Epoch [1/1], Step [1622/3504], Loss: 0.3693\n",
      "Epoch [1/1], Step [1623/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [1624/3504], Loss: 0.3521\n",
      "Epoch [1/1], Step [1625/3504], Loss: 0.4716\n",
      "Epoch [1/1], Step [1626/3504], Loss: 0.3729\n",
      "Epoch [1/1], Step [1627/3504], Loss: 0.4240\n",
      "Epoch [1/1], Step [1628/3504], Loss: 0.3094\n",
      "Epoch [1/1], Step [1629/3504], Loss: 0.2821\n",
      "Epoch [1/1], Step [1630/3504], Loss: 0.2906\n",
      "Epoch [1/1], Step [1631/3504], Loss: 0.3778\n",
      "Epoch [1/1], Step [1632/3504], Loss: 0.3912\n",
      "Epoch [1/1], Step [1633/3504], Loss: 0.3106\n",
      "Epoch [1/1], Step [1634/3504], Loss: 0.3554\n",
      "Epoch [1/1], Step [1635/3504], Loss: 0.2874\n",
      "Epoch [1/1], Step [1636/3504], Loss: 0.4081\n",
      "Epoch [1/1], Step [1637/3504], Loss: 0.3701\n",
      "Epoch [1/1], Step [1638/3504], Loss: 0.3217\n",
      "Epoch [1/1], Step [1639/3504], Loss: 0.4760\n",
      "Epoch [1/1], Step [1640/3504], Loss: 0.3329\n",
      "Epoch [1/1], Step [1641/3504], Loss: 0.4062\n",
      "Epoch [1/1], Step [1642/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [1643/3504], Loss: 0.4373\n",
      "Epoch [1/1], Step [1644/3504], Loss: 0.2570\n",
      "Epoch [1/1], Step [1645/3504], Loss: 0.4092\n",
      "Epoch [1/1], Step [1646/3504], Loss: 0.3363\n",
      "Epoch [1/1], Step [1647/3504], Loss: 0.4729\n",
      "Epoch [1/1], Step [1648/3504], Loss: 0.3259\n",
      "Epoch [1/1], Step [1649/3504], Loss: 0.3103\n",
      "Epoch [1/1], Step [1650/3504], Loss: 0.4363\n",
      "Epoch [1/1], Step [1651/3504], Loss: 0.2681\n",
      "Epoch [1/1], Step [1652/3504], Loss: 0.3441\n",
      "Epoch [1/1], Step [1653/3504], Loss: 0.3322\n",
      "Epoch [1/1], Step [1654/3504], Loss: 0.4210\n",
      "Epoch [1/1], Step [1655/3504], Loss: 0.3842\n",
      "Epoch [1/1], Step [1656/3504], Loss: 0.4324\n",
      "Epoch [1/1], Step [1657/3504], Loss: 0.4054\n",
      "Epoch [1/1], Step [1658/3504], Loss: 0.3111\n",
      "Epoch [1/1], Step [1659/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [1660/3504], Loss: 0.2674\n",
      "Epoch [1/1], Step [1661/3504], Loss: 0.3359\n",
      "Epoch [1/1], Step [1662/3504], Loss: 0.4620\n",
      "Epoch [1/1], Step [1663/3504], Loss: 0.4283\n",
      "Epoch [1/1], Step [1664/3504], Loss: 0.4176\n",
      "Epoch [1/1], Step [1665/3504], Loss: 0.3780\n",
      "Epoch [1/1], Step [1666/3504], Loss: 0.3014\n",
      "Epoch [1/1], Step [1667/3504], Loss: 0.3964\n",
      "Epoch [1/1], Step [1668/3504], Loss: 0.4167\n",
      "Epoch [1/1], Step [1669/3504], Loss: 0.3392\n",
      "Epoch [1/1], Step [1670/3504], Loss: 0.4554\n",
      "Epoch [1/1], Step [1671/3504], Loss: 0.4703\n",
      "Epoch [1/1], Step [1672/3504], Loss: 0.4039\n",
      "Epoch [1/1], Step [1673/3504], Loss: 0.3091\n",
      "Epoch [1/1], Step [1674/3504], Loss: 0.3994\n",
      "Epoch [1/1], Step [1675/3504], Loss: 0.4024\n",
      "Epoch [1/1], Step [1676/3504], Loss: 0.4147\n",
      "Epoch [1/1], Step [1677/3504], Loss: 0.4764\n",
      "Epoch [1/1], Step [1678/3504], Loss: 0.4553\n",
      "Epoch [1/1], Step [1679/3504], Loss: 0.3722\n",
      "Epoch [1/1], Step [1680/3504], Loss: 0.4326\n",
      "Epoch [1/1], Step [1681/3504], Loss: 0.3877\n",
      "Epoch [1/1], Step [1682/3504], Loss: 0.4592\n",
      "Epoch [1/1], Step [1683/3504], Loss: 0.4712\n",
      "Epoch [1/1], Step [1684/3504], Loss: 0.3281\n",
      "Epoch [1/1], Step [1685/3504], Loss: 0.3990\n",
      "Epoch [1/1], Step [1686/3504], Loss: 0.3766\n",
      "Epoch [1/1], Step [1687/3504], Loss: 0.4062\n",
      "Epoch [1/1], Step [1688/3504], Loss: 0.4277\n",
      "Epoch [1/1], Step [1689/3504], Loss: 0.4532\n",
      "Epoch [1/1], Step [1690/3504], Loss: 0.4255\n",
      "Epoch [1/1], Step [1691/3504], Loss: 0.4129\n",
      "Epoch [1/1], Step [1692/3504], Loss: 0.4400\n",
      "Epoch [1/1], Step [1693/3504], Loss: 0.3794\n",
      "Epoch [1/1], Step [1694/3504], Loss: 0.3206\n",
      "Epoch [1/1], Step [1695/3504], Loss: 0.4315\n",
      "Epoch [1/1], Step [1696/3504], Loss: 0.3334\n",
      "Epoch [1/1], Step [1697/3504], Loss: 0.3541\n",
      "Epoch [1/1], Step [1698/3504], Loss: 0.3415\n",
      "Epoch [1/1], Step [1699/3504], Loss: 0.4196\n",
      "Epoch [1/1], Step [1700/3504], Loss: 0.4087\n",
      "Epoch [1/1], Step [1701/3504], Loss: 0.4377\n",
      "Epoch [1/1], Step [1702/3504], Loss: 0.4175\n",
      "Epoch [1/1], Step [1703/3504], Loss: 0.3628\n",
      "Epoch [1/1], Step [1704/3504], Loss: 0.3781\n",
      "Epoch [1/1], Step [1705/3504], Loss: 0.2916\n",
      "Epoch [1/1], Step [1706/3504], Loss: 0.3385\n",
      "Epoch [1/1], Step [1707/3504], Loss: 0.4878\n",
      "Epoch [1/1], Step [1708/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [1709/3504], Loss: 0.4489\n",
      "Epoch [1/1], Step [1710/3504], Loss: 0.4773\n",
      "Epoch [1/1], Step [1711/3504], Loss: 0.4135\n",
      "Epoch [1/1], Step [1712/3504], Loss: 0.3848\n",
      "Epoch [1/1], Step [1713/3504], Loss: 0.4382\n",
      "Epoch [1/1], Step [1714/3504], Loss: 0.4358\n",
      "Epoch [1/1], Step [1715/3504], Loss: 0.3225\n",
      "Epoch [1/1], Step [1716/3504], Loss: 0.4723\n",
      "Epoch [1/1], Step [1717/3504], Loss: 0.3359\n",
      "Epoch [1/1], Step [1718/3504], Loss: 0.4302\n",
      "Epoch [1/1], Step [1719/3504], Loss: 0.3574\n",
      "Epoch [1/1], Step [1720/3504], Loss: 0.3965\n",
      "Epoch [1/1], Step [1721/3504], Loss: 0.3935\n",
      "Epoch [1/1], Step [1722/3504], Loss: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1723/3504], Loss: 0.3627\n",
      "Epoch [1/1], Step [1724/3504], Loss: 0.4958\n",
      "Epoch [1/1], Step [1725/3504], Loss: 0.3963\n",
      "Epoch [1/1], Step [1726/3504], Loss: 0.4360\n",
      "Epoch [1/1], Step [1727/3504], Loss: 0.3162\n",
      "Epoch [1/1], Step [1728/3504], Loss: 0.3201\n",
      "Epoch [1/1], Step [1729/3504], Loss: 0.3848\n",
      "Epoch [1/1], Step [1730/3504], Loss: 0.3760\n",
      "Epoch [1/1], Step [1731/3504], Loss: 0.3203\n",
      "Epoch [1/1], Step [1732/3504], Loss: 0.3737\n",
      "Epoch [1/1], Step [1733/3504], Loss: 0.4111\n",
      "Epoch [1/1], Step [1734/3504], Loss: 0.3590\n",
      "Epoch [1/1], Step [1735/3504], Loss: 0.3300\n",
      "Epoch [1/1], Step [1736/3504], Loss: 0.3146\n",
      "Epoch [1/1], Step [1737/3504], Loss: 0.3923\n",
      "Epoch [1/1], Step [1738/3504], Loss: 0.3769\n",
      "Epoch [1/1], Step [1739/3504], Loss: 0.3139\n",
      "Epoch [1/1], Step [1740/3504], Loss: 0.4104\n",
      "Epoch [1/1], Step [1741/3504], Loss: 0.4140\n",
      "Epoch [1/1], Step [1742/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [1743/3504], Loss: 0.3520\n",
      "Epoch [1/1], Step [1744/3504], Loss: 0.3846\n",
      "Epoch [1/1], Step [1745/3504], Loss: 0.3055\n",
      "Epoch [1/1], Step [1746/3504], Loss: 0.4360\n",
      "Epoch [1/1], Step [1747/3504], Loss: 0.3310\n",
      "Epoch [1/1], Step [1748/3504], Loss: 0.3028\n",
      "Epoch [1/1], Step [1749/3504], Loss: 0.3234\n",
      "Epoch [1/1], Step [1750/3504], Loss: 0.3384\n",
      "Epoch [1/1], Step [1751/3504], Loss: 0.3384\n",
      "Epoch [1/1], Step [1752/3504], Loss: 0.4180\n",
      "Epoch [1/1], Step [1753/3504], Loss: 0.4386\n",
      "Epoch [1/1], Step [1754/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [1755/3504], Loss: 0.3538\n",
      "Epoch [1/1], Step [1756/3504], Loss: 0.2996\n",
      "Epoch [1/1], Step [1757/3504], Loss: 0.4160\n",
      "Epoch [1/1], Step [1758/3504], Loss: 0.3444\n",
      "Epoch [1/1], Step [1759/3504], Loss: 0.3527\n",
      "Epoch [1/1], Step [1760/3504], Loss: 0.3006\n",
      "Epoch [1/1], Step [1761/3504], Loss: 0.3388\n",
      "Epoch [1/1], Step [1762/3504], Loss: 0.2348\n",
      "Epoch [1/1], Step [1763/3504], Loss: 0.3532\n",
      "Epoch [1/1], Step [1764/3504], Loss: 0.3784\n",
      "Epoch [1/1], Step [1765/3504], Loss: 0.2987\n",
      "Epoch [1/1], Step [1766/3504], Loss: 0.3799\n",
      "Epoch [1/1], Step [1767/3504], Loss: 0.3777\n",
      "Epoch [1/1], Step [1768/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [1769/3504], Loss: 0.2567\n",
      "Epoch [1/1], Step [1770/3504], Loss: 0.2989\n",
      "Epoch [1/1], Step [1771/3504], Loss: 0.2803\n",
      "Epoch [1/1], Step [1772/3504], Loss: 0.3821\n",
      "Epoch [1/1], Step [1773/3504], Loss: 0.3128\n",
      "Epoch [1/1], Step [1774/3504], Loss: 0.3825\n",
      "Epoch [1/1], Step [1775/3504], Loss: 0.3871\n",
      "Epoch [1/1], Step [1776/3504], Loss: 0.4604\n",
      "Epoch [1/1], Step [1777/3504], Loss: 0.3566\n",
      "Epoch [1/1], Step [1778/3504], Loss: 0.4538\n",
      "Epoch [1/1], Step [1779/3504], Loss: 0.3753\n",
      "Epoch [1/1], Step [1780/3504], Loss: 0.2919\n",
      "Epoch [1/1], Step [1781/3504], Loss: 0.2864\n",
      "Epoch [1/1], Step [1782/3504], Loss: 0.3536\n",
      "Epoch [1/1], Step [1783/3504], Loss: 0.2999\n",
      "Epoch [1/1], Step [1784/3504], Loss: 0.4303\n",
      "Epoch [1/1], Step [1785/3504], Loss: 0.3283\n",
      "Epoch [1/1], Step [1786/3504], Loss: 0.2928\n",
      "Epoch [1/1], Step [1787/3504], Loss: 0.3447\n",
      "Epoch [1/1], Step [1788/3504], Loss: 0.3517\n",
      "Epoch [1/1], Step [1789/3504], Loss: 0.3893\n",
      "Epoch [1/1], Step [1790/3504], Loss: 0.3993\n",
      "Epoch [1/1], Step [1791/3504], Loss: 0.3960\n",
      "Epoch [1/1], Step [1792/3504], Loss: 0.5081\n",
      "Epoch [1/1], Step [1793/3504], Loss: 0.4167\n",
      "Epoch [1/1], Step [1794/3504], Loss: 0.4099\n",
      "Epoch [1/1], Step [1795/3504], Loss: 0.3810\n",
      "Epoch [1/1], Step [1796/3504], Loss: 0.3831\n",
      "Epoch [1/1], Step [1797/3504], Loss: 0.3186\n",
      "Epoch [1/1], Step [1798/3504], Loss: 0.3629\n",
      "Epoch [1/1], Step [1799/3504], Loss: 0.4371\n",
      "Epoch [1/1], Step [1800/3504], Loss: 0.4238\n",
      "Epoch [1/1], Step [1801/3504], Loss: 0.4664\n",
      "Epoch [1/1], Step [1802/3504], Loss: 0.3511\n",
      "Epoch [1/1], Step [1803/3504], Loss: 0.4038\n",
      "Epoch [1/1], Step [1804/3504], Loss: 0.4396\n",
      "Epoch [1/1], Step [1805/3504], Loss: 0.3263\n",
      "Epoch [1/1], Step [1806/3504], Loss: 0.4085\n",
      "Epoch [1/1], Step [1807/3504], Loss: 0.4197\n",
      "Epoch [1/1], Step [1808/3504], Loss: 0.5457\n",
      "Epoch [1/1], Step [1809/3504], Loss: 0.3629\n",
      "Epoch [1/1], Step [1810/3504], Loss: 0.3375\n",
      "Epoch [1/1], Step [1811/3504], Loss: 0.3506\n",
      "Epoch [1/1], Step [1812/3504], Loss: 0.3744\n",
      "Epoch [1/1], Step [1813/3504], Loss: 0.3154\n",
      "Epoch [1/1], Step [1814/3504], Loss: 0.4551\n",
      "Epoch [1/1], Step [1815/3504], Loss: 0.4564\n",
      "Epoch [1/1], Step [1816/3504], Loss: 0.3879\n",
      "Epoch [1/1], Step [1817/3504], Loss: 0.3817\n",
      "Epoch [1/1], Step [1818/3504], Loss: 0.4454\n",
      "Epoch [1/1], Step [1819/3504], Loss: 0.4661\n",
      "Epoch [1/1], Step [1820/3504], Loss: 0.4464\n",
      "Epoch [1/1], Step [1821/3504], Loss: 0.3994\n",
      "Epoch [1/1], Step [1822/3504], Loss: 0.3924\n",
      "Epoch [1/1], Step [1823/3504], Loss: 0.4321\n",
      "Epoch [1/1], Step [1824/3504], Loss: 0.4225\n",
      "Epoch [1/1], Step [1825/3504], Loss: 0.4670\n",
      "Epoch [1/1], Step [1826/3504], Loss: 0.2757\n",
      "Epoch [1/1], Step [1827/3504], Loss: 0.4225\n",
      "Epoch [1/1], Step [1828/3504], Loss: 0.4652\n",
      "Epoch [1/1], Step [1829/3504], Loss: 0.3791\n",
      "Epoch [1/1], Step [1830/3504], Loss: 0.4125\n",
      "Epoch [1/1], Step [1831/3504], Loss: 0.4086\n",
      "Epoch [1/1], Step [1832/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [1833/3504], Loss: 0.3357\n",
      "Epoch [1/1], Step [1834/3504], Loss: 0.3796\n",
      "Epoch [1/1], Step [1835/3504], Loss: 0.3818\n",
      "Epoch [1/1], Step [1836/3504], Loss: 0.3178\n",
      "Epoch [1/1], Step [1837/3504], Loss: 0.4400\n",
      "Epoch [1/1], Step [1838/3504], Loss: 1.9197\n",
      "Epoch [1/1], Step [1839/3504], Loss: 0.3050\n",
      "Epoch [1/1], Step [1840/3504], Loss: 0.3830\n",
      "Epoch [1/1], Step [1841/3504], Loss: 0.3553\n",
      "Epoch [1/1], Step [1842/3504], Loss: 0.3681\n",
      "Epoch [1/1], Step [1843/3504], Loss: 0.3579\n",
      "Epoch [1/1], Step [1844/3504], Loss: 0.3975\n",
      "Epoch [1/1], Step [1845/3504], Loss: 0.4130\n",
      "Epoch [1/1], Step [1846/3504], Loss: 0.3798\n",
      "Epoch [1/1], Step [1847/3504], Loss: 0.3638\n",
      "Epoch [1/1], Step [1848/3504], Loss: 0.4353\n",
      "Epoch [1/1], Step [1849/3504], Loss: 0.6811\n",
      "Epoch [1/1], Step [1850/3504], Loss: 0.4865\n",
      "Epoch [1/1], Step [1851/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [1852/3504], Loss: 0.4434\n",
      "Epoch [1/1], Step [1853/3504], Loss: 0.4550\n",
      "Epoch [1/1], Step [1854/3504], Loss: 0.4666\n",
      "Epoch [1/1], Step [1855/3504], Loss: 0.3783\n",
      "Epoch [1/1], Step [1856/3504], Loss: 0.3347\n",
      "Epoch [1/1], Step [1857/3504], Loss: 0.3901\n",
      "Epoch [1/1], Step [1858/3504], Loss: 0.3117\n",
      "Epoch [1/1], Step [1859/3504], Loss: 0.4594\n",
      "Epoch [1/1], Step [1860/3504], Loss: 0.4313\n",
      "Epoch [1/1], Step [1861/3504], Loss: 0.4458\n",
      "Epoch [1/1], Step [1862/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [1863/3504], Loss: 0.3900\n",
      "Epoch [1/1], Step [1864/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [1865/3504], Loss: 0.3925\n",
      "Epoch [1/1], Step [1866/3504], Loss: 0.3172\n",
      "Epoch [1/1], Step [1867/3504], Loss: 0.4076\n",
      "Epoch [1/1], Step [1868/3504], Loss: 0.3458\n",
      "Epoch [1/1], Step [1869/3504], Loss: 0.3312\n",
      "Epoch [1/1], Step [1870/3504], Loss: 0.4564\n",
      "Epoch [1/1], Step [1871/3504], Loss: 0.3395\n",
      "Epoch [1/1], Step [1872/3504], Loss: 0.4136\n",
      "Epoch [1/1], Step [1873/3504], Loss: 0.3824\n",
      "Epoch [1/1], Step [1874/3504], Loss: 0.4403\n",
      "Epoch [1/1], Step [1875/3504], Loss: 0.3436\n",
      "Epoch [1/1], Step [1876/3504], Loss: 0.3635\n",
      "Epoch [1/1], Step [1877/3504], Loss: 0.3576\n",
      "Epoch [1/1], Step [1878/3504], Loss: 0.4441\n",
      "Epoch [1/1], Step [1879/3504], Loss: 0.3476\n",
      "Epoch [1/1], Step [1880/3504], Loss: 0.2546\n",
      "Epoch [1/1], Step [1881/3504], Loss: 0.3256\n",
      "Epoch [1/1], Step [1882/3504], Loss: 0.3932\n",
      "Epoch [1/1], Step [1883/3504], Loss: 0.4293\n",
      "Epoch [1/1], Step [1884/3504], Loss: 0.4065\n",
      "Epoch [1/1], Step [1885/3504], Loss: 0.4668\n",
      "Epoch [1/1], Step [1886/3504], Loss: 0.3882\n",
      "Epoch [1/1], Step [1887/3504], Loss: 0.3662\n",
      "Epoch [1/1], Step [1888/3504], Loss: 0.4083\n",
      "Epoch [1/1], Step [1889/3504], Loss: 0.3532\n",
      "Epoch [1/1], Step [1890/3504], Loss: 0.3644\n",
      "Epoch [1/1], Step [1891/3504], Loss: 0.5133\n",
      "Epoch [1/1], Step [1892/3504], Loss: 0.3098\n",
      "Epoch [1/1], Step [1893/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [1894/3504], Loss: 0.3760\n",
      "Epoch [1/1], Step [1895/3504], Loss: 0.4636\n",
      "Epoch [1/1], Step [1896/3504], Loss: 0.3509\n",
      "Epoch [1/1], Step [1897/3504], Loss: 0.2658\n",
      "Epoch [1/1], Step [1898/3504], Loss: 0.2813\n",
      "Epoch [1/1], Step [1899/3504], Loss: 0.6242\n",
      "Epoch [1/1], Step [1900/3504], Loss: 0.3186\n",
      "Epoch [1/1], Step [1901/3504], Loss: 0.4348\n",
      "Epoch [1/1], Step [1902/3504], Loss: 0.3825\n",
      "Epoch [1/1], Step [1903/3504], Loss: 0.3593\n",
      "Epoch [1/1], Step [1904/3504], Loss: 0.2997\n",
      "Epoch [1/1], Step [1905/3504], Loss: 0.4360\n",
      "Epoch [1/1], Step [1906/3504], Loss: 0.3444\n",
      "Epoch [1/1], Step [1907/3504], Loss: 0.3599\n",
      "Epoch [1/1], Step [1908/3504], Loss: 0.4581\n",
      "Epoch [1/1], Step [1909/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [1910/3504], Loss: 0.3475\n",
      "Epoch [1/1], Step [1911/3504], Loss: 0.4119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1912/3504], Loss: 0.3745\n",
      "Epoch [1/1], Step [1913/3504], Loss: 0.3807\n",
      "Epoch [1/1], Step [1914/3504], Loss: 0.3516\n",
      "Epoch [1/1], Step [1915/3504], Loss: 0.3324\n",
      "Epoch [1/1], Step [1916/3504], Loss: 0.3604\n",
      "Epoch [1/1], Step [1917/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [1918/3504], Loss: 0.3991\n",
      "Epoch [1/1], Step [1919/3504], Loss: 0.3997\n",
      "Epoch [1/1], Step [1920/3504], Loss: 0.4821\n",
      "Epoch [1/1], Step [1921/3504], Loss: 0.4397\n",
      "Epoch [1/1], Step [1922/3504], Loss: 0.4612\n",
      "Epoch [1/1], Step [1923/3504], Loss: 0.3054\n",
      "Epoch [1/1], Step [1924/3504], Loss: 0.3625\n",
      "Epoch [1/1], Step [1925/3504], Loss: 0.3640\n",
      "Epoch [1/1], Step [1926/3504], Loss: 0.4535\n",
      "Epoch [1/1], Step [1927/3504], Loss: 0.3972\n",
      "Epoch [1/1], Step [1928/3504], Loss: 0.3577\n",
      "Epoch [1/1], Step [1929/3504], Loss: 0.4382\n",
      "Epoch [1/1], Step [1930/3504], Loss: 0.4087\n",
      "Epoch [1/1], Step [1931/3504], Loss: 0.3577\n",
      "Epoch [1/1], Step [1932/3504], Loss: 0.3346\n",
      "Epoch [1/1], Step [1933/3504], Loss: 0.2781\n",
      "Epoch [1/1], Step [1934/3504], Loss: 0.3873\n",
      "Epoch [1/1], Step [1935/3504], Loss: 0.4584\n",
      "Epoch [1/1], Step [1936/3504], Loss: 0.3104\n",
      "Epoch [1/1], Step [1937/3504], Loss: 0.3244\n",
      "Epoch [1/1], Step [1938/3504], Loss: 0.2968\n",
      "Epoch [1/1], Step [1939/3504], Loss: 0.2692\n",
      "Epoch [1/1], Step [1940/3504], Loss: 0.4015\n",
      "Epoch [1/1], Step [1941/3504], Loss: 0.2938\n",
      "Epoch [1/1], Step [1942/3504], Loss: 0.3917\n",
      "Epoch [1/1], Step [1943/3504], Loss: 0.4111\n",
      "Epoch [1/1], Step [1944/3504], Loss: 0.2569\n",
      "Epoch [1/1], Step [1945/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [1946/3504], Loss: 0.3980\n",
      "Epoch [1/1], Step [1947/3504], Loss: 0.4119\n",
      "Epoch [1/1], Step [1948/3504], Loss: 0.4111\n",
      "Epoch [1/1], Step [1949/3504], Loss: 0.4183\n",
      "Epoch [1/1], Step [1950/3504], Loss: 0.3555\n",
      "Epoch [1/1], Step [1951/3504], Loss: 0.4267\n",
      "Epoch [1/1], Step [1952/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [1953/3504], Loss: 0.4581\n",
      "Epoch [1/1], Step [1954/3504], Loss: 0.2850\n",
      "Epoch [1/1], Step [1955/3504], Loss: 0.4660\n",
      "Epoch [1/1], Step [1956/3504], Loss: 0.3697\n",
      "Epoch [1/1], Step [1957/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [1958/3504], Loss: 0.4615\n",
      "Epoch [1/1], Step [1959/3504], Loss: 0.2840\n",
      "Epoch [1/1], Step [1960/3504], Loss: 0.4616\n",
      "Epoch [1/1], Step [1961/3504], Loss: 0.3605\n",
      "Epoch [1/1], Step [1962/3504], Loss: 0.3549\n",
      "Epoch [1/1], Step [1963/3504], Loss: 0.3480\n",
      "Epoch [1/1], Step [1964/3504], Loss: 0.3380\n",
      "Epoch [1/1], Step [1965/3504], Loss: 0.3513\n",
      "Epoch [1/1], Step [1966/3504], Loss: 0.3177\n",
      "Epoch [1/1], Step [1967/3504], Loss: 0.5087\n",
      "Epoch [1/1], Step [1968/3504], Loss: 0.3845\n",
      "Epoch [1/1], Step [1969/3504], Loss: 0.3607\n",
      "Epoch [1/1], Step [1970/3504], Loss: 0.2857\n",
      "Epoch [1/1], Step [1971/3504], Loss: 0.3341\n",
      "Epoch [1/1], Step [1972/3504], Loss: 0.3348\n",
      "Epoch [1/1], Step [1973/3504], Loss: 0.3823\n",
      "Epoch [1/1], Step [1974/3504], Loss: 0.3733\n",
      "Epoch [1/1], Step [1975/3504], Loss: 0.3643\n",
      "Epoch [1/1], Step [1976/3504], Loss: 0.4152\n",
      "Epoch [1/1], Step [1977/3504], Loss: 0.3473\n",
      "Epoch [1/1], Step [1978/3504], Loss: 0.3260\n",
      "Epoch [1/1], Step [1979/3504], Loss: 0.4154\n",
      "Epoch [1/1], Step [1980/3504], Loss: 0.3611\n",
      "Epoch [1/1], Step [1981/3504], Loss: 0.3745\n",
      "Epoch [1/1], Step [1982/3504], Loss: 0.3031\n",
      "Epoch [1/1], Step [1983/3504], Loss: 0.5186\n",
      "Epoch [1/1], Step [1984/3504], Loss: 0.4186\n",
      "Epoch [1/1], Step [1985/3504], Loss: 0.3103\n",
      "Epoch [1/1], Step [1986/3504], Loss: 0.3817\n",
      "Epoch [1/1], Step [1987/3504], Loss: 0.3404\n",
      "Epoch [1/1], Step [1988/3504], Loss: 0.3748\n",
      "Epoch [1/1], Step [1989/3504], Loss: 0.3070\n",
      "Epoch [1/1], Step [1990/3504], Loss: 0.2855\n",
      "Epoch [1/1], Step [1991/3504], Loss: 0.3036\n",
      "Epoch [1/1], Step [1992/3504], Loss: 0.3816\n",
      "Epoch [1/1], Step [1993/3504], Loss: 0.2564\n",
      "Epoch [1/1], Step [1994/3504], Loss: 0.2798\n",
      "Epoch [1/1], Step [1995/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [1996/3504], Loss: 0.3981\n",
      "Epoch [1/1], Step [1997/3504], Loss: 0.4419\n",
      "Epoch [1/1], Step [1998/3504], Loss: 0.2744\n",
      "Epoch [1/1], Step [1999/3504], Loss: 0.3154\n",
      "Epoch [1/1], Step [2000/3504], Loss: 0.3766\n",
      "Epoch [1/1], Step [2001/3504], Loss: 0.3754\n",
      "Epoch [1/1], Step [2002/3504], Loss: 0.4262\n",
      "Epoch [1/1], Step [2003/3504], Loss: 0.4146\n",
      "Epoch [1/1], Step [2004/3504], Loss: 0.4490\n",
      "Epoch [1/1], Step [2005/3504], Loss: 0.4750\n",
      "Epoch [1/1], Step [2006/3504], Loss: 0.3577\n",
      "Epoch [1/1], Step [2007/3504], Loss: 0.3583\n",
      "Epoch [1/1], Step [2008/3504], Loss: 0.3261\n",
      "Epoch [1/1], Step [2009/3504], Loss: 0.2546\n",
      "Epoch [1/1], Step [2010/3504], Loss: 0.3240\n",
      "Epoch [1/1], Step [2011/3504], Loss: 0.3433\n",
      "Epoch [1/1], Step [2012/3504], Loss: 0.2612\n",
      "Epoch [1/1], Step [2013/3504], Loss: 0.3818\n",
      "Epoch [1/1], Step [2014/3504], Loss: 0.2721\n",
      "Epoch [1/1], Step [2015/3504], Loss: 0.3802\n",
      "Epoch [1/1], Step [2016/3504], Loss: 0.3982\n",
      "Epoch [1/1], Step [2017/3504], Loss: 0.3216\n",
      "Epoch [1/1], Step [2018/3504], Loss: 0.3174\n",
      "Epoch [1/1], Step [2019/3504], Loss: 0.2899\n",
      "Epoch [1/1], Step [2020/3504], Loss: 0.3883\n",
      "Epoch [1/1], Step [2021/3504], Loss: 0.3174\n",
      "Epoch [1/1], Step [2022/3504], Loss: 0.4058\n",
      "Epoch [1/1], Step [2023/3504], Loss: 0.4046\n",
      "Epoch [1/1], Step [2024/3504], Loss: 0.4124\n",
      "Epoch [1/1], Step [2025/3504], Loss: 0.4371\n",
      "Epoch [1/1], Step [2026/3504], Loss: 0.3790\n",
      "Epoch [1/1], Step [2027/3504], Loss: 0.4669\n",
      "Epoch [1/1], Step [2028/3504], Loss: 0.3176\n",
      "Epoch [1/1], Step [2029/3504], Loss: 0.3097\n",
      "Epoch [1/1], Step [2030/3504], Loss: 0.3861\n",
      "Epoch [1/1], Step [2031/3504], Loss: 0.4347\n",
      "Epoch [1/1], Step [2032/3504], Loss: 0.5710\n",
      "Epoch [1/1], Step [2033/3504], Loss: 0.2651\n",
      "Epoch [1/1], Step [2034/3504], Loss: 0.3003\n",
      "Epoch [1/1], Step [2035/3504], Loss: 0.3932\n",
      "Epoch [1/1], Step [2036/3504], Loss: 0.3075\n",
      "Epoch [1/1], Step [2037/3504], Loss: 0.3737\n",
      "Epoch [1/1], Step [2038/3504], Loss: 0.3680\n",
      "Epoch [1/1], Step [2039/3504], Loss: 0.4597\n",
      "Epoch [1/1], Step [2040/3504], Loss: 0.3840\n",
      "Epoch [1/1], Step [2041/3504], Loss: 0.4438\n",
      "Epoch [1/1], Step [2042/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [2043/3504], Loss: 0.3241\n",
      "Epoch [1/1], Step [2044/3504], Loss: 0.3412\n",
      "Epoch [1/1], Step [2045/3504], Loss: 0.4388\n",
      "Epoch [1/1], Step [2046/3504], Loss: 0.3488\n",
      "Epoch [1/1], Step [2047/3504], Loss: 0.2891\n",
      "Epoch [1/1], Step [2048/3504], Loss: 0.3482\n",
      "Epoch [1/1], Step [2049/3504], Loss: 0.3648\n",
      "Epoch [1/1], Step [2050/3504], Loss: 0.4262\n",
      "Epoch [1/1], Step [2051/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [2052/3504], Loss: 0.3745\n",
      "Epoch [1/1], Step [2053/3504], Loss: 0.3639\n",
      "Epoch [1/1], Step [2054/3504], Loss: 0.3462\n",
      "Epoch [1/1], Step [2055/3504], Loss: 0.2846\n",
      "Epoch [1/1], Step [2056/3504], Loss: 0.3052\n",
      "Epoch [1/1], Step [2057/3504], Loss: 0.4304\n",
      "Epoch [1/1], Step [2058/3504], Loss: 0.3989\n",
      "Epoch [1/1], Step [2059/3504], Loss: 0.3335\n",
      "Epoch [1/1], Step [2060/3504], Loss: 0.3725\n",
      "Epoch [1/1], Step [2061/3504], Loss: 0.2557\n",
      "Epoch [1/1], Step [2062/3504], Loss: 0.3695\n",
      "Epoch [1/1], Step [2063/3504], Loss: 0.3791\n",
      "Epoch [1/1], Step [2064/3504], Loss: 0.5036\n",
      "Epoch [1/1], Step [2065/3504], Loss: 0.4420\n",
      "Epoch [1/1], Step [2066/3504], Loss: 0.3700\n",
      "Epoch [1/1], Step [2067/3504], Loss: 0.3640\n",
      "Epoch [1/1], Step [2068/3504], Loss: 0.4532\n",
      "Epoch [1/1], Step [2069/3504], Loss: 0.3366\n",
      "Epoch [1/1], Step [2070/3504], Loss: 0.4199\n",
      "Epoch [1/1], Step [2071/3504], Loss: 0.4697\n",
      "Epoch [1/1], Step [2072/3504], Loss: 0.4178\n",
      "Epoch [1/1], Step [2073/3504], Loss: 0.4423\n",
      "Epoch [1/1], Step [2074/3504], Loss: 0.3867\n",
      "Epoch [1/1], Step [2075/3504], Loss: 0.4446\n",
      "Epoch [1/1], Step [2076/3504], Loss: 0.4043\n",
      "Epoch [1/1], Step [2077/3504], Loss: 0.3345\n",
      "Epoch [1/1], Step [2078/3504], Loss: 0.3962\n",
      "Epoch [1/1], Step [2079/3504], Loss: 0.3042\n",
      "Epoch [1/1], Step [2080/3504], Loss: 0.3977\n",
      "Epoch [1/1], Step [2081/3504], Loss: 0.3140\n",
      "Epoch [1/1], Step [2082/3504], Loss: 0.3814\n",
      "Epoch [1/1], Step [2083/3504], Loss: 0.3383\n",
      "Epoch [1/1], Step [2084/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [2085/3504], Loss: 0.3472\n",
      "Epoch [1/1], Step [2086/3504], Loss: 0.3744\n",
      "Epoch [1/1], Step [2087/3504], Loss: 0.3562\n",
      "Epoch [1/1], Step [2088/3504], Loss: 0.4638\n",
      "Epoch [1/1], Step [2089/3504], Loss: 0.3334\n",
      "Epoch [1/1], Step [2090/3504], Loss: 0.3644\n",
      "Epoch [1/1], Step [2091/3504], Loss: 0.3885\n",
      "Epoch [1/1], Step [2092/3504], Loss: 0.3849\n",
      "Epoch [1/1], Step [2093/3504], Loss: 0.2710\n",
      "Epoch [1/1], Step [2094/3504], Loss: 0.3146\n",
      "Epoch [1/1], Step [2095/3504], Loss: 0.3273\n",
      "Epoch [1/1], Step [2096/3504], Loss: 0.4413\n",
      "Epoch [1/1], Step [2097/3504], Loss: 0.3864\n",
      "Epoch [1/1], Step [2098/3504], Loss: 0.3706\n",
      "Epoch [1/1], Step [2099/3504], Loss: 0.3663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [2100/3504], Loss: 0.3741\n",
      "Epoch [1/1], Step [2101/3504], Loss: 0.4211\n",
      "Epoch [1/1], Step [2102/3504], Loss: 0.3609\n",
      "Epoch [1/1], Step [2103/3504], Loss: 0.2903\n",
      "Epoch [1/1], Step [2104/3504], Loss: 0.3975\n",
      "Epoch [1/1], Step [2105/3504], Loss: 0.3934\n",
      "Epoch [1/1], Step [2106/3504], Loss: 0.4304\n",
      "Epoch [1/1], Step [2107/3504], Loss: 0.4618\n",
      "Epoch [1/1], Step [2108/3504], Loss: 0.3572\n",
      "Epoch [1/1], Step [2109/3504], Loss: 0.4006\n",
      "Epoch [1/1], Step [2110/3504], Loss: 0.3619\n",
      "Epoch [1/1], Step [2111/3504], Loss: 0.4112\n",
      "Epoch [1/1], Step [2112/3504], Loss: 0.4360\n",
      "Epoch [1/1], Step [2113/3504], Loss: 0.2286\n",
      "Epoch [1/1], Step [2114/3504], Loss: 0.4316\n",
      "Epoch [1/1], Step [2115/3504], Loss: 0.3325\n",
      "Epoch [1/1], Step [2116/3504], Loss: 0.3993\n",
      "Epoch [1/1], Step [2117/3504], Loss: 0.3929\n",
      "Epoch [1/1], Step [2118/3504], Loss: 0.4105\n",
      "Epoch [1/1], Step [2119/3504], Loss: 0.3736\n",
      "Epoch [1/1], Step [2120/3504], Loss: 0.3635\n",
      "Epoch [1/1], Step [2121/3504], Loss: 0.3215\n",
      "Epoch [1/1], Step [2122/3504], Loss: 0.3031\n",
      "Epoch [1/1], Step [2123/3504], Loss: 0.5212\n",
      "Epoch [1/1], Step [2124/3504], Loss: 0.6604\n",
      "Epoch [1/1], Step [2125/3504], Loss: 0.3311\n",
      "Epoch [1/1], Step [2126/3504], Loss: 0.3946\n",
      "Epoch [1/1], Step [2127/3504], Loss: 0.4548\n",
      "Epoch [1/1], Step [2128/3504], Loss: 0.3700\n",
      "Epoch [1/1], Step [2129/3504], Loss: 0.4406\n",
      "Epoch [1/1], Step [2130/3504], Loss: 0.3789\n",
      "Epoch [1/1], Step [2131/3504], Loss: 0.3128\n",
      "Epoch [1/1], Step [2132/3504], Loss: 0.3176\n",
      "Epoch [1/1], Step [2133/3504], Loss: 0.3179\n",
      "Epoch [1/1], Step [2134/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [2135/3504], Loss: 0.4583\n",
      "Epoch [1/1], Step [2136/3504], Loss: 0.4381\n",
      "Epoch [1/1], Step [2137/3504], Loss: 0.3163\n",
      "Epoch [1/1], Step [2138/3504], Loss: 0.3693\n",
      "Epoch [1/1], Step [2139/3504], Loss: 0.3747\n",
      "Epoch [1/1], Step [2140/3504], Loss: 0.3621\n",
      "Epoch [1/1], Step [2141/3504], Loss: 0.4950\n",
      "Epoch [1/1], Step [2142/3504], Loss: 0.3492\n",
      "Epoch [1/1], Step [2143/3504], Loss: 0.4140\n",
      "Epoch [1/1], Step [2144/3504], Loss: 0.3186\n",
      "Epoch [1/1], Step [2145/3504], Loss: 0.4076\n",
      "Epoch [1/1], Step [2146/3504], Loss: 0.3247\n",
      "Epoch [1/1], Step [2147/3504], Loss: 0.5029\n",
      "Epoch [1/1], Step [2148/3504], Loss: 0.4162\n",
      "Epoch [1/1], Step [2149/3504], Loss: 0.3506\n",
      "Epoch [1/1], Step [2150/3504], Loss: 0.3424\n",
      "Epoch [1/1], Step [2151/3504], Loss: 0.3926\n",
      "Epoch [1/1], Step [2152/3504], Loss: 0.4032\n",
      "Epoch [1/1], Step [2153/3504], Loss: 0.3658\n",
      "Epoch [1/1], Step [2154/3504], Loss: 0.3303\n",
      "Epoch [1/1], Step [2155/3504], Loss: 0.3469\n",
      "Epoch [1/1], Step [2156/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [2157/3504], Loss: 0.4324\n",
      "Epoch [1/1], Step [2158/3504], Loss: 0.3365\n",
      "Epoch [1/1], Step [2159/3504], Loss: 0.3745\n",
      "Epoch [1/1], Step [2160/3504], Loss: 0.3859\n",
      "Epoch [1/1], Step [2161/3504], Loss: 0.3216\n",
      "Epoch [1/1], Step [2162/3504], Loss: 0.3884\n",
      "Epoch [1/1], Step [2163/3504], Loss: 0.3799\n",
      "Epoch [1/1], Step [2164/3504], Loss: 0.3892\n",
      "Epoch [1/1], Step [2165/3504], Loss: 0.4799\n",
      "Epoch [1/1], Step [2166/3504], Loss: 0.5221\n",
      "Epoch [1/1], Step [2167/3504], Loss: 0.4476\n",
      "Epoch [1/1], Step [2168/3504], Loss: 0.5239\n",
      "Epoch [1/1], Step [2169/3504], Loss: 0.3217\n",
      "Epoch [1/1], Step [2170/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [2171/3504], Loss: 0.3785\n",
      "Epoch [1/1], Step [2172/3504], Loss: 0.4355\n",
      "Epoch [1/1], Step [2173/3504], Loss: 0.2479\n",
      "Epoch [1/1], Step [2174/3504], Loss: 0.2864\n",
      "Epoch [1/1], Step [2175/3504], Loss: 0.3760\n",
      "Epoch [1/1], Step [2176/3504], Loss: 0.5108\n",
      "Epoch [1/1], Step [2177/3504], Loss: 0.3184\n",
      "Epoch [1/1], Step [2178/3504], Loss: 0.3242\n",
      "Epoch [1/1], Step [2179/3504], Loss: 0.4294\n",
      "Epoch [1/1], Step [2180/3504], Loss: 0.3566\n",
      "Epoch [1/1], Step [2181/3504], Loss: 0.2951\n",
      "Epoch [1/1], Step [2182/3504], Loss: 0.4096\n",
      "Epoch [1/1], Step [2183/3504], Loss: 0.3102\n",
      "Epoch [1/1], Step [2184/3504], Loss: 0.3836\n",
      "Epoch [1/1], Step [2185/3504], Loss: 0.4353\n",
      "Epoch [1/1], Step [2186/3504], Loss: 0.3947\n",
      "Epoch [1/1], Step [2187/3504], Loss: 0.4686\n",
      "Epoch [1/1], Step [2188/3504], Loss: 0.4559\n",
      "Epoch [1/1], Step [2189/3504], Loss: 0.4118\n",
      "Epoch [1/1], Step [2190/3504], Loss: 0.3553\n",
      "Epoch [1/1], Step [2191/3504], Loss: 0.3115\n",
      "Epoch [1/1], Step [2192/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [2193/3504], Loss: 0.2220\n",
      "Epoch [1/1], Step [2194/3504], Loss: 0.4146\n",
      "Epoch [1/1], Step [2195/3504], Loss: 0.3902\n",
      "Epoch [1/1], Step [2196/3504], Loss: 0.5576\n",
      "Epoch [1/1], Step [2197/3504], Loss: 0.4825\n",
      "Epoch [1/1], Step [2198/3504], Loss: 0.4053\n",
      "Epoch [1/1], Step [2199/3504], Loss: 0.3838\n",
      "Epoch [1/1], Step [2200/3504], Loss: 0.4115\n",
      "Epoch [1/1], Step [2201/3504], Loss: 0.3767\n",
      "Epoch [1/1], Step [2202/3504], Loss: 0.5035\n",
      "Epoch [1/1], Step [2203/3504], Loss: 0.3593\n",
      "Epoch [1/1], Step [2204/3504], Loss: 0.3242\n",
      "Epoch [1/1], Step [2205/3504], Loss: 0.3865\n",
      "Epoch [1/1], Step [2206/3504], Loss: 0.3070\n",
      "Epoch [1/1], Step [2207/3504], Loss: 0.3796\n",
      "Epoch [1/1], Step [2208/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [2209/3504], Loss: 0.2868\n",
      "Epoch [1/1], Step [2210/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [2211/3504], Loss: 0.3633\n",
      "Epoch [1/1], Step [2212/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [2213/3504], Loss: 0.3151\n",
      "Epoch [1/1], Step [2214/3504], Loss: 0.3561\n",
      "Epoch [1/1], Step [2215/3504], Loss: 0.2905\n",
      "Epoch [1/1], Step [2216/3504], Loss: 0.3926\n",
      "Epoch [1/1], Step [2217/3504], Loss: 0.4039\n",
      "Epoch [1/1], Step [2218/3504], Loss: 0.3455\n",
      "Epoch [1/1], Step [2219/3504], Loss: 0.2939\n",
      "Epoch [1/1], Step [2220/3504], Loss: 0.3841\n",
      "Epoch [1/1], Step [2221/3504], Loss: 0.4787\n",
      "Epoch [1/1], Step [2222/3504], Loss: 0.3794\n",
      "Epoch [1/1], Step [2223/3504], Loss: 0.2801\n",
      "Epoch [1/1], Step [2224/3504], Loss: 0.4059\n",
      "Epoch [1/1], Step [2225/3504], Loss: 0.4075\n",
      "Epoch [1/1], Step [2226/3504], Loss: 0.3383\n",
      "Epoch [1/1], Step [2227/3504], Loss: 0.3219\n",
      "Epoch [1/1], Step [2228/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [2229/3504], Loss: 0.3262\n",
      "Epoch [1/1], Step [2230/3504], Loss: 0.2839\n",
      "Epoch [1/1], Step [2231/3504], Loss: 0.4429\n",
      "Epoch [1/1], Step [2232/3504], Loss: 0.5242\n",
      "Epoch [1/1], Step [2233/3504], Loss: 0.3529\n",
      "Epoch [1/1], Step [2234/3504], Loss: 0.3212\n",
      "Epoch [1/1], Step [2235/3504], Loss: 0.4017\n",
      "Epoch [1/1], Step [2236/3504], Loss: 0.3692\n",
      "Epoch [1/1], Step [2237/3504], Loss: 0.4535\n",
      "Epoch [1/1], Step [2238/3504], Loss: 0.3302\n",
      "Epoch [1/1], Step [2239/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [2240/3504], Loss: 0.4921\n",
      "Epoch [1/1], Step [2241/3504], Loss: 0.3337\n",
      "Epoch [1/1], Step [2242/3504], Loss: 0.3599\n",
      "Epoch [1/1], Step [2243/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [2244/3504], Loss: 0.3513\n",
      "Epoch [1/1], Step [2245/3504], Loss: 0.3976\n",
      "Epoch [1/1], Step [2246/3504], Loss: 0.3655\n",
      "Epoch [1/1], Step [2247/3504], Loss: 0.3473\n",
      "Epoch [1/1], Step [2248/3504], Loss: 0.3360\n",
      "Epoch [1/1], Step [2249/3504], Loss: 0.3628\n",
      "Epoch [1/1], Step [2250/3504], Loss: 0.3584\n",
      "Epoch [1/1], Step [2251/3504], Loss: 0.4463\n",
      "Epoch [1/1], Step [2252/3504], Loss: 0.3561\n",
      "Epoch [1/1], Step [2253/3504], Loss: 0.3285\n",
      "Epoch [1/1], Step [2254/3504], Loss: 0.3568\n",
      "Epoch [1/1], Step [2255/3504], Loss: 0.3344\n",
      "Epoch [1/1], Step [2256/3504], Loss: 0.4012\n",
      "Epoch [1/1], Step [2257/3504], Loss: 0.4028\n",
      "Epoch [1/1], Step [2258/3504], Loss: 0.3211\n",
      "Epoch [1/1], Step [2259/3504], Loss: 0.3420\n",
      "Epoch [1/1], Step [2260/3504], Loss: 0.2928\n",
      "Epoch [1/1], Step [2261/3504], Loss: 0.4373\n",
      "Epoch [1/1], Step [2262/3504], Loss: 0.4142\n",
      "Epoch [1/1], Step [2263/3504], Loss: 0.3426\n",
      "Epoch [1/1], Step [2264/3504], Loss: 0.3942\n",
      "Epoch [1/1], Step [2265/3504], Loss: 0.4086\n",
      "Epoch [1/1], Step [2266/3504], Loss: 0.2627\n",
      "Epoch [1/1], Step [2267/3504], Loss: 0.3453\n",
      "Epoch [1/1], Step [2268/3504], Loss: 0.3675\n",
      "Epoch [1/1], Step [2269/3504], Loss: 0.2807\n",
      "Epoch [1/1], Step [2270/3504], Loss: 0.4298\n",
      "Epoch [1/1], Step [2271/3504], Loss: 0.4233\n",
      "Epoch [1/1], Step [2272/3504], Loss: 0.3560\n",
      "Epoch [1/1], Step [2273/3504], Loss: 0.3348\n",
      "Epoch [1/1], Step [2274/3504], Loss: 0.3842\n",
      "Epoch [1/1], Step [2275/3504], Loss: 0.2208\n",
      "Epoch [1/1], Step [2276/3504], Loss: 0.3365\n",
      "Epoch [1/1], Step [2277/3504], Loss: 0.4914\n",
      "Epoch [1/1], Step [2278/3504], Loss: 0.4401\n",
      "Epoch [1/1], Step [2279/3504], Loss: 0.2659\n",
      "Epoch [1/1], Step [2280/3504], Loss: 0.3811\n",
      "Epoch [1/1], Step [2281/3504], Loss: 0.4473\n",
      "Epoch [1/1], Step [2282/3504], Loss: 0.4383\n",
      "Epoch [1/1], Step [2283/3504], Loss: 0.4579\n",
      "Epoch [1/1], Step [2284/3504], Loss: 0.2931\n",
      "Epoch [1/1], Step [2285/3504], Loss: 0.3031\n",
      "Epoch [1/1], Step [2286/3504], Loss: 0.4091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [2287/3504], Loss: 0.3743\n",
      "Epoch [1/1], Step [2288/3504], Loss: 0.4883\n",
      "Epoch [1/1], Step [2289/3504], Loss: 0.3281\n",
      "Epoch [1/1], Step [2290/3504], Loss: 0.2985\n",
      "Epoch [1/1], Step [2291/3504], Loss: 0.3995\n",
      "Epoch [1/1], Step [2292/3504], Loss: 0.5760\n",
      "Epoch [1/1], Step [2293/3504], Loss: 0.3437\n",
      "Epoch [1/1], Step [2294/3504], Loss: 0.3515\n",
      "Epoch [1/1], Step [2295/3504], Loss: 0.3390\n",
      "Epoch [1/1], Step [2296/3504], Loss: 0.4366\n",
      "Epoch [1/1], Step [2297/3504], Loss: 0.4221\n",
      "Epoch [1/1], Step [2298/3504], Loss: 0.4106\n",
      "Epoch [1/1], Step [2299/3504], Loss: 0.4186\n",
      "Epoch [1/1], Step [2300/3504], Loss: 0.4010\n",
      "Epoch [1/1], Step [2301/3504], Loss: 0.4353\n",
      "Epoch [1/1], Step [2302/3504], Loss: 0.2823\n",
      "Epoch [1/1], Step [2303/3504], Loss: 0.4840\n",
      "Epoch [1/1], Step [2304/3504], Loss: 0.4137\n",
      "Epoch [1/1], Step [2305/3504], Loss: 0.4077\n",
      "Epoch [1/1], Step [2306/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [2307/3504], Loss: 0.4384\n",
      "Epoch [1/1], Step [2308/3504], Loss: 0.4447\n",
      "Epoch [1/1], Step [2309/3504], Loss: 0.3310\n",
      "Epoch [1/1], Step [2310/3504], Loss: 0.7100\n",
      "Epoch [1/1], Step [2311/3504], Loss: 0.3180\n",
      "Epoch [1/1], Step [2312/3504], Loss: 0.4343\n",
      "Epoch [1/1], Step [2313/3504], Loss: 0.3988\n",
      "Epoch [1/1], Step [2314/3504], Loss: 0.3931\n",
      "Epoch [1/1], Step [2315/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [2316/3504], Loss: 0.3146\n",
      "Epoch [1/1], Step [2317/3504], Loss: 0.3374\n",
      "Epoch [1/1], Step [2318/3504], Loss: 0.2544\n",
      "Epoch [1/1], Step [2319/3504], Loss: 0.3339\n",
      "Epoch [1/1], Step [2320/3504], Loss: 0.3806\n",
      "Epoch [1/1], Step [2321/3504], Loss: 0.2978\n",
      "Epoch [1/1], Step [2322/3504], Loss: 0.3503\n",
      "Epoch [1/1], Step [2323/3504], Loss: 0.3840\n",
      "Epoch [1/1], Step [2324/3504], Loss: 0.3711\n",
      "Epoch [1/1], Step [2325/3504], Loss: 0.3036\n",
      "Epoch [1/1], Step [2326/3504], Loss: 0.2765\n",
      "Epoch [1/1], Step [2327/3504], Loss: 0.3620\n",
      "Epoch [1/1], Step [2328/3504], Loss: 0.4424\n",
      "Epoch [1/1], Step [2329/3504], Loss: 0.3710\n",
      "Epoch [1/1], Step [2330/3504], Loss: 0.2986\n",
      "Epoch [1/1], Step [2331/3504], Loss: 0.2997\n",
      "Epoch [1/1], Step [2332/3504], Loss: 0.2641\n",
      "Epoch [1/1], Step [2333/3504], Loss: 0.4226\n",
      "Epoch [1/1], Step [2334/3504], Loss: 0.5047\n",
      "Epoch [1/1], Step [2335/3504], Loss: 0.4516\n",
      "Epoch [1/1], Step [2336/3504], Loss: 0.3851\n",
      "Epoch [1/1], Step [2337/3504], Loss: 0.4168\n",
      "Epoch [1/1], Step [2338/3504], Loss: 0.3417\n",
      "Epoch [1/1], Step [2339/3504], Loss: 0.3105\n",
      "Epoch [1/1], Step [2340/3504], Loss: 0.3589\n",
      "Epoch [1/1], Step [2341/3504], Loss: 0.4425\n",
      "Epoch [1/1], Step [2342/3504], Loss: 0.4352\n",
      "Epoch [1/1], Step [2343/3504], Loss: 0.4246\n",
      "Epoch [1/1], Step [2344/3504], Loss: 0.3445\n",
      "Epoch [1/1], Step [2345/3504], Loss: 0.4066\n",
      "Epoch [1/1], Step [2346/3504], Loss: 0.3496\n",
      "Epoch [1/1], Step [2347/3504], Loss: 0.3714\n",
      "Epoch [1/1], Step [2348/3504], Loss: 0.3994\n",
      "Epoch [1/1], Step [2349/3504], Loss: 0.4093\n",
      "Epoch [1/1], Step [2350/3504], Loss: 0.3987\n",
      "Epoch [1/1], Step [2351/3504], Loss: 0.4257\n",
      "Epoch [1/1], Step [2352/3504], Loss: 0.3898\n",
      "Epoch [1/1], Step [2353/3504], Loss: 0.4161\n",
      "Epoch [1/1], Step [2354/3504], Loss: 0.3610\n",
      "Epoch [1/1], Step [2355/3504], Loss: 0.3239\n",
      "Epoch [1/1], Step [2356/3504], Loss: 0.3131\n",
      "Epoch [1/1], Step [2357/3504], Loss: 0.3185\n",
      "Epoch [1/1], Step [2358/3504], Loss: 0.3970\n",
      "Epoch [1/1], Step [2359/3504], Loss: 0.2983\n",
      "Epoch [1/1], Step [2360/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [2361/3504], Loss: 0.4091\n",
      "Epoch [1/1], Step [2362/3504], Loss: 0.3037\n",
      "Epoch [1/1], Step [2363/3504], Loss: 0.3024\n",
      "Epoch [1/1], Step [2364/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [2365/3504], Loss: 0.4267\n",
      "Epoch [1/1], Step [2366/3504], Loss: 0.4562\n",
      "Epoch [1/1], Step [2367/3504], Loss: 0.3113\n",
      "Epoch [1/1], Step [2368/3504], Loss: 0.2725\n",
      "Epoch [1/1], Step [2369/3504], Loss: 0.3166\n",
      "Epoch [1/1], Step [2370/3504], Loss: 0.3277\n",
      "Epoch [1/1], Step [2371/3504], Loss: 0.3721\n",
      "Epoch [1/1], Step [2372/3504], Loss: 0.4311\n",
      "Epoch [1/1], Step [2373/3504], Loss: 0.2899\n",
      "Epoch [1/1], Step [2374/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [2375/3504], Loss: 0.4135\n",
      "Epoch [1/1], Step [2376/3504], Loss: 0.3805\n",
      "Epoch [1/1], Step [2377/3504], Loss: 0.2644\n",
      "Epoch [1/1], Step [2378/3504], Loss: 0.2874\n",
      "Epoch [1/1], Step [2379/3504], Loss: 0.3767\n",
      "Epoch [1/1], Step [2380/3504], Loss: 0.3857\n",
      "Epoch [1/1], Step [2381/3504], Loss: 0.4838\n",
      "Epoch [1/1], Step [2382/3504], Loss: 0.2797\n",
      "Epoch [1/1], Step [2383/3504], Loss: 0.4122\n",
      "Epoch [1/1], Step [2384/3504], Loss: 0.4273\n",
      "Epoch [1/1], Step [2385/3504], Loss: 0.2922\n",
      "Epoch [1/1], Step [2386/3504], Loss: 0.4735\n",
      "Epoch [1/1], Step [2387/3504], Loss: 0.4545\n",
      "Epoch [1/1], Step [2388/3504], Loss: 0.4184\n",
      "Epoch [1/1], Step [2389/3504], Loss: 0.2998\n",
      "Epoch [1/1], Step [2390/3504], Loss: 0.3078\n",
      "Epoch [1/1], Step [2391/3504], Loss: 0.3686\n",
      "Epoch [1/1], Step [2392/3504], Loss: 0.3377\n",
      "Epoch [1/1], Step [2393/3504], Loss: 0.4212\n",
      "Epoch [1/1], Step [2394/3504], Loss: 0.3819\n",
      "Epoch [1/1], Step [2395/3504], Loss: 0.3622\n",
      "Epoch [1/1], Step [2396/3504], Loss: 0.2557\n",
      "Epoch [1/1], Step [2397/3504], Loss: 0.4113\n",
      "Epoch [1/1], Step [2398/3504], Loss: 0.3346\n",
      "Epoch [1/1], Step [2399/3504], Loss: 0.3470\n",
      "Epoch [1/1], Step [2400/3504], Loss: 0.3477\n",
      "Epoch [1/1], Step [2401/3504], Loss: 0.2753\n",
      "Epoch [1/1], Step [2402/3504], Loss: 0.3778\n",
      "Epoch [1/1], Step [2403/3504], Loss: 0.3814\n",
      "Epoch [1/1], Step [2404/3504], Loss: 0.3972\n",
      "Epoch [1/1], Step [2405/3504], Loss: 0.4080\n",
      "Epoch [1/1], Step [2406/3504], Loss: 0.2898\n",
      "Epoch [1/1], Step [2407/3504], Loss: 0.3501\n",
      "Epoch [1/1], Step [2408/3504], Loss: 0.2685\n",
      "Epoch [1/1], Step [2409/3504], Loss: 0.3396\n",
      "Epoch [1/1], Step [2410/3504], Loss: 0.2851\n",
      "Epoch [1/1], Step [2411/3504], Loss: 0.4142\n",
      "Epoch [1/1], Step [2412/3504], Loss: 0.4592\n",
      "Epoch [1/1], Step [2413/3504], Loss: 0.4375\n",
      "Epoch [1/1], Step [2414/3504], Loss: 0.3568\n",
      "Epoch [1/1], Step [2415/3504], Loss: 0.2311\n",
      "Epoch [1/1], Step [2416/3504], Loss: 0.4076\n",
      "Epoch [1/1], Step [2417/3504], Loss: 0.3303\n",
      "Epoch [1/1], Step [2418/3504], Loss: 0.2948\n",
      "Epoch [1/1], Step [2419/3504], Loss: 0.3063\n",
      "Epoch [1/1], Step [2420/3504], Loss: 0.3662\n",
      "Epoch [1/1], Step [2421/3504], Loss: 0.4057\n",
      "Epoch [1/1], Step [2422/3504], Loss: 0.4049\n",
      "Epoch [1/1], Step [2423/3504], Loss: 0.3926\n",
      "Epoch [1/1], Step [2424/3504], Loss: 0.3812\n",
      "Epoch [1/1], Step [2425/3504], Loss: 0.3374\n",
      "Epoch [1/1], Step [2426/3504], Loss: 0.3688\n",
      "Epoch [1/1], Step [2427/3504], Loss: 0.3518\n",
      "Epoch [1/1], Step [2428/3504], Loss: 0.3517\n",
      "Epoch [1/1], Step [2429/3504], Loss: 0.4135\n",
      "Epoch [1/1], Step [2430/3504], Loss: 0.3260\n",
      "Epoch [1/1], Step [2431/3504], Loss: 0.4205\n",
      "Epoch [1/1], Step [2432/3504], Loss: 0.3738\n",
      "Epoch [1/1], Step [2433/3504], Loss: 0.2793\n",
      "Epoch [1/1], Step [2434/3504], Loss: 0.3956\n",
      "Epoch [1/1], Step [2435/3504], Loss: 0.3474\n",
      "Epoch [1/1], Step [2436/3504], Loss: 0.3124\n",
      "Epoch [1/1], Step [2437/3504], Loss: 0.4557\n",
      "Epoch [1/1], Step [2438/3504], Loss: 0.3599\n",
      "Epoch [1/1], Step [2439/3504], Loss: 0.3530\n",
      "Epoch [1/1], Step [2440/3504], Loss: 0.3241\n",
      "Epoch [1/1], Step [2441/3504], Loss: 0.4616\n",
      "Epoch [1/1], Step [2442/3504], Loss: 0.3033\n",
      "Epoch [1/1], Step [2443/3504], Loss: 0.4549\n",
      "Epoch [1/1], Step [2444/3504], Loss: 0.3671\n",
      "Epoch [1/1], Step [2445/3504], Loss: 0.4385\n",
      "Epoch [1/1], Step [2446/3504], Loss: 0.3449\n",
      "Epoch [1/1], Step [2447/3504], Loss: 0.3934\n",
      "Epoch [1/1], Step [2448/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [2449/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [2450/3504], Loss: 0.3733\n",
      "Epoch [1/1], Step [2451/3504], Loss: 0.2585\n",
      "Epoch [1/1], Step [2452/3504], Loss: 0.4217\n",
      "Epoch [1/1], Step [2453/3504], Loss: 0.4161\n",
      "Epoch [1/1], Step [2454/3504], Loss: 0.3447\n",
      "Epoch [1/1], Step [2455/3504], Loss: 0.3341\n",
      "Epoch [1/1], Step [2456/3504], Loss: 0.4179\n",
      "Epoch [1/1], Step [2457/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [2458/3504], Loss: 0.4230\n",
      "Epoch [1/1], Step [2459/3504], Loss: 0.4738\n",
      "Epoch [1/1], Step [2460/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [2461/3504], Loss: 0.3350\n",
      "Epoch [1/1], Step [2462/3504], Loss: 0.4146\n",
      "Epoch [1/1], Step [2463/3504], Loss: 0.3648\n",
      "Epoch [1/1], Step [2464/3504], Loss: 0.3834\n",
      "Epoch [1/1], Step [2465/3504], Loss: 0.2655\n",
      "Epoch [1/1], Step [2466/3504], Loss: 0.3303\n",
      "Epoch [1/1], Step [2467/3504], Loss: 0.3503\n",
      "Epoch [1/1], Step [2468/3504], Loss: 0.3129\n",
      "Epoch [1/1], Step [2469/3504], Loss: 0.3072\n",
      "Epoch [1/1], Step [2470/3504], Loss: 0.3577\n",
      "Epoch [1/1], Step [2471/3504], Loss: 0.5062\n",
      "Epoch [1/1], Step [2472/3504], Loss: 0.3490\n",
      "Epoch [1/1], Step [2473/3504], Loss: 0.3575\n",
      "Epoch [1/1], Step [2474/3504], Loss: 0.3387\n",
      "Epoch [1/1], Step [2475/3504], Loss: 0.3047\n",
      "Epoch [1/1], Step [2476/3504], Loss: 0.3472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [2477/3504], Loss: 0.3914\n",
      "Epoch [1/1], Step [2478/3504], Loss: 0.3978\n",
      "Epoch [1/1], Step [2479/3504], Loss: 0.3312\n",
      "Epoch [1/1], Step [2480/3504], Loss: 0.3310\n",
      "Epoch [1/1], Step [2481/3504], Loss: 0.4383\n",
      "Epoch [1/1], Step [2482/3504], Loss: 0.3147\n",
      "Epoch [1/1], Step [2483/3504], Loss: 0.4254\n",
      "Epoch [1/1], Step [2484/3504], Loss: 0.3106\n",
      "Epoch [1/1], Step [2485/3504], Loss: 0.2795\n",
      "Epoch [1/1], Step [2486/3504], Loss: 0.4368\n",
      "Epoch [1/1], Step [2487/3504], Loss: 0.4108\n",
      "Epoch [1/1], Step [2488/3504], Loss: 0.4007\n",
      "Epoch [1/1], Step [2489/3504], Loss: 0.4400\n",
      "Epoch [1/1], Step [2490/3504], Loss: 0.4355\n",
      "Epoch [1/1], Step [2491/3504], Loss: 0.4484\n",
      "Epoch [1/1], Step [2492/3504], Loss: 0.3872\n",
      "Epoch [1/1], Step [2493/3504], Loss: 0.3153\n",
      "Epoch [1/1], Step [2494/3504], Loss: 0.3812\n",
      "Epoch [1/1], Step [2495/3504], Loss: 0.4492\n",
      "Epoch [1/1], Step [2496/3504], Loss: 0.3065\n",
      "Epoch [1/1], Step [2497/3504], Loss: 0.3149\n",
      "Epoch [1/1], Step [2498/3504], Loss: 0.3729\n",
      "Epoch [1/1], Step [2499/3504], Loss: 0.4580\n",
      "Epoch [1/1], Step [2500/3504], Loss: 0.3577\n",
      "Epoch [1/1], Step [2501/3504], Loss: 0.3165\n",
      "Epoch [1/1], Step [2502/3504], Loss: 0.2989\n",
      "Epoch [1/1], Step [2503/3504], Loss: 0.3246\n",
      "Epoch [1/1], Step [2504/3504], Loss: 0.4901\n",
      "Epoch [1/1], Step [2505/3504], Loss: 0.4011\n",
      "Epoch [1/1], Step [2506/3504], Loss: 0.3328\n",
      "Epoch [1/1], Step [2507/3504], Loss: 0.2415\n",
      "Epoch [1/1], Step [2508/3504], Loss: 0.3519\n",
      "Epoch [1/1], Step [2509/3504], Loss: 0.4298\n",
      "Epoch [1/1], Step [2510/3504], Loss: 0.4053\n",
      "Epoch [1/1], Step [2511/3504], Loss: 0.4010\n",
      "Epoch [1/1], Step [2512/3504], Loss: 0.3503\n",
      "Epoch [1/1], Step [2513/3504], Loss: 0.3663\n",
      "Epoch [1/1], Step [2514/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [2515/3504], Loss: 0.3147\n",
      "Epoch [1/1], Step [2516/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [2517/3504], Loss: 0.3477\n",
      "Epoch [1/1], Step [2518/3504], Loss: 0.4547\n",
      "Epoch [1/1], Step [2519/3504], Loss: 0.3449\n",
      "Epoch [1/1], Step [2520/3504], Loss: 0.3008\n",
      "Epoch [1/1], Step [2521/3504], Loss: 0.3998\n",
      "Epoch [1/1], Step [2522/3504], Loss: 0.3117\n",
      "Epoch [1/1], Step [2523/3504], Loss: 0.3473\n",
      "Epoch [1/1], Step [2524/3504], Loss: 0.2927\n",
      "Epoch [1/1], Step [2525/3504], Loss: 0.2614\n",
      "Epoch [1/1], Step [2526/3504], Loss: 0.4303\n",
      "Epoch [1/1], Step [2527/3504], Loss: 0.3294\n",
      "Epoch [1/1], Step [2528/3504], Loss: 0.3966\n",
      "Epoch [1/1], Step [2529/3504], Loss: 0.3507\n",
      "Epoch [1/1], Step [2530/3504], Loss: 0.3817\n",
      "Epoch [1/1], Step [2531/3504], Loss: 0.4040\n",
      "Epoch [1/1], Step [2532/3504], Loss: 0.3659\n",
      "Epoch [1/1], Step [2533/3504], Loss: 0.3445\n",
      "Epoch [1/1], Step [2534/3504], Loss: 0.4214\n",
      "Epoch [1/1], Step [2535/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [2536/3504], Loss: 0.3692\n",
      "Epoch [1/1], Step [2537/3504], Loss: 0.3301\n",
      "Epoch [1/1], Step [2538/3504], Loss: 0.3332\n",
      "Epoch [1/1], Step [2539/3504], Loss: 0.2642\n",
      "Epoch [1/1], Step [2540/3504], Loss: 0.3828\n",
      "Epoch [1/1], Step [2541/3504], Loss: 0.3125\n",
      "Epoch [1/1], Step [2542/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [2543/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [2544/3504], Loss: 0.3684\n",
      "Epoch [1/1], Step [2545/3504], Loss: 0.3235\n",
      "Epoch [1/1], Step [2546/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [2547/3504], Loss: 0.3878\n",
      "Epoch [1/1], Step [2548/3504], Loss: 0.4340\n",
      "Epoch [1/1], Step [2549/3504], Loss: 0.3795\n",
      "Epoch [1/1], Step [2550/3504], Loss: 0.4023\n",
      "Epoch [1/1], Step [2551/3504], Loss: 0.3455\n",
      "Epoch [1/1], Step [2552/3504], Loss: 0.2790\n",
      "Epoch [1/1], Step [2553/3504], Loss: 0.4020\n",
      "Epoch [1/1], Step [2554/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [2555/3504], Loss: 0.3967\n",
      "Epoch [1/1], Step [2556/3504], Loss: 0.4973\n",
      "Epoch [1/1], Step [2557/3504], Loss: 0.4101\n",
      "Epoch [1/1], Step [2558/3504], Loss: 0.3447\n",
      "Epoch [1/1], Step [2559/3504], Loss: 0.3676\n",
      "Epoch [1/1], Step [2560/3504], Loss: 0.3304\n",
      "Epoch [1/1], Step [2561/3504], Loss: 0.4063\n",
      "Epoch [1/1], Step [2562/3504], Loss: 0.3554\n",
      "Epoch [1/1], Step [2563/3504], Loss: 0.2817\n",
      "Epoch [1/1], Step [2564/3504], Loss: 0.4010\n",
      "Epoch [1/1], Step [2565/3504], Loss: 0.3117\n",
      "Epoch [1/1], Step [2566/3504], Loss: 0.3604\n",
      "Epoch [1/1], Step [2567/3504], Loss: 0.4036\n",
      "Epoch [1/1], Step [2568/3504], Loss: 0.3188\n",
      "Epoch [1/1], Step [2569/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [2570/3504], Loss: 0.3496\n",
      "Epoch [1/1], Step [2571/3504], Loss: 0.3602\n",
      "Epoch [1/1], Step [2572/3504], Loss: 0.4683\n",
      "Epoch [1/1], Step [2573/3504], Loss: 0.3416\n",
      "Epoch [1/1], Step [2574/3504], Loss: 0.3150\n",
      "Epoch [1/1], Step [2575/3504], Loss: 0.2754\n",
      "Epoch [1/1], Step [2576/3504], Loss: 0.3552\n",
      "Epoch [1/1], Step [2577/3504], Loss: 0.3689\n",
      "Epoch [1/1], Step [2578/3504], Loss: 0.3151\n",
      "Epoch [1/1], Step [2579/3504], Loss: 0.3904\n",
      "Epoch [1/1], Step [2580/3504], Loss: 0.3154\n",
      "Epoch [1/1], Step [2581/3504], Loss: 0.3966\n",
      "Epoch [1/1], Step [2582/3504], Loss: 0.3937\n",
      "Epoch [1/1], Step [2583/3504], Loss: 0.3722\n",
      "Epoch [1/1], Step [2584/3504], Loss: 0.2929\n",
      "Epoch [1/1], Step [2585/3504], Loss: 0.3256\n",
      "Epoch [1/1], Step [2586/3504], Loss: 0.3964\n",
      "Epoch [1/1], Step [2587/3504], Loss: 0.4680\n",
      "Epoch [1/1], Step [2588/3504], Loss: 0.3791\n",
      "Epoch [1/1], Step [2589/3504], Loss: 0.3306\n",
      "Epoch [1/1], Step [2590/3504], Loss: 0.3337\n",
      "Epoch [1/1], Step [2591/3504], Loss: 0.3861\n",
      "Epoch [1/1], Step [2592/3504], Loss: 0.3475\n",
      "Epoch [1/1], Step [2593/3504], Loss: 0.3656\n",
      "Epoch [1/1], Step [2594/3504], Loss: 0.3237\n",
      "Epoch [1/1], Step [2595/3504], Loss: 0.3335\n",
      "Epoch [1/1], Step [2596/3504], Loss: 0.3011\n",
      "Epoch [1/1], Step [2597/3504], Loss: 0.3600\n",
      "Epoch [1/1], Step [2598/3504], Loss: 0.3690\n",
      "Epoch [1/1], Step [2599/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [2600/3504], Loss: 0.4537\n",
      "Epoch [1/1], Step [2601/3504], Loss: 0.3181\n",
      "Epoch [1/1], Step [2602/3504], Loss: 0.3556\n",
      "Epoch [1/1], Step [2603/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [2604/3504], Loss: 0.3373\n",
      "Epoch [1/1], Step [2605/3504], Loss: 0.3547\n",
      "Epoch [1/1], Step [2606/3504], Loss: 0.2938\n",
      "Epoch [1/1], Step [2607/3504], Loss: 0.3218\n",
      "Epoch [1/1], Step [2608/3504], Loss: 0.3704\n",
      "Epoch [1/1], Step [2609/3504], Loss: 0.3158\n",
      "Epoch [1/1], Step [2610/3504], Loss: 0.2849\n",
      "Epoch [1/1], Step [2611/3504], Loss: 0.4200\n",
      "Epoch [1/1], Step [2612/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [2613/3504], Loss: 0.3334\n",
      "Epoch [1/1], Step [2614/3504], Loss: 0.3489\n",
      "Epoch [1/1], Step [2615/3504], Loss: 0.2812\n",
      "Epoch [1/1], Step [2616/3504], Loss: 0.3904\n",
      "Epoch [1/1], Step [2617/3504], Loss: 0.3819\n",
      "Epoch [1/1], Step [2618/3504], Loss: 0.3186\n",
      "Epoch [1/1], Step [2619/3504], Loss: 0.3331\n",
      "Epoch [1/1], Step [2620/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [2621/3504], Loss: 0.4497\n",
      "Epoch [1/1], Step [2622/3504], Loss: 0.2554\n",
      "Epoch [1/1], Step [2623/3504], Loss: 0.3780\n",
      "Epoch [1/1], Step [2624/3504], Loss: 0.2266\n",
      "Epoch [1/1], Step [2625/3504], Loss: 0.3277\n",
      "Epoch [1/1], Step [2626/3504], Loss: 0.3278\n",
      "Epoch [1/1], Step [2627/3504], Loss: 0.3122\n",
      "Epoch [1/1], Step [2628/3504], Loss: 0.3554\n",
      "Epoch [1/1], Step [2629/3504], Loss: 0.3603\n",
      "Epoch [1/1], Step [2630/3504], Loss: 0.3304\n",
      "Epoch [1/1], Step [2631/3504], Loss: 0.4513\n",
      "Epoch [1/1], Step [2632/3504], Loss: 0.5314\n",
      "Epoch [1/1], Step [2633/3504], Loss: 0.4029\n",
      "Epoch [1/1], Step [2634/3504], Loss: 0.4216\n",
      "Epoch [1/1], Step [2635/3504], Loss: 0.3534\n",
      "Epoch [1/1], Step [2636/3504], Loss: 0.4011\n",
      "Epoch [1/1], Step [2637/3504], Loss: 0.3346\n",
      "Epoch [1/1], Step [2638/3504], Loss: 0.3525\n",
      "Epoch [1/1], Step [2639/3504], Loss: 0.3202\n",
      "Epoch [1/1], Step [2640/3504], Loss: 0.3447\n",
      "Epoch [1/1], Step [2641/3504], Loss: 0.3206\n",
      "Epoch [1/1], Step [2642/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [2643/3504], Loss: 0.3728\n",
      "Epoch [1/1], Step [2644/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [2645/3504], Loss: 0.2732\n",
      "Epoch [1/1], Step [2646/3504], Loss: 0.3750\n",
      "Epoch [1/1], Step [2647/3504], Loss: 0.4748\n",
      "Epoch [1/1], Step [2648/3504], Loss: 0.3375\n",
      "Epoch [1/1], Step [2649/3504], Loss: 0.3599\n",
      "Epoch [1/1], Step [2650/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [2651/3504], Loss: 0.3507\n",
      "Epoch [1/1], Step [2652/3504], Loss: 0.2947\n",
      "Epoch [1/1], Step [2653/3504], Loss: 0.3227\n",
      "Epoch [1/1], Step [2654/3504], Loss: 0.3315\n",
      "Epoch [1/1], Step [2655/3504], Loss: 0.2885\n",
      "Epoch [1/1], Step [2656/3504], Loss: 0.3725\n",
      "Epoch [1/1], Step [2657/3504], Loss: 0.3577\n",
      "Epoch [1/1], Step [2658/3504], Loss: 0.3684\n",
      "Epoch [1/1], Step [2659/3504], Loss: 0.3550\n",
      "Epoch [1/1], Step [2660/3504], Loss: 0.3742\n",
      "Epoch [1/1], Step [2661/3504], Loss: 0.3877\n",
      "Epoch [1/1], Step [2662/3504], Loss: 0.2955\n",
      "Epoch [1/1], Step [2663/3504], Loss: 0.3290\n",
      "Epoch [1/1], Step [2664/3504], Loss: 0.4260\n",
      "Epoch [1/1], Step [2665/3504], Loss: 0.3695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [2666/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [2667/3504], Loss: 0.3929\n",
      "Epoch [1/1], Step [2668/3504], Loss: 0.3123\n",
      "Epoch [1/1], Step [2669/3504], Loss: 0.4900\n",
      "Epoch [1/1], Step [2670/3504], Loss: 0.3400\n",
      "Epoch [1/1], Step [2671/3504], Loss: 0.3250\n",
      "Epoch [1/1], Step [2672/3504], Loss: 0.4354\n",
      "Epoch [1/1], Step [2673/3504], Loss: 0.4433\n",
      "Epoch [1/1], Step [2674/3504], Loss: 0.3584\n",
      "Epoch [1/1], Step [2675/3504], Loss: 0.4177\n",
      "Epoch [1/1], Step [2676/3504], Loss: 0.3527\n",
      "Epoch [1/1], Step [2677/3504], Loss: 0.3818\n",
      "Epoch [1/1], Step [2678/3504], Loss: 0.4246\n",
      "Epoch [1/1], Step [2679/3504], Loss: 0.3253\n",
      "Epoch [1/1], Step [2680/3504], Loss: 0.3329\n",
      "Epoch [1/1], Step [2681/3504], Loss: 0.3331\n",
      "Epoch [1/1], Step [2682/3504], Loss: 0.2663\n",
      "Epoch [1/1], Step [2683/3504], Loss: 0.3179\n",
      "Epoch [1/1], Step [2684/3504], Loss: 0.3196\n",
      "Epoch [1/1], Step [2685/3504], Loss: 0.3958\n",
      "Epoch [1/1], Step [2686/3504], Loss: 0.5959\n",
      "Epoch [1/1], Step [2687/3504], Loss: 0.3964\n",
      "Epoch [1/1], Step [2688/3504], Loss: 0.3816\n",
      "Epoch [1/1], Step [2689/3504], Loss: 0.3621\n",
      "Epoch [1/1], Step [2690/3504], Loss: 0.3243\n",
      "Epoch [1/1], Step [2691/3504], Loss: 0.3587\n",
      "Epoch [1/1], Step [2692/3504], Loss: 0.3955\n",
      "Epoch [1/1], Step [2693/3504], Loss: 0.3115\n",
      "Epoch [1/1], Step [2694/3504], Loss: 0.4008\n",
      "Epoch [1/1], Step [2695/3504], Loss: 0.3246\n",
      "Epoch [1/1], Step [2696/3504], Loss: 0.4063\n",
      "Epoch [1/1], Step [2697/3504], Loss: 0.3091\n",
      "Epoch [1/1], Step [2698/3504], Loss: 0.4478\n",
      "Epoch [1/1], Step [2699/3504], Loss: 0.4533\n",
      "Epoch [1/1], Step [2700/3504], Loss: 0.3360\n",
      "Epoch [1/1], Step [2701/3504], Loss: 0.3864\n",
      "Epoch [1/1], Step [2702/3504], Loss: 0.3535\n",
      "Epoch [1/1], Step [2703/3504], Loss: 0.4475\n",
      "Epoch [1/1], Step [2704/3504], Loss: 0.3879\n",
      "Epoch [1/1], Step [2705/3504], Loss: 0.3890\n",
      "Epoch [1/1], Step [2706/3504], Loss: 0.4649\n",
      "Epoch [1/1], Step [2707/3504], Loss: 0.5331\n",
      "Epoch [1/1], Step [2708/3504], Loss: 0.3834\n",
      "Epoch [1/1], Step [2709/3504], Loss: 0.3411\n",
      "Epoch [1/1], Step [2710/3504], Loss: 0.4117\n",
      "Epoch [1/1], Step [2711/3504], Loss: 0.3542\n",
      "Epoch [1/1], Step [2712/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [2713/3504], Loss: 0.3629\n",
      "Epoch [1/1], Step [2714/3504], Loss: 0.2881\n",
      "Epoch [1/1], Step [2715/3504], Loss: 0.3701\n",
      "Epoch [1/1], Step [2716/3504], Loss: 0.3888\n",
      "Epoch [1/1], Step [2717/3504], Loss: 0.3391\n",
      "Epoch [1/1], Step [2718/3504], Loss: 0.3008\n",
      "Epoch [1/1], Step [2719/3504], Loss: 0.3442\n",
      "Epoch [1/1], Step [2720/3504], Loss: 0.3948\n",
      "Epoch [1/1], Step [2721/3504], Loss: 0.4398\n",
      "Epoch [1/1], Step [2722/3504], Loss: 0.4343\n",
      "Epoch [1/1], Step [2723/3504], Loss: 0.3817\n",
      "Epoch [1/1], Step [2724/3504], Loss: 0.3326\n",
      "Epoch [1/1], Step [2725/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [2726/3504], Loss: 0.4420\n",
      "Epoch [1/1], Step [2727/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [2728/3504], Loss: 0.4819\n",
      "Epoch [1/1], Step [2729/3504], Loss: 0.4613\n",
      "Epoch [1/1], Step [2730/3504], Loss: 0.4946\n",
      "Epoch [1/1], Step [2731/3504], Loss: 0.3708\n",
      "Epoch [1/1], Step [2732/3504], Loss: 0.3894\n",
      "Epoch [1/1], Step [2733/3504], Loss: 0.4308\n",
      "Epoch [1/1], Step [2734/3504], Loss: 0.4065\n",
      "Epoch [1/1], Step [2735/3504], Loss: 0.3894\n",
      "Epoch [1/1], Step [2736/3504], Loss: 0.4220\n",
      "Epoch [1/1], Step [2737/3504], Loss: 0.3542\n",
      "Epoch [1/1], Step [2738/3504], Loss: 0.3362\n",
      "Epoch [1/1], Step [2739/3504], Loss: 0.4559\n",
      "Epoch [1/1], Step [2740/3504], Loss: 0.4253\n",
      "Epoch [1/1], Step [2741/3504], Loss: 0.4281\n",
      "Epoch [1/1], Step [2742/3504], Loss: 0.3527\n",
      "Epoch [1/1], Step [2743/3504], Loss: 0.3475\n",
      "Epoch [1/1], Step [2744/3504], Loss: 0.3751\n",
      "Epoch [1/1], Step [2745/3504], Loss: 0.3809\n",
      "Epoch [1/1], Step [2746/3504], Loss: 0.3465\n",
      "Epoch [1/1], Step [2747/3504], Loss: 0.4303\n",
      "Epoch [1/1], Step [2748/3504], Loss: 0.4397\n",
      "Epoch [1/1], Step [2749/3504], Loss: 0.3471\n",
      "Epoch [1/1], Step [2750/3504], Loss: 0.3246\n",
      "Epoch [1/1], Step [2751/3504], Loss: 0.3987\n",
      "Epoch [1/1], Step [2752/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [2753/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [2754/3504], Loss: 0.4196\n",
      "Epoch [1/1], Step [2755/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [2756/3504], Loss: 0.2825\n",
      "Epoch [1/1], Step [2757/3504], Loss: 0.3469\n",
      "Epoch [1/1], Step [2758/3504], Loss: 0.3657\n",
      "Epoch [1/1], Step [2759/3504], Loss: 0.3594\n",
      "Epoch [1/1], Step [2760/3504], Loss: 0.4183\n",
      "Epoch [1/1], Step [2761/3504], Loss: 0.4013\n",
      "Epoch [1/1], Step [2762/3504], Loss: 0.3256\n",
      "Epoch [1/1], Step [2763/3504], Loss: 0.2835\n",
      "Epoch [1/1], Step [2764/3504], Loss: 0.4406\n",
      "Epoch [1/1], Step [2765/3504], Loss: 0.3195\n",
      "Epoch [1/1], Step [2766/3504], Loss: 0.3442\n",
      "Epoch [1/1], Step [2767/3504], Loss: 0.3067\n",
      "Epoch [1/1], Step [2768/3504], Loss: 0.3918\n",
      "Epoch [1/1], Step [2769/3504], Loss: 0.3432\n",
      "Epoch [1/1], Step [2770/3504], Loss: 0.3274\n",
      "Epoch [1/1], Step [2771/3504], Loss: 0.4950\n",
      "Epoch [1/1], Step [2772/3504], Loss: 0.2687\n",
      "Epoch [1/1], Step [2773/3504], Loss: 0.3204\n",
      "Epoch [1/1], Step [2774/3504], Loss: 0.5114\n",
      "Epoch [1/1], Step [2775/3504], Loss: 0.3896\n",
      "Epoch [1/1], Step [2776/3504], Loss: 0.3191\n",
      "Epoch [1/1], Step [2777/3504], Loss: 0.4114\n",
      "Epoch [1/1], Step [2778/3504], Loss: 0.3246\n",
      "Epoch [1/1], Step [2779/3504], Loss: 0.3939\n",
      "Epoch [1/1], Step [2780/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [2781/3504], Loss: 0.4124\n",
      "Epoch [1/1], Step [2782/3504], Loss: 0.4296\n",
      "Epoch [1/1], Step [2783/3504], Loss: 0.2763\n",
      "Epoch [1/1], Step [2784/3504], Loss: 0.3666\n",
      "Epoch [1/1], Step [2785/3504], Loss: 0.3293\n",
      "Epoch [1/1], Step [2786/3504], Loss: 0.4262\n",
      "Epoch [1/1], Step [2787/3504], Loss: 0.3443\n",
      "Epoch [1/1], Step [2788/3504], Loss: 0.3588\n",
      "Epoch [1/1], Step [2789/3504], Loss: 0.2976\n",
      "Epoch [1/1], Step [2790/3504], Loss: 0.3872\n",
      "Epoch [1/1], Step [2791/3504], Loss: 0.2642\n",
      "Epoch [1/1], Step [2792/3504], Loss: 0.3103\n",
      "Epoch [1/1], Step [2793/3504], Loss: 0.4976\n",
      "Epoch [1/1], Step [2794/3504], Loss: 0.4074\n",
      "Epoch [1/1], Step [2795/3504], Loss: 0.3758\n",
      "Epoch [1/1], Step [2796/3504], Loss: 0.3478\n",
      "Epoch [1/1], Step [2797/3504], Loss: 0.4767\n",
      "Epoch [1/1], Step [2798/3504], Loss: 0.2821\n",
      "Epoch [1/1], Step [2799/3504], Loss: 0.4179\n",
      "Epoch [1/1], Step [2800/3504], Loss: 0.3371\n",
      "Epoch [1/1], Step [2801/3504], Loss: 0.3568\n",
      "Epoch [1/1], Step [2802/3504], Loss: 0.3656\n",
      "Epoch [1/1], Step [2803/3504], Loss: 0.3064\n",
      "Epoch [1/1], Step [2804/3504], Loss: 0.3998\n",
      "Epoch [1/1], Step [2805/3504], Loss: 0.3833\n",
      "Epoch [1/1], Step [2806/3504], Loss: 0.3681\n",
      "Epoch [1/1], Step [2807/3504], Loss: 0.3969\n",
      "Epoch [1/1], Step [2808/3504], Loss: 0.3705\n",
      "Epoch [1/1], Step [2809/3504], Loss: 0.3180\n",
      "Epoch [1/1], Step [2810/3504], Loss: 0.2375\n",
      "Epoch [1/1], Step [2811/3504], Loss: 0.3381\n",
      "Epoch [1/1], Step [2812/3504], Loss: 0.4974\n",
      "Epoch [1/1], Step [2813/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [2814/3504], Loss: 0.3560\n",
      "Epoch [1/1], Step [2815/3504], Loss: 0.3252\n",
      "Epoch [1/1], Step [2816/3504], Loss: 0.4401\n",
      "Epoch [1/1], Step [2817/3504], Loss: 0.4207\n",
      "Epoch [1/1], Step [2818/3504], Loss: 0.2727\n",
      "Epoch [1/1], Step [2819/3504], Loss: 0.4146\n",
      "Epoch [1/1], Step [2820/3504], Loss: 0.3969\n",
      "Epoch [1/1], Step [2821/3504], Loss: 0.3999\n",
      "Epoch [1/1], Step [2822/3504], Loss: 0.3203\n",
      "Epoch [1/1], Step [2823/3504], Loss: 0.4115\n",
      "Epoch [1/1], Step [2824/3504], Loss: 0.3868\n",
      "Epoch [1/1], Step [2825/3504], Loss: 0.3917\n",
      "Epoch [1/1], Step [2826/3504], Loss: 0.3556\n",
      "Epoch [1/1], Step [2827/3504], Loss: 0.3855\n",
      "Epoch [1/1], Step [2828/3504], Loss: 0.4224\n",
      "Epoch [1/1], Step [2829/3504], Loss: 0.4151\n",
      "Epoch [1/1], Step [2830/3504], Loss: 0.3476\n",
      "Epoch [1/1], Step [2831/3504], Loss: 0.4636\n",
      "Epoch [1/1], Step [2832/3504], Loss: 0.3079\n",
      "Epoch [1/1], Step [2833/3504], Loss: 0.4600\n",
      "Epoch [1/1], Step [2834/3504], Loss: 0.3116\n",
      "Epoch [1/1], Step [2835/3504], Loss: 0.3590\n",
      "Epoch [1/1], Step [2836/3504], Loss: 0.4541\n",
      "Epoch [1/1], Step [2837/3504], Loss: 0.4560\n",
      "Epoch [1/1], Step [2838/3504], Loss: 0.3149\n",
      "Epoch [1/1], Step [2839/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [2840/3504], Loss: 0.3154\n",
      "Epoch [1/1], Step [2841/3504], Loss: 0.3175\n",
      "Epoch [1/1], Step [2842/3504], Loss: 0.2859\n",
      "Epoch [1/1], Step [2843/3504], Loss: 0.4283\n",
      "Epoch [1/1], Step [2844/3504], Loss: 0.3016\n",
      "Epoch [1/1], Step [2845/3504], Loss: 0.2988\n",
      "Epoch [1/1], Step [2846/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [2847/3504], Loss: 0.3723\n",
      "Epoch [1/1], Step [2848/3504], Loss: 0.4024\n",
      "Epoch [1/1], Step [2849/3504], Loss: 0.2971\n",
      "Epoch [1/1], Step [2850/3504], Loss: 0.3807\n",
      "Epoch [1/1], Step [2851/3504], Loss: 0.5144\n",
      "Epoch [1/1], Step [2852/3504], Loss: 0.2759\n",
      "Epoch [1/1], Step [2853/3504], Loss: 0.3827\n",
      "Epoch [1/1], Step [2854/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [2855/3504], Loss: 0.3871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [2856/3504], Loss: 0.3784\n",
      "Epoch [1/1], Step [2857/3504], Loss: 0.3548\n",
      "Epoch [1/1], Step [2858/3504], Loss: 0.3611\n",
      "Epoch [1/1], Step [2859/3504], Loss: 0.4089\n",
      "Epoch [1/1], Step [2860/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [2861/3504], Loss: 0.3586\n",
      "Epoch [1/1], Step [2862/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [2863/3504], Loss: 0.2484\n",
      "Epoch [1/1], Step [2864/3504], Loss: 0.3923\n",
      "Epoch [1/1], Step [2865/3504], Loss: 0.3978\n",
      "Epoch [1/1], Step [2866/3504], Loss: 0.3851\n",
      "Epoch [1/1], Step [2867/3504], Loss: 0.3159\n",
      "Epoch [1/1], Step [2868/3504], Loss: 0.3434\n",
      "Epoch [1/1], Step [2869/3504], Loss: 0.3889\n",
      "Epoch [1/1], Step [2870/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [2871/3504], Loss: 0.2653\n",
      "Epoch [1/1], Step [2872/3504], Loss: 0.4613\n",
      "Epoch [1/1], Step [2873/3504], Loss: 0.3381\n",
      "Epoch [1/1], Step [2874/3504], Loss: 0.3316\n",
      "Epoch [1/1], Step [2875/3504], Loss: 0.4075\n",
      "Epoch [1/1], Step [2876/3504], Loss: 0.2374\n",
      "Epoch [1/1], Step [2877/3504], Loss: 0.3560\n",
      "Epoch [1/1], Step [2878/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [2879/3504], Loss: 0.4642\n",
      "Epoch [1/1], Step [2880/3504], Loss: 0.2865\n",
      "Epoch [1/1], Step [2881/3504], Loss: 0.3870\n",
      "Epoch [1/1], Step [2882/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [2883/3504], Loss: 0.3724\n",
      "Epoch [1/1], Step [2884/3504], Loss: 0.3800\n",
      "Epoch [1/1], Step [2885/3504], Loss: 0.2999\n",
      "Epoch [1/1], Step [2886/3504], Loss: 0.3216\n",
      "Epoch [1/1], Step [2887/3504], Loss: 0.3587\n",
      "Epoch [1/1], Step [2888/3504], Loss: 0.3553\n",
      "Epoch [1/1], Step [2889/3504], Loss: 0.3737\n",
      "Epoch [1/1], Step [2890/3504], Loss: 0.3411\n",
      "Epoch [1/1], Step [2891/3504], Loss: 0.2655\n",
      "Epoch [1/1], Step [2892/3504], Loss: 0.3604\n",
      "Epoch [1/1], Step [2893/3504], Loss: 0.3274\n",
      "Epoch [1/1], Step [2894/3504], Loss: 0.4414\n",
      "Epoch [1/1], Step [2895/3504], Loss: 0.4539\n",
      "Epoch [1/1], Step [2896/3504], Loss: 0.3039\n",
      "Epoch [1/1], Step [2897/3504], Loss: 0.3281\n",
      "Epoch [1/1], Step [2898/3504], Loss: 0.3135\n",
      "Epoch [1/1], Step [2899/3504], Loss: 0.3776\n",
      "Epoch [1/1], Step [2900/3504], Loss: 0.3496\n",
      "Epoch [1/1], Step [2901/3504], Loss: 0.2879\n",
      "Epoch [1/1], Step [2902/3504], Loss: 0.4101\n",
      "Epoch [1/1], Step [2903/3504], Loss: 0.4133\n",
      "Epoch [1/1], Step [2904/3504], Loss: 0.3731\n",
      "Epoch [1/1], Step [2905/3504], Loss: 0.3887\n",
      "Epoch [1/1], Step [2906/3504], Loss: 0.2992\n",
      "Epoch [1/1], Step [2907/3504], Loss: 0.2884\n",
      "Epoch [1/1], Step [2908/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [2909/3504], Loss: 0.3454\n",
      "Epoch [1/1], Step [2910/3504], Loss: 0.2809\n",
      "Epoch [1/1], Step [2911/3504], Loss: 0.3929\n",
      "Epoch [1/1], Step [2912/3504], Loss: 0.2721\n",
      "Epoch [1/1], Step [2913/3504], Loss: 0.3926\n",
      "Epoch [1/1], Step [2914/3504], Loss: 0.3705\n",
      "Epoch [1/1], Step [2915/3504], Loss: 0.3067\n",
      "Epoch [1/1], Step [2916/3504], Loss: 0.3236\n",
      "Epoch [1/1], Step [2917/3504], Loss: 0.4493\n",
      "Epoch [1/1], Step [2918/3504], Loss: 0.3237\n",
      "Epoch [1/1], Step [2919/3504], Loss: 0.3231\n",
      "Epoch [1/1], Step [2920/3504], Loss: 0.3593\n",
      "Epoch [1/1], Step [2921/3504], Loss: 0.3549\n",
      "Epoch [1/1], Step [2922/3504], Loss: 0.3949\n",
      "Epoch [1/1], Step [2923/3504], Loss: 0.3738\n",
      "Epoch [1/1], Step [2924/3504], Loss: 0.3764\n",
      "Epoch [1/1], Step [2925/3504], Loss: 0.3730\n",
      "Epoch [1/1], Step [2926/3504], Loss: 0.3903\n",
      "Epoch [1/1], Step [2927/3504], Loss: 0.4096\n",
      "Epoch [1/1], Step [2928/3504], Loss: 0.3418\n",
      "Epoch [1/1], Step [2929/3504], Loss: 0.3276\n",
      "Epoch [1/1], Step [2930/3504], Loss: 0.3394\n",
      "Epoch [1/1], Step [2931/3504], Loss: 0.3521\n",
      "Epoch [1/1], Step [2932/3504], Loss: 0.3393\n",
      "Epoch [1/1], Step [2933/3504], Loss: 0.3777\n",
      "Epoch [1/1], Step [2934/3504], Loss: 0.3319\n",
      "Epoch [1/1], Step [2935/3504], Loss: 0.3311\n",
      "Epoch [1/1], Step [2936/3504], Loss: 0.3652\n",
      "Epoch [1/1], Step [2937/3504], Loss: 0.3513\n",
      "Epoch [1/1], Step [2938/3504], Loss: 0.2799\n",
      "Epoch [1/1], Step [2939/3504], Loss: 0.3409\n",
      "Epoch [1/1], Step [2940/3504], Loss: 0.3766\n",
      "Epoch [1/1], Step [2941/3504], Loss: 0.3610\n",
      "Epoch [1/1], Step [2942/3504], Loss: 0.4786\n",
      "Epoch [1/1], Step [2943/3504], Loss: 0.3716\n",
      "Epoch [1/1], Step [2944/3504], Loss: 0.3363\n",
      "Epoch [1/1], Step [2945/3504], Loss: 0.3597\n",
      "Epoch [1/1], Step [2946/3504], Loss: 0.3209\n",
      "Epoch [1/1], Step [2947/3504], Loss: 0.3221\n",
      "Epoch [1/1], Step [2948/3504], Loss: 0.2847\n",
      "Epoch [1/1], Step [2949/3504], Loss: 0.3235\n",
      "Epoch [1/1], Step [2950/3504], Loss: 0.3381\n",
      "Epoch [1/1], Step [2951/3504], Loss: 0.3423\n",
      "Epoch [1/1], Step [2952/3504], Loss: 0.3530\n",
      "Epoch [1/1], Step [2953/3504], Loss: 0.4653\n",
      "Epoch [1/1], Step [2954/3504], Loss: 0.3351\n",
      "Epoch [1/1], Step [2955/3504], Loss: 0.3534\n",
      "Epoch [1/1], Step [2956/3504], Loss: 0.3449\n",
      "Epoch [1/1], Step [2957/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [2958/3504], Loss: 0.4447\n",
      "Epoch [1/1], Step [2959/3504], Loss: 0.2984\n",
      "Epoch [1/1], Step [2960/3504], Loss: 0.3115\n",
      "Epoch [1/1], Step [2961/3504], Loss: 0.2865\n",
      "Epoch [1/1], Step [2962/3504], Loss: 0.4212\n",
      "Epoch [1/1], Step [2963/3504], Loss: 0.3353\n",
      "Epoch [1/1], Step [2964/3504], Loss: 0.4331\n",
      "Epoch [1/1], Step [2965/3504], Loss: 0.4556\n",
      "Epoch [1/1], Step [2966/3504], Loss: 0.2844\n",
      "Epoch [1/1], Step [2967/3504], Loss: 0.2934\n",
      "Epoch [1/1], Step [2968/3504], Loss: 0.3606\n",
      "Epoch [1/1], Step [2969/3504], Loss: 0.4277\n",
      "Epoch [1/1], Step [2970/3504], Loss: 0.4158\n",
      "Epoch [1/1], Step [2971/3504], Loss: 0.3265\n",
      "Epoch [1/1], Step [2972/3504], Loss: 0.3822\n",
      "Epoch [1/1], Step [2973/3504], Loss: 0.3130\n",
      "Epoch [1/1], Step [2974/3504], Loss: 0.3532\n",
      "Epoch [1/1], Step [2975/3504], Loss: 0.3858\n",
      "Epoch [1/1], Step [2976/3504], Loss: 0.3454\n",
      "Epoch [1/1], Step [2977/3504], Loss: 0.3665\n",
      "Epoch [1/1], Step [2978/3504], Loss: 0.3821\n",
      "Epoch [1/1], Step [2979/3504], Loss: 0.3620\n",
      "Epoch [1/1], Step [2980/3504], Loss: 0.3798\n",
      "Epoch [1/1], Step [2981/3504], Loss: 0.2982\n",
      "Epoch [1/1], Step [2982/3504], Loss: 0.3219\n",
      "Epoch [1/1], Step [2983/3504], Loss: 0.3869\n",
      "Epoch [1/1], Step [2984/3504], Loss: 0.3469\n",
      "Epoch [1/1], Step [2985/3504], Loss: 0.3047\n",
      "Epoch [1/1], Step [2986/3504], Loss: 0.2946\n",
      "Epoch [1/1], Step [2987/3504], Loss: 0.3041\n",
      "Epoch [1/1], Step [2988/3504], Loss: 0.3847\n",
      "Epoch [1/1], Step [2989/3504], Loss: 0.2614\n",
      "Epoch [1/1], Step [2990/3504], Loss: 0.3099\n",
      "Epoch [1/1], Step [2991/3504], Loss: 0.4404\n",
      "Epoch [1/1], Step [2992/3504], Loss: 0.2996\n",
      "Epoch [1/1], Step [2993/3504], Loss: 0.3405\n",
      "Epoch [1/1], Step [2994/3504], Loss: 0.3915\n",
      "Epoch [1/1], Step [2995/3504], Loss: 0.4511\n",
      "Epoch [1/1], Step [2996/3504], Loss: 0.3733\n",
      "Epoch [1/1], Step [2997/3504], Loss: 0.4597\n",
      "Epoch [1/1], Step [2998/3504], Loss: 0.3443\n",
      "Epoch [1/1], Step [2999/3504], Loss: 0.4555\n",
      "Epoch [1/1], Step [3000/3504], Loss: 0.2932\n",
      "Epoch [1/1], Step [3001/3504], Loss: 0.2934\n",
      "Epoch [1/1], Step [3002/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [3003/3504], Loss: 0.3727\n",
      "Epoch [1/1], Step [3004/3504], Loss: 0.3579\n",
      "Epoch [1/1], Step [3005/3504], Loss: 0.3752\n",
      "Epoch [1/1], Step [3006/3504], Loss: 0.4124\n",
      "Epoch [1/1], Step [3007/3504], Loss: 0.3063\n",
      "Epoch [1/1], Step [3008/3504], Loss: 0.3782\n",
      "Epoch [1/1], Step [3009/3504], Loss: 0.3989\n",
      "Epoch [1/1], Step [3010/3504], Loss: 0.4169\n",
      "Epoch [1/1], Step [3011/3504], Loss: 0.3501\n",
      "Epoch [1/1], Step [3012/3504], Loss: 0.3336\n",
      "Epoch [1/1], Step [3013/3504], Loss: 0.4358\n",
      "Epoch [1/1], Step [3014/3504], Loss: 0.4099\n",
      "Epoch [1/1], Step [3015/3504], Loss: 0.3954\n",
      "Epoch [1/1], Step [3016/3504], Loss: 0.3835\n",
      "Epoch [1/1], Step [3017/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [3018/3504], Loss: 0.4127\n",
      "Epoch [1/1], Step [3019/3504], Loss: 0.2866\n",
      "Epoch [1/1], Step [3020/3504], Loss: 0.3850\n",
      "Epoch [1/1], Step [3021/3504], Loss: 0.4019\n",
      "Epoch [1/1], Step [3022/3504], Loss: 0.3525\n",
      "Epoch [1/1], Step [3023/3504], Loss: 0.2585\n",
      "Epoch [1/1], Step [3024/3504], Loss: 0.3059\n",
      "Epoch [1/1], Step [3025/3504], Loss: 0.3507\n",
      "Epoch [1/1], Step [3026/3504], Loss: 0.3102\n",
      "Epoch [1/1], Step [3027/3504], Loss: 0.3458\n",
      "Epoch [1/1], Step [3028/3504], Loss: 0.3582\n",
      "Epoch [1/1], Step [3029/3504], Loss: 0.3350\n",
      "Epoch [1/1], Step [3030/3504], Loss: 0.3029\n",
      "Epoch [1/1], Step [3031/3504], Loss: 0.3451\n",
      "Epoch [1/1], Step [3032/3504], Loss: 0.4160\n",
      "Epoch [1/1], Step [3033/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [3034/3504], Loss: 0.2928\n",
      "Epoch [1/1], Step [3035/3504], Loss: 0.3516\n",
      "Epoch [1/1], Step [3036/3504], Loss: 0.3428\n",
      "Epoch [1/1], Step [3037/3504], Loss: 0.4277\n",
      "Epoch [1/1], Step [3038/3504], Loss: 0.3007\n",
      "Epoch [1/1], Step [3039/3504], Loss: 0.3940\n",
      "Epoch [1/1], Step [3040/3504], Loss: 0.4309\n",
      "Epoch [1/1], Step [3041/3504], Loss: 0.4202\n",
      "Epoch [1/1], Step [3042/3504], Loss: 0.3687\n",
      "Epoch [1/1], Step [3043/3504], Loss: 0.3789\n",
      "Epoch [1/1], Step [3044/3504], Loss: 0.3582\n",
      "Epoch [1/1], Step [3045/3504], Loss: 0.3426\n",
      "Epoch [1/1], Step [3046/3504], Loss: 0.2903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [3047/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [3048/3504], Loss: 0.3893\n",
      "Epoch [1/1], Step [3049/3504], Loss: 0.3168\n",
      "Epoch [1/1], Step [3050/3504], Loss: 0.3104\n",
      "Epoch [1/1], Step [3051/3504], Loss: 0.3697\n",
      "Epoch [1/1], Step [3052/3504], Loss: 0.4366\n",
      "Epoch [1/1], Step [3053/3504], Loss: 0.3170\n",
      "Epoch [1/1], Step [3054/3504], Loss: 0.3948\n",
      "Epoch [1/1], Step [3055/3504], Loss: 0.3293\n",
      "Epoch [1/1], Step [3056/3504], Loss: 0.3317\n",
      "Epoch [1/1], Step [3057/3504], Loss: 0.3180\n",
      "Epoch [1/1], Step [3058/3504], Loss: 0.3358\n",
      "Epoch [1/1], Step [3059/3504], Loss: 0.2569\n",
      "Epoch [1/1], Step [3060/3504], Loss: 0.3406\n",
      "Epoch [1/1], Step [3061/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [3062/3504], Loss: 0.3236\n",
      "Epoch [1/1], Step [3063/3504], Loss: 0.4149\n",
      "Epoch [1/1], Step [3064/3504], Loss: 0.4480\n",
      "Epoch [1/1], Step [3065/3504], Loss: 0.3692\n",
      "Epoch [1/1], Step [3066/3504], Loss: 0.4103\n",
      "Epoch [1/1], Step [3067/3504], Loss: 0.3536\n",
      "Epoch [1/1], Step [3068/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [3069/3504], Loss: 0.2899\n",
      "Epoch [1/1], Step [3070/3504], Loss: 0.4158\n",
      "Epoch [1/1], Step [3071/3504], Loss: 0.3647\n",
      "Epoch [1/1], Step [3072/3504], Loss: 0.3460\n",
      "Epoch [1/1], Step [3073/3504], Loss: 0.3895\n",
      "Epoch [1/1], Step [3074/3504], Loss: 0.2791\n",
      "Epoch [1/1], Step [3075/3504], Loss: 0.3278\n",
      "Epoch [1/1], Step [3076/3504], Loss: 0.3659\n",
      "Epoch [1/1], Step [3077/3504], Loss: 0.3473\n",
      "Epoch [1/1], Step [3078/3504], Loss: 0.3724\n",
      "Epoch [1/1], Step [3079/3504], Loss: 0.3657\n",
      "Epoch [1/1], Step [3080/3504], Loss: 0.3378\n",
      "Epoch [1/1], Step [3081/3504], Loss: 0.3331\n",
      "Epoch [1/1], Step [3082/3504], Loss: 0.4332\n",
      "Epoch [1/1], Step [3083/3504], Loss: 0.2725\n",
      "Epoch [1/1], Step [3084/3504], Loss: 0.3099\n",
      "Epoch [1/1], Step [3085/3504], Loss: 0.4147\n",
      "Epoch [1/1], Step [3086/3504], Loss: 0.3697\n",
      "Epoch [1/1], Step [3087/3504], Loss: 0.3229\n",
      "Epoch [1/1], Step [3088/3504], Loss: 0.3596\n",
      "Epoch [1/1], Step [3089/3504], Loss: 0.3646\n",
      "Epoch [1/1], Step [3090/3504], Loss: 0.3462\n",
      "Epoch [1/1], Step [3091/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [3092/3504], Loss: 0.3861\n",
      "Epoch [1/1], Step [3093/3504], Loss: 0.4316\n",
      "Epoch [1/1], Step [3094/3504], Loss: 0.3071\n",
      "Epoch [1/1], Step [3095/3504], Loss: 0.2920\n",
      "Epoch [1/1], Step [3096/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [3097/3504], Loss: 0.4049\n",
      "Epoch [1/1], Step [3098/3504], Loss: 0.3112\n",
      "Epoch [1/1], Step [3099/3504], Loss: 0.3712\n",
      "Epoch [1/1], Step [3100/3504], Loss: 0.3958\n",
      "Epoch [1/1], Step [3101/3504], Loss: 0.4310\n",
      "Epoch [1/1], Step [3102/3504], Loss: 0.3881\n",
      "Epoch [1/1], Step [3103/3504], Loss: 0.3341\n",
      "Epoch [1/1], Step [3104/3504], Loss: 0.4083\n",
      "Epoch [1/1], Step [3105/3504], Loss: 0.5100\n",
      "Epoch [1/1], Step [3106/3504], Loss: 0.2600\n",
      "Epoch [1/1], Step [3107/3504], Loss: 0.3569\n",
      "Epoch [1/1], Step [3108/3504], Loss: 0.2937\n",
      "Epoch [1/1], Step [3109/3504], Loss: 0.3027\n",
      "Epoch [1/1], Step [3110/3504], Loss: 0.3706\n",
      "Epoch [1/1], Step [3111/3504], Loss: 0.3521\n",
      "Epoch [1/1], Step [3112/3504], Loss: 0.3268\n",
      "Epoch [1/1], Step [3113/3504], Loss: 0.3184\n",
      "Epoch [1/1], Step [3114/3504], Loss: 0.3232\n",
      "Epoch [1/1], Step [3115/3504], Loss: 0.2725\n",
      "Epoch [1/1], Step [3116/3504], Loss: 0.3703\n",
      "Epoch [1/1], Step [3117/3504], Loss: 0.3437\n",
      "Epoch [1/1], Step [3118/3504], Loss: 0.2528\n",
      "Epoch [1/1], Step [3119/3504], Loss: 0.2898\n",
      "Epoch [1/1], Step [3120/3504], Loss: 0.3425\n",
      "Epoch [1/1], Step [3121/3504], Loss: 0.3630\n",
      "Epoch [1/1], Step [3122/3504], Loss: 0.3319\n",
      "Epoch [1/1], Step [3123/3504], Loss: 0.3509\n",
      "Epoch [1/1], Step [3124/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [3125/3504], Loss: 0.3392\n",
      "Epoch [1/1], Step [3126/3504], Loss: 0.4218\n",
      "Epoch [1/1], Step [3127/3504], Loss: 0.4331\n",
      "Epoch [1/1], Step [3128/3504], Loss: 0.3716\n",
      "Epoch [1/1], Step [3129/3504], Loss: 0.3550\n",
      "Epoch [1/1], Step [3130/3504], Loss: 0.3835\n",
      "Epoch [1/1], Step [3131/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [3132/3504], Loss: 0.4058\n",
      "Epoch [1/1], Step [3133/3504], Loss: 0.2945\n",
      "Epoch [1/1], Step [3134/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [3135/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [3136/3504], Loss: 0.3284\n",
      "Epoch [1/1], Step [3137/3504], Loss: 0.3477\n",
      "Epoch [1/1], Step [3138/3504], Loss: 0.3209\n",
      "Epoch [1/1], Step [3139/3504], Loss: 0.3150\n",
      "Epoch [1/1], Step [3140/3504], Loss: 0.3226\n",
      "Epoch [1/1], Step [3141/3504], Loss: 0.3340\n",
      "Epoch [1/1], Step [3142/3504], Loss: 0.3551\n",
      "Epoch [1/1], Step [3143/3504], Loss: 0.2853\n",
      "Epoch [1/1], Step [3144/3504], Loss: 0.3978\n",
      "Epoch [1/1], Step [3145/3504], Loss: 0.3482\n",
      "Epoch [1/1], Step [3146/3504], Loss: 0.3111\n",
      "Epoch [1/1], Step [3147/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [3148/3504], Loss: 0.3170\n",
      "Epoch [1/1], Step [3149/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [3150/3504], Loss: 0.4095\n",
      "Epoch [1/1], Step [3151/3504], Loss: 0.3654\n",
      "Epoch [1/1], Step [3152/3504], Loss: 0.2640\n",
      "Epoch [1/1], Step [3153/3504], Loss: 0.4200\n",
      "Epoch [1/1], Step [3154/3504], Loss: 0.3576\n",
      "Epoch [1/1], Step [3155/3504], Loss: 0.3649\n",
      "Epoch [1/1], Step [3156/3504], Loss: 0.3325\n",
      "Epoch [1/1], Step [3157/3504], Loss: 0.2802\n",
      "Epoch [1/1], Step [3158/3504], Loss: 0.3693\n",
      "Epoch [1/1], Step [3159/3504], Loss: 0.3926\n",
      "Epoch [1/1], Step [3160/3504], Loss: 0.2597\n",
      "Epoch [1/1], Step [3161/3504], Loss: 0.3479\n",
      "Epoch [1/1], Step [3162/3504], Loss: 0.5030\n",
      "Epoch [1/1], Step [3163/3504], Loss: 0.3539\n",
      "Epoch [1/1], Step [3164/3504], Loss: 0.2646\n",
      "Epoch [1/1], Step [3165/3504], Loss: 0.3511\n",
      "Epoch [1/1], Step [3166/3504], Loss: 0.3564\n",
      "Epoch [1/1], Step [3167/3504], Loss: 0.3101\n",
      "Epoch [1/1], Step [3168/3504], Loss: 0.3052\n",
      "Epoch [1/1], Step [3169/3504], Loss: 0.3549\n",
      "Epoch [1/1], Step [3170/3504], Loss: 0.3598\n",
      "Epoch [1/1], Step [3171/3504], Loss: 0.4163\n",
      "Epoch [1/1], Step [3172/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [3173/3504], Loss: 0.3709\n",
      "Epoch [1/1], Step [3174/3504], Loss: 0.3089\n",
      "Epoch [1/1], Step [3175/3504], Loss: 0.3153\n",
      "Epoch [1/1], Step [3176/3504], Loss: 0.3954\n",
      "Epoch [1/1], Step [3177/3504], Loss: 0.4772\n",
      "Epoch [1/1], Step [3178/3504], Loss: 0.3651\n",
      "Epoch [1/1], Step [3179/3504], Loss: 0.2759\n",
      "Epoch [1/1], Step [3180/3504], Loss: 0.4276\n",
      "Epoch [1/1], Step [3181/3504], Loss: 0.4485\n",
      "Epoch [1/1], Step [3182/3504], Loss: 0.3553\n",
      "Epoch [1/1], Step [3183/3504], Loss: 0.4336\n",
      "Epoch [1/1], Step [3184/3504], Loss: 0.2916\n",
      "Epoch [1/1], Step [3185/3504], Loss: 0.3476\n",
      "Epoch [1/1], Step [3186/3504], Loss: 0.4072\n",
      "Epoch [1/1], Step [3187/3504], Loss: 0.4768\n",
      "Epoch [1/1], Step [3188/3504], Loss: 0.3983\n",
      "Epoch [1/1], Step [3189/3504], Loss: 0.3758\n",
      "Epoch [1/1], Step [3190/3504], Loss: 0.4696\n",
      "Epoch [1/1], Step [3191/3504], Loss: 0.4565\n",
      "Epoch [1/1], Step [3192/3504], Loss: 0.3683\n",
      "Epoch [1/1], Step [3193/3504], Loss: 0.3876\n",
      "Epoch [1/1], Step [3194/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [3195/3504], Loss: 0.3137\n",
      "Epoch [1/1], Step [3196/3504], Loss: 0.4236\n",
      "Epoch [1/1], Step [3197/3504], Loss: 0.3840\n",
      "Epoch [1/1], Step [3198/3504], Loss: 0.4741\n",
      "Epoch [1/1], Step [3199/3504], Loss: 0.3661\n",
      "Epoch [1/1], Step [3200/3504], Loss: 0.3287\n",
      "Epoch [1/1], Step [3201/3504], Loss: 0.3067\n",
      "Epoch [1/1], Step [3202/3504], Loss: 0.4202\n",
      "Epoch [1/1], Step [3203/3504], Loss: 0.4291\n",
      "Epoch [1/1], Step [3204/3504], Loss: 0.2962\n",
      "Epoch [1/1], Step [3205/3504], Loss: 0.2803\n",
      "Epoch [1/1], Step [3206/3504], Loss: 0.3593\n",
      "Epoch [1/1], Step [3207/3504], Loss: 0.3935\n",
      "Epoch [1/1], Step [3208/3504], Loss: 0.2924\n",
      "Epoch [1/1], Step [3209/3504], Loss: 0.3747\n",
      "Epoch [1/1], Step [3210/3504], Loss: 0.3722\n",
      "Epoch [1/1], Step [3211/3504], Loss: 0.3160\n",
      "Epoch [1/1], Step [3212/3504], Loss: 0.3053\n",
      "Epoch [1/1], Step [3213/3504], Loss: 0.3596\n",
      "Epoch [1/1], Step [3214/3504], Loss: 0.4247\n",
      "Epoch [1/1], Step [3215/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [3216/3504], Loss: 0.3384\n",
      "Epoch [1/1], Step [3217/3504], Loss: 0.3621\n",
      "Epoch [1/1], Step [3218/3504], Loss: 0.3931\n",
      "Epoch [1/1], Step [3219/3504], Loss: 0.2442\n",
      "Epoch [1/1], Step [3220/3504], Loss: 0.4946\n",
      "Epoch [1/1], Step [3221/3504], Loss: 0.3323\n",
      "Epoch [1/1], Step [3222/3504], Loss: 0.3444\n",
      "Epoch [1/1], Step [3223/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [3224/3504], Loss: 0.3027\n",
      "Epoch [1/1], Step [3225/3504], Loss: 0.3026\n",
      "Epoch [1/1], Step [3226/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [3227/3504], Loss: 0.4137\n",
      "Epoch [1/1], Step [3228/3504], Loss: 0.3263\n",
      "Epoch [1/1], Step [3229/3504], Loss: 0.3874\n",
      "Epoch [1/1], Step [3230/3504], Loss: 0.4533\n",
      "Epoch [1/1], Step [3231/3504], Loss: 0.2887\n",
      "Epoch [1/1], Step [3232/3504], Loss: 0.3288\n",
      "Epoch [1/1], Step [3233/3504], Loss: 0.4322\n",
      "Epoch [1/1], Step [3234/3504], Loss: 0.3536\n",
      "Epoch [1/1], Step [3235/3504], Loss: 0.3926\n",
      "Epoch [1/1], Step [3236/3504], Loss: 0.3825\n",
      "Epoch [1/1], Step [3237/3504], Loss: 0.3022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [3238/3504], Loss: 0.3373\n",
      "Epoch [1/1], Step [3239/3504], Loss: 0.3286\n",
      "Epoch [1/1], Step [3240/3504], Loss: 0.3991\n",
      "Epoch [1/1], Step [3241/3504], Loss: 0.3840\n",
      "Epoch [1/1], Step [3242/3504], Loss: 0.2999\n",
      "Epoch [1/1], Step [3243/3504], Loss: 0.3438\n",
      "Epoch [1/1], Step [3244/3504], Loss: 0.4189\n",
      "Epoch [1/1], Step [3245/3504], Loss: 0.4176\n",
      "Epoch [1/1], Step [3246/3504], Loss: 0.4214\n",
      "Epoch [1/1], Step [3247/3504], Loss: 0.3691\n",
      "Epoch [1/1], Step [3248/3504], Loss: 0.4139\n",
      "Epoch [1/1], Step [3249/3504], Loss: 0.3802\n",
      "Epoch [1/1], Step [3250/3504], Loss: 0.2689\n",
      "Epoch [1/1], Step [3251/3504], Loss: 0.3293\n",
      "Epoch [1/1], Step [3252/3504], Loss: 0.3540\n",
      "Epoch [1/1], Step [3253/3504], Loss: 0.3726\n",
      "Epoch [1/1], Step [3254/3504], Loss: 0.4798\n",
      "Epoch [1/1], Step [3255/3504], Loss: 0.3445\n",
      "Epoch [1/1], Step [3256/3504], Loss: 0.3225\n",
      "Epoch [1/1], Step [3257/3504], Loss: 0.3346\n",
      "Epoch [1/1], Step [3258/3504], Loss: 0.3970\n",
      "Epoch [1/1], Step [3259/3504], Loss: 0.6679\n",
      "Epoch [1/1], Step [3260/3504], Loss: 0.4716\n",
      "Epoch [1/1], Step [3261/3504], Loss: 0.3017\n",
      "Epoch [1/1], Step [3262/3504], Loss: 0.3977\n",
      "Epoch [1/1], Step [3263/3504], Loss: 0.3472\n",
      "Epoch [1/1], Step [3264/3504], Loss: 0.3570\n",
      "Epoch [1/1], Step [3265/3504], Loss: 0.3326\n",
      "Epoch [1/1], Step [3266/3504], Loss: 0.4451\n",
      "Epoch [1/1], Step [3267/3504], Loss: 0.3561\n",
      "Epoch [1/1], Step [3268/3504], Loss: 0.4034\n",
      "Epoch [1/1], Step [3269/3504], Loss: 0.3412\n",
      "Epoch [1/1], Step [3270/3504], Loss: 0.4048\n",
      "Epoch [1/1], Step [3271/3504], Loss: 0.3583\n",
      "Epoch [1/1], Step [3272/3504], Loss: 0.3663\n",
      "Epoch [1/1], Step [3273/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [3274/3504], Loss: 0.3263\n",
      "Epoch [1/1], Step [3275/3504], Loss: 0.3335\n",
      "Epoch [1/1], Step [3276/3504], Loss: 0.3836\n",
      "Epoch [1/1], Step [3277/3504], Loss: 0.4363\n",
      "Epoch [1/1], Step [3278/3504], Loss: 0.4051\n",
      "Epoch [1/1], Step [3279/3504], Loss: 0.3744\n",
      "Epoch [1/1], Step [3280/3504], Loss: 0.3351\n",
      "Epoch [1/1], Step [3281/3504], Loss: 0.4700\n",
      "Epoch [1/1], Step [3282/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [3283/3504], Loss: 0.3052\n",
      "Epoch [1/1], Step [3284/3504], Loss: 0.3499\n",
      "Epoch [1/1], Step [3285/3504], Loss: 0.4082\n",
      "Epoch [1/1], Step [3286/3504], Loss: 0.4167\n",
      "Epoch [1/1], Step [3287/3504], Loss: 0.3672\n",
      "Epoch [1/1], Step [3288/3504], Loss: 0.2986\n",
      "Epoch [1/1], Step [3289/3504], Loss: 0.3628\n",
      "Epoch [1/1], Step [3290/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [3291/3504], Loss: 0.2881\n",
      "Epoch [1/1], Step [3292/3504], Loss: 0.2956\n",
      "Epoch [1/1], Step [3293/3504], Loss: 0.3929\n",
      "Epoch [1/1], Step [3294/3504], Loss: 0.3339\n",
      "Epoch [1/1], Step [3295/3504], Loss: 0.3492\n",
      "Epoch [1/1], Step [3296/3504], Loss: 0.3552\n",
      "Epoch [1/1], Step [3297/3504], Loss: 0.3525\n",
      "Epoch [1/1], Step [3298/3504], Loss: 0.3761\n",
      "Epoch [1/1], Step [3299/3504], Loss: 0.3333\n",
      "Epoch [1/1], Step [3300/3504], Loss: 0.3923\n",
      "Epoch [1/1], Step [3301/3504], Loss: 0.4181\n",
      "Epoch [1/1], Step [3302/3504], Loss: 0.3772\n",
      "Epoch [1/1], Step [3303/3504], Loss: 0.3283\n",
      "Epoch [1/1], Step [3304/3504], Loss: 0.3571\n",
      "Epoch [1/1], Step [3305/3504], Loss: 0.3827\n",
      "Epoch [1/1], Step [3306/3504], Loss: 0.3708\n",
      "Epoch [1/1], Step [3307/3504], Loss: 0.3322\n",
      "Epoch [1/1], Step [3308/3504], Loss: 0.3144\n",
      "Epoch [1/1], Step [3309/3504], Loss: 0.4201\n",
      "Epoch [1/1], Step [3310/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [3311/3504], Loss: 0.4161\n",
      "Epoch [1/1], Step [3312/3504], Loss: 0.3068\n",
      "Epoch [1/1], Step [3313/3504], Loss: 0.3178\n",
      "Epoch [1/1], Step [3314/3504], Loss: 0.2917\n",
      "Epoch [1/1], Step [3315/3504], Loss: 0.3937\n",
      "Epoch [1/1], Step [3316/3504], Loss: 0.4254\n",
      "Epoch [1/1], Step [3317/3504], Loss: 0.3570\n",
      "Epoch [1/1], Step [3318/3504], Loss: 0.3028\n",
      "Epoch [1/1], Step [3319/3504], Loss: 0.2339\n",
      "Epoch [1/1], Step [3320/3504], Loss: 0.2530\n",
      "Epoch [1/1], Step [3321/3504], Loss: 0.4860\n",
      "Epoch [1/1], Step [3322/3504], Loss: 0.3116\n",
      "Epoch [1/1], Step [3323/3504], Loss: 0.3222\n",
      "Epoch [1/1], Step [3324/3504], Loss: 0.3500\n",
      "Epoch [1/1], Step [3325/3504], Loss: 0.2860\n",
      "Epoch [1/1], Step [3326/3504], Loss: 0.3273\n",
      "Epoch [1/1], Step [3327/3504], Loss: 0.4120\n",
      "Epoch [1/1], Step [3328/3504], Loss: 0.3778\n",
      "Epoch [1/1], Step [3329/3504], Loss: 0.6278\n",
      "Epoch [1/1], Step [3330/3504], Loss: 0.2859\n",
      "Epoch [1/1], Step [3331/3504], Loss: 0.3778\n",
      "Epoch [1/1], Step [3332/3504], Loss: 0.3421\n",
      "Epoch [1/1], Step [3333/3504], Loss: 0.2900\n",
      "Epoch [1/1], Step [3334/3504], Loss: 0.4303\n",
      "Epoch [1/1], Step [3335/3504], Loss: 0.3959\n",
      "Epoch [1/1], Step [3336/3504], Loss: 0.2886\n",
      "Epoch [1/1], Step [3337/3504], Loss: 0.3246\n",
      "Epoch [1/1], Step [3338/3504], Loss: 0.2899\n",
      "Epoch [1/1], Step [3339/3504], Loss: 0.3084\n",
      "Epoch [1/1], Step [3340/3504], Loss: 0.2691\n",
      "Epoch [1/1], Step [3341/3504], Loss: 0.4451\n",
      "Epoch [1/1], Step [3342/3504], Loss: 0.3277\n",
      "Epoch [1/1], Step [3343/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [3344/3504], Loss: 0.3265\n",
      "Epoch [1/1], Step [3345/3504], Loss: 0.3399\n",
      "Epoch [1/1], Step [3346/3504], Loss: 0.4541\n",
      "Epoch [1/1], Step [3347/3504], Loss: 0.3646\n",
      "Epoch [1/1], Step [3348/3504], Loss: 0.3332\n",
      "Epoch [1/1], Step [3349/3504], Loss: 0.4793\n",
      "Epoch [1/1], Step [3350/3504], Loss: 0.4463\n",
      "Epoch [1/1], Step [3351/3504], Loss: 0.3723\n",
      "Epoch [1/1], Step [3352/3504], Loss: 0.3155\n",
      "Epoch [1/1], Step [3353/3504], Loss: 0.3609\n",
      "Epoch [1/1], Step [3354/3504], Loss: 0.3867\n",
      "Epoch [1/1], Step [3355/3504], Loss: 0.3638\n",
      "Epoch [1/1], Step [3356/3504], Loss: 0.3290\n",
      "Epoch [1/1], Step [3357/3504], Loss: 0.3452\n",
      "Epoch [1/1], Step [3358/3504], Loss: 0.4376\n",
      "Epoch [1/1], Step [3359/3504], Loss: 0.4007\n",
      "Epoch [1/1], Step [3360/3504], Loss: 0.3280\n",
      "Epoch [1/1], Step [3361/3504], Loss: 0.3936\n",
      "Epoch [1/1], Step [3362/3504], Loss: 0.2809\n",
      "Epoch [1/1], Step [3363/3504], Loss: 0.3050\n",
      "Epoch [1/1], Step [3364/3504], Loss: 0.2689\n",
      "Epoch [1/1], Step [3365/3504], Loss: 0.3829\n",
      "Epoch [1/1], Step [3366/3504], Loss: 0.3198\n",
      "Epoch [1/1], Step [3367/3504], Loss: 0.4291\n",
      "Epoch [1/1], Step [3368/3504], Loss: 0.4386\n",
      "Epoch [1/1], Step [3369/3504], Loss: 0.3441\n",
      "Epoch [1/1], Step [3370/3504], Loss: 0.3270\n",
      "Epoch [1/1], Step [3371/3504], Loss: 0.4759\n",
      "Epoch [1/1], Step [3372/3504], Loss: 0.3760\n",
      "Epoch [1/1], Step [3373/3504], Loss: 0.3053\n",
      "Epoch [1/1], Step [3374/3504], Loss: 0.3995\n",
      "Epoch [1/1], Step [3375/3504], Loss: 0.3583\n",
      "Epoch [1/1], Step [3376/3504], Loss: 0.2696\n",
      "Epoch [1/1], Step [3377/3504], Loss: 0.4254\n",
      "Epoch [1/1], Step [3378/3504], Loss: 0.2772\n",
      "Epoch [1/1], Step [3379/3504], Loss: 0.3519\n",
      "Epoch [1/1], Step [3380/3504], Loss: 0.3643\n",
      "Epoch [1/1], Step [3381/3504], Loss: 0.3214\n",
      "Epoch [1/1], Step [3382/3504], Loss: 0.3464\n",
      "Epoch [1/1], Step [3383/3504], Loss: 0.3451\n",
      "Epoch [1/1], Step [3384/3504], Loss: 0.3918\n",
      "Epoch [1/1], Step [3385/3504], Loss: 0.3236\n",
      "Epoch [1/1], Step [3386/3504], Loss: 0.3135\n",
      "Epoch [1/1], Step [3387/3504], Loss: 0.3051\n",
      "Epoch [1/1], Step [3388/3504], Loss: 0.4541\n",
      "Epoch [1/1], Step [3389/3504], Loss: 0.4071\n",
      "Epoch [1/1], Step [3390/3504], Loss: 0.2738\n",
      "Epoch [1/1], Step [3391/3504], Loss: 0.3152\n",
      "Epoch [1/1], Step [3392/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [3393/3504], Loss: 0.3451\n",
      "Epoch [1/1], Step [3394/3504], Loss: 0.3188\n",
      "Epoch [1/1], Step [3395/3504], Loss: 0.4064\n",
      "Epoch [1/1], Step [3396/3504], Loss: 0.2999\n",
      "Epoch [1/1], Step [3397/3504], Loss: 0.3387\n",
      "Epoch [1/1], Step [3398/3504], Loss: 0.3322\n",
      "Epoch [1/1], Step [3399/3504], Loss: 0.3597\n",
      "Epoch [1/1], Step [3400/3504], Loss: 0.3597\n",
      "Epoch [1/1], Step [3401/3504], Loss: 0.2858\n",
      "Epoch [1/1], Step [3402/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [3403/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [3404/3504], Loss: 0.3097\n",
      "Epoch [1/1], Step [3405/3504], Loss: 0.2809\n",
      "Epoch [1/1], Step [3406/3504], Loss: 0.3000\n",
      "Epoch [1/1], Step [3407/3504], Loss: 0.3333\n",
      "Epoch [1/1], Step [3408/3504], Loss: 0.2612\n",
      "Epoch [1/1], Step [3409/3504], Loss: 0.4314\n",
      "Epoch [1/1], Step [3410/3504], Loss: 0.4145\n",
      "Epoch [1/1], Step [3411/3504], Loss: 0.3723\n",
      "Epoch [1/1], Step [3412/3504], Loss: 0.4242\n",
      "Epoch [1/1], Step [3413/3504], Loss: 0.3377\n",
      "Epoch [1/1], Step [3414/3504], Loss: 0.3094\n",
      "Epoch [1/1], Step [3415/3504], Loss: 0.3800\n",
      "Epoch [1/1], Step [3416/3504], Loss: 0.4699\n",
      "Epoch [1/1], Step [3417/3504], Loss: 0.3109\n",
      "Epoch [1/1], Step [3418/3504], Loss: 0.3719\n",
      "Epoch [1/1], Step [3419/3504], Loss: 0.3645\n",
      "Epoch [1/1], Step [3420/3504], Loss: 0.3131\n",
      "Epoch [1/1], Step [3421/3504], Loss: 0.3518\n",
      "Epoch [1/1], Step [3422/3504], Loss: 0.2988\n",
      "Epoch [1/1], Step [3423/3504], Loss: 0.3334\n",
      "Epoch [1/1], Step [3424/3504], Loss: 0.3163\n",
      "Epoch [1/1], Step [3425/3504], Loss: 0.3522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [3426/3504], Loss: 0.2972\n",
      "Epoch [1/1], Step [3427/3504], Loss: 0.3202\n",
      "Epoch [1/1], Step [3428/3504], Loss: 0.3498\n",
      "Epoch [1/1], Step [3429/3504], Loss: 0.3712\n",
      "Epoch [1/1], Step [3430/3504], Loss: 0.3324\n",
      "Epoch [1/1], Step [3431/3504], Loss: 0.2613\n",
      "Epoch [1/1], Step [3432/3504], Loss: 0.3652\n",
      "Epoch [1/1], Step [3433/3504], Loss: 0.2847\n",
      "Epoch [1/1], Step [3434/3504], Loss: 0.3315\n",
      "Epoch [1/1], Step [3435/3504], Loss: 0.3712\n",
      "Epoch [1/1], Step [3436/3504], Loss: 0.4178\n",
      "Epoch [1/1], Step [3437/3504], Loss: 0.3433\n",
      "Epoch [1/1], Step [3438/3504], Loss: 0.3848\n",
      "Epoch [1/1], Step [3439/3504], Loss: 0.3367\n",
      "Epoch [1/1], Step [3440/3504], Loss: 0.3295\n",
      "Epoch [1/1], Step [3441/3504], Loss: 0.3810\n",
      "Epoch [1/1], Step [3442/3504], Loss: 0.4316\n",
      "Epoch [1/1], Step [3443/3504], Loss: 0.3771\n",
      "Epoch [1/1], Step [3444/3504], Loss: 0.4287\n",
      "Epoch [1/1], Step [3445/3504], Loss: 0.4020\n",
      "Epoch [1/1], Step [3446/3504], Loss: 0.4295\n",
      "Epoch [1/1], Step [3447/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [3448/3504], Loss: 0.3697\n",
      "Epoch [1/1], Step [3449/3504], Loss: 0.4564\n",
      "Epoch [1/1], Step [3450/3504], Loss: 0.3641\n",
      "Epoch [1/1], Step [3451/3504], Loss: 0.3451\n",
      "Epoch [1/1], Step [3452/3504], Loss: 0.3661\n",
      "Epoch [1/1], Step [3453/3504], Loss: 0.3307\n",
      "Epoch [1/1], Step [3454/3504], Loss: 0.3064\n",
      "Epoch [1/1], Step [3455/3504], Loss: 0.3814\n",
      "Epoch [1/1], Step [3456/3504], Loss: 0.4044\n",
      "Epoch [1/1], Step [3457/3504], Loss: 0.3970\n",
      "Epoch [1/1], Step [3458/3504], Loss: 0.2600\n",
      "Epoch [1/1], Step [3459/3504], Loss: 0.3742\n",
      "Epoch [1/1], Step [3460/3504], Loss: 0.2808\n",
      "Epoch [1/1], Step [3461/3504], Loss: 0.3170\n",
      "Epoch [1/1], Step [3462/3504], Loss: 0.3582\n",
      "Epoch [1/1], Step [3463/3504], Loss: 0.4066\n",
      "Epoch [1/1], Step [3464/3504], Loss: 0.4427\n",
      "Epoch [1/1], Step [3465/3504], Loss: 0.3137\n",
      "Epoch [1/1], Step [3466/3504], Loss: 0.3902\n",
      "Epoch [1/1], Step [3467/3504], Loss: 0.3998\n",
      "Epoch [1/1], Step [3468/3504], Loss: 0.2984\n",
      "Epoch [1/1], Step [3469/3504], Loss: 0.2093\n",
      "Epoch [1/1], Step [3470/3504], Loss: 0.2927\n",
      "Epoch [1/1], Step [3471/3504], Loss: 0.4357\n",
      "Epoch [1/1], Step [3472/3504], Loss: 0.3828\n",
      "Epoch [1/1], Step [3473/3504], Loss: 0.4168\n",
      "Epoch [1/1], Step [3474/3504], Loss: 0.2941\n",
      "Epoch [1/1], Step [3475/3504], Loss: 0.3811\n",
      "Epoch [1/1], Step [3476/3504], Loss: 0.4041\n",
      "Epoch [1/1], Step [3477/3504], Loss: 0.3309\n",
      "Epoch [1/1], Step [3478/3504], Loss: 0.3892\n",
      "Epoch [1/1], Step [3479/3504], Loss: 0.3589\n",
      "Epoch [1/1], Step [3480/3504], Loss: 0.3061\n",
      "Epoch [1/1], Step [3481/3504], Loss: 0.3132\n",
      "Epoch [1/1], Step [3482/3504], Loss: 0.3252\n",
      "Epoch [1/1], Step [3483/3504], Loss: 0.3321\n",
      "Epoch [1/1], Step [3484/3504], Loss: 0.3635\n",
      "Epoch [1/1], Step [3485/3504], Loss: 0.3396\n",
      "Epoch [1/1], Step [3486/3504], Loss: 0.3157\n",
      "Epoch [1/1], Step [3487/3504], Loss: 0.2944\n",
      "Epoch [1/1], Step [3488/3504], Loss: 0.2766\n",
      "Epoch [1/1], Step [3489/3504], Loss: 0.4226\n",
      "Epoch [1/1], Step [3490/3504], Loss: 0.3238\n",
      "Epoch [1/1], Step [3491/3504], Loss: 0.3502\n",
      "Epoch [1/1], Step [3492/3504], Loss: 0.3499\n",
      "Epoch [1/1], Step [3493/3504], Loss: 0.2980\n",
      "Epoch [1/1], Step [3494/3504], Loss: 0.3076\n",
      "Epoch [1/1], Step [3495/3504], Loss: 0.4051\n",
      "Epoch [1/1], Step [3496/3504], Loss: 0.4497\n",
      "Epoch [1/1], Step [3497/3504], Loss: 0.4056\n",
      "Epoch [1/1], Step [3498/3504], Loss: 0.2742\n",
      "Epoch [1/1], Step [3499/3504], Loss: 0.2979\n",
      "Epoch [1/1], Step [3500/3504], Loss: 0.4046\n",
      "Epoch [1/1], Step [3501/3504], Loss: 0.2449\n",
      "Epoch [1/1], Step [3502/3504], Loss: 0.4850\n",
      "Epoch [1/1], Step [3503/3504], Loss: 0.3168\n",
      "Epoch [1/1], Step [3504/3504], Loss: 0.1481\n",
      "Epoch [1/1], Train Accuracy: 0.8512\n",
      "Epoch [1/1], Test Accuracy: 0.8518\n"
     ]
    }
   ],
   "source": [
    "train_accs, test_accs = trainer(small_model, train_dataloader, test_dataloader, 1, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b71548a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PaperModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(970, 30) # 972 - 2 features\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(30, 30)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(30, 30)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        self.linear4 = torch.nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.linear4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba315b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_model = PaperModel().to(device)\n",
    "optimizer = torch.optim.RMSprop(paper_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5ef2cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [2/3504], Loss: 0.3757\n",
      "Epoch [1/1], Step [3/3504], Loss: 0.3656\n",
      "Epoch [1/1], Step [4/3504], Loss: 0.3582\n",
      "Epoch [1/1], Step [5/3504], Loss: 0.3829\n",
      "Epoch [1/1], Step [6/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [7/3504], Loss: 0.3451\n",
      "Epoch [1/1], Step [8/3504], Loss: 0.3555\n",
      "Epoch [1/1], Step [9/3504], Loss: 0.2830\n",
      "Epoch [1/1], Step [10/3504], Loss: 0.3125\n",
      "Epoch [1/1], Step [11/3504], Loss: 0.3114\n",
      "Epoch [1/1], Step [12/3504], Loss: 0.3141\n",
      "Epoch [1/1], Step [13/3504], Loss: 0.3746\n",
      "Epoch [1/1], Step [14/3504], Loss: 0.3350\n",
      "Epoch [1/1], Step [15/3504], Loss: 0.3950\n",
      "Epoch [1/1], Step [16/3504], Loss: 0.2670\n",
      "Epoch [1/1], Step [17/3504], Loss: 0.3665\n",
      "Epoch [1/1], Step [18/3504], Loss: 0.3557\n",
      "Epoch [1/1], Step [19/3504], Loss: 0.2613\n",
      "Epoch [1/1], Step [20/3504], Loss: 0.3077\n",
      "Epoch [1/1], Step [21/3504], Loss: 0.2782\n",
      "Epoch [1/1], Step [22/3504], Loss: 0.3448\n",
      "Epoch [1/1], Step [23/3504], Loss: 0.3446\n",
      "Epoch [1/1], Step [24/3504], Loss: 0.2620\n",
      "Epoch [1/1], Step [25/3504], Loss: 0.3741\n",
      "Epoch [1/1], Step [26/3504], Loss: 0.3191\n",
      "Epoch [1/1], Step [27/3504], Loss: 0.2318\n",
      "Epoch [1/1], Step [28/3504], Loss: 0.3211\n",
      "Epoch [1/1], Step [29/3504], Loss: 0.3229\n",
      "Epoch [1/1], Step [30/3504], Loss: 0.3688\n",
      "Epoch [1/1], Step [31/3504], Loss: 0.4304\n",
      "Epoch [1/1], Step [32/3504], Loss: 0.3578\n",
      "Epoch [1/1], Step [33/3504], Loss: 0.3787\n",
      "Epoch [1/1], Step [34/3504], Loss: 0.3019\n",
      "Epoch [1/1], Step [35/3504], Loss: 0.3501\n",
      "Epoch [1/1], Step [36/3504], Loss: 0.3528\n",
      "Epoch [1/1], Step [37/3504], Loss: 0.4428\n",
      "Epoch [1/1], Step [38/3504], Loss: 0.3445\n",
      "Epoch [1/1], Step [39/3504], Loss: 0.2931\n",
      "Epoch [1/1], Step [40/3504], Loss: 0.4222\n",
      "Epoch [1/1], Step [41/3504], Loss: 0.2679\n",
      "Epoch [1/1], Step [42/3504], Loss: 0.2954\n",
      "Epoch [1/1], Step [43/3504], Loss: 0.3782\n",
      "Epoch [1/1], Step [44/3504], Loss: 0.4256\n",
      "Epoch [1/1], Step [45/3504], Loss: 0.3872\n",
      "Epoch [1/1], Step [46/3504], Loss: 0.4176\n",
      "Epoch [1/1], Step [47/3504], Loss: 0.3141\n",
      "Epoch [1/1], Step [48/3504], Loss: 0.3542\n",
      "Epoch [1/1], Step [49/3504], Loss: 0.3598\n",
      "Epoch [1/1], Step [50/3504], Loss: 0.3411\n",
      "Epoch [1/1], Step [51/3504], Loss: 0.3297\n",
      "Epoch [1/1], Step [52/3504], Loss: 0.3125\n",
      "Epoch [1/1], Step [53/3504], Loss: 0.4300\n",
      "Epoch [1/1], Step [54/3504], Loss: 0.3807\n",
      "Epoch [1/1], Step [55/3504], Loss: 0.3530\n",
      "Epoch [1/1], Step [56/3504], Loss: 0.3259\n",
      "Epoch [1/1], Step [57/3504], Loss: 0.2729\n",
      "Epoch [1/1], Step [58/3504], Loss: 0.2893\n",
      "Epoch [1/1], Step [59/3504], Loss: 0.3299\n",
      "Epoch [1/1], Step [60/3504], Loss: 0.3974\n",
      "Epoch [1/1], Step [61/3504], Loss: 0.3312\n",
      "Epoch [1/1], Step [62/3504], Loss: 0.2822\n",
      "Epoch [1/1], Step [63/3504], Loss: 0.3630\n",
      "Epoch [1/1], Step [64/3504], Loss: 0.2962\n",
      "Epoch [1/1], Step [65/3504], Loss: 0.3390\n",
      "Epoch [1/1], Step [66/3504], Loss: 0.3585\n",
      "Epoch [1/1], Step [67/3504], Loss: 0.4171\n",
      "Epoch [1/1], Step [68/3504], Loss: 0.3695\n",
      "Epoch [1/1], Step [69/3504], Loss: 0.3255\n",
      "Epoch [1/1], Step [70/3504], Loss: 0.2847\n",
      "Epoch [1/1], Step [71/3504], Loss: 0.3480\n",
      "Epoch [1/1], Step [72/3504], Loss: 0.3776\n",
      "Epoch [1/1], Step [73/3504], Loss: 0.3107\n",
      "Epoch [1/1], Step [74/3504], Loss: 0.3715\n",
      "Epoch [1/1], Step [75/3504], Loss: 0.3162\n",
      "Epoch [1/1], Step [76/3504], Loss: 0.4159\n",
      "Epoch [1/1], Step [77/3504], Loss: 0.3028\n",
      "Epoch [1/1], Step [78/3504], Loss: 0.3263\n",
      "Epoch [1/1], Step [79/3504], Loss: 0.3211\n",
      "Epoch [1/1], Step [80/3504], Loss: 0.3416\n",
      "Epoch [1/1], Step [81/3504], Loss: 0.3208\n",
      "Epoch [1/1], Step [82/3504], Loss: 0.3085\n",
      "Epoch [1/1], Step [83/3504], Loss: 0.3203\n",
      "Epoch [1/1], Step [84/3504], Loss: 0.3173\n",
      "Epoch [1/1], Step [85/3504], Loss: 0.3419\n",
      "Epoch [1/1], Step [86/3504], Loss: 0.3970\n",
      "Epoch [1/1], Step [87/3504], Loss: 0.3872\n",
      "Epoch [1/1], Step [88/3504], Loss: 0.2603\n",
      "Epoch [1/1], Step [89/3504], Loss: 0.2814\n",
      "Epoch [1/1], Step [90/3504], Loss: 0.2609\n",
      "Epoch [1/1], Step [91/3504], Loss: 0.3125\n",
      "Epoch [1/1], Step [92/3504], Loss: 0.3012\n",
      "Epoch [1/1], Step [93/3504], Loss: 0.2810\n",
      "Epoch [1/1], Step [94/3504], Loss: 0.4224\n",
      "Epoch [1/1], Step [95/3504], Loss: 0.3416\n",
      "Epoch [1/1], Step [96/3504], Loss: 0.3321\n",
      "Epoch [1/1], Step [97/3504], Loss: 0.2717\n",
      "Epoch [1/1], Step [98/3504], Loss: 0.3268\n",
      "Epoch [1/1], Step [99/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [100/3504], Loss: 0.3611\n",
      "Epoch [1/1], Step [101/3504], Loss: 0.4298\n",
      "Epoch [1/1], Step [102/3504], Loss: 0.3296\n",
      "Epoch [1/1], Step [103/3504], Loss: 0.3706\n",
      "Epoch [1/1], Step [104/3504], Loss: 0.3198\n",
      "Epoch [1/1], Step [105/3504], Loss: 0.3617\n",
      "Epoch [1/1], Step [106/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [107/3504], Loss: 0.4041\n",
      "Epoch [1/1], Step [108/3504], Loss: 0.2525\n",
      "Epoch [1/1], Step [109/3504], Loss: 0.3401\n",
      "Epoch [1/1], Step [110/3504], Loss: 0.2670\n",
      "Epoch [1/1], Step [111/3504], Loss: 0.3618\n",
      "Epoch [1/1], Step [112/3504], Loss: 0.2635\n",
      "Epoch [1/1], Step [113/3504], Loss: 0.2726\n",
      "Epoch [1/1], Step [114/3504], Loss: 0.3378\n",
      "Epoch [1/1], Step [115/3504], Loss: 0.3109\n",
      "Epoch [1/1], Step [116/3504], Loss: 0.3177\n",
      "Epoch [1/1], Step [117/3504], Loss: 0.4131\n",
      "Epoch [1/1], Step [118/3504], Loss: 0.2903\n",
      "Epoch [1/1], Step [119/3504], Loss: 0.4000\n",
      "Epoch [1/1], Step [120/3504], Loss: 0.4359\n",
      "Epoch [1/1], Step [121/3504], Loss: 0.3537\n",
      "Epoch [1/1], Step [122/3504], Loss: 0.3265\n",
      "Epoch [1/1], Step [123/3504], Loss: 0.4202\n",
      "Epoch [1/1], Step [124/3504], Loss: 0.4016\n",
      "Epoch [1/1], Step [125/3504], Loss: 0.4096\n",
      "Epoch [1/1], Step [126/3504], Loss: 0.3299\n",
      "Epoch [1/1], Step [127/3504], Loss: 0.3743\n",
      "Epoch [1/1], Step [128/3504], Loss: 0.3511\n",
      "Epoch [1/1], Step [129/3504], Loss: 0.3567\n",
      "Epoch [1/1], Step [130/3504], Loss: 0.2590\n",
      "Epoch [1/1], Step [131/3504], Loss: 0.3693\n",
      "Epoch [1/1], Step [132/3504], Loss: 0.3747\n",
      "Epoch [1/1], Step [133/3504], Loss: 0.2529\n",
      "Epoch [1/1], Step [134/3504], Loss: 0.4010\n",
      "Epoch [1/1], Step [135/3504], Loss: 0.3469\n",
      "Epoch [1/1], Step [136/3504], Loss: 0.3168\n",
      "Epoch [1/1], Step [137/3504], Loss: 0.2980\n",
      "Epoch [1/1], Step [138/3504], Loss: 0.3404\n",
      "Epoch [1/1], Step [139/3504], Loss: 0.3195\n",
      "Epoch [1/1], Step [140/3504], Loss: 0.4041\n",
      "Epoch [1/1], Step [141/3504], Loss: 0.3126\n",
      "Epoch [1/1], Step [142/3504], Loss: 0.3471\n",
      "Epoch [1/1], Step [143/3504], Loss: 0.3010\n",
      "Epoch [1/1], Step [144/3504], Loss: 0.3860\n",
      "Epoch [1/1], Step [145/3504], Loss: 0.3074\n",
      "Epoch [1/1], Step [146/3504], Loss: 0.3921\n",
      "Epoch [1/1], Step [147/3504], Loss: 0.3583\n",
      "Epoch [1/1], Step [148/3504], Loss: 0.3415\n",
      "Epoch [1/1], Step [149/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [150/3504], Loss: 0.3964\n",
      "Epoch [1/1], Step [151/3504], Loss: 0.3805\n",
      "Epoch [1/1], Step [152/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [153/3504], Loss: 0.4309\n",
      "Epoch [1/1], Step [154/3504], Loss: 0.3949\n",
      "Epoch [1/1], Step [155/3504], Loss: 0.3371\n",
      "Epoch [1/1], Step [156/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [157/3504], Loss: 0.4044\n",
      "Epoch [1/1], Step [158/3504], Loss: 0.3228\n",
      "Epoch [1/1], Step [159/3504], Loss: 0.2762\n",
      "Epoch [1/1], Step [160/3504], Loss: 0.3014\n",
      "Epoch [1/1], Step [161/3504], Loss: 0.4873\n",
      "Epoch [1/1], Step [162/3504], Loss: 0.3712\n",
      "Epoch [1/1], Step [163/3504], Loss: 0.4147\n",
      "Epoch [1/1], Step [164/3504], Loss: 0.3421\n",
      "Epoch [1/1], Step [165/3504], Loss: 0.3139\n",
      "Epoch [1/1], Step [166/3504], Loss: 0.3565\n",
      "Epoch [1/1], Step [167/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [168/3504], Loss: 0.3838\n",
      "Epoch [1/1], Step [169/3504], Loss: 0.4061\n",
      "Epoch [1/1], Step [170/3504], Loss: 0.3097\n",
      "Epoch [1/1], Step [171/3504], Loss: 0.4206\n",
      "Epoch [1/1], Step [172/3504], Loss: 0.3236\n",
      "Epoch [1/1], Step [173/3504], Loss: 0.3920\n",
      "Epoch [1/1], Step [174/3504], Loss: 0.3800\n",
      "Epoch [1/1], Step [175/3504], Loss: 0.3611\n",
      "Epoch [1/1], Step [176/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [177/3504], Loss: 0.3474\n",
      "Epoch [1/1], Step [178/3504], Loss: 0.3738\n",
      "Epoch [1/1], Step [179/3504], Loss: 0.3522\n",
      "Epoch [1/1], Step [180/3504], Loss: 0.3824\n",
      "Epoch [1/1], Step [181/3504], Loss: 0.4306\n",
      "Epoch [1/1], Step [182/3504], Loss: 0.3526\n",
      "Epoch [1/1], Step [183/3504], Loss: 0.3053\n",
      "Epoch [1/1], Step [184/3504], Loss: 0.2671\n",
      "Epoch [1/1], Step [185/3504], Loss: 0.4046\n",
      "Epoch [1/1], Step [186/3504], Loss: 0.3451\n",
      "Epoch [1/1], Step [187/3504], Loss: 0.3402\n",
      "Epoch [1/1], Step [188/3504], Loss: 0.3082\n",
      "Epoch [1/1], Step [189/3504], Loss: 0.4508\n",
      "Epoch [1/1], Step [190/3504], Loss: 0.3354\n",
      "Epoch [1/1], Step [191/3504], Loss: 0.3591\n",
      "Epoch [1/1], Step [192/3504], Loss: 0.2468\n",
      "Epoch [1/1], Step [193/3504], Loss: 0.3951\n",
      "Epoch [1/1], Step [194/3504], Loss: 0.4063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [195/3504], Loss: 0.4074\n",
      "Epoch [1/1], Step [196/3504], Loss: 0.3066\n",
      "Epoch [1/1], Step [197/3504], Loss: 0.3047\n",
      "Epoch [1/1], Step [198/3504], Loss: 0.3605\n",
      "Epoch [1/1], Step [199/3504], Loss: 0.3825\n",
      "Epoch [1/1], Step [200/3504], Loss: 0.3680\n",
      "Epoch [1/1], Step [201/3504], Loss: 0.2729\n",
      "Epoch [1/1], Step [202/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [203/3504], Loss: 0.3674\n",
      "Epoch [1/1], Step [204/3504], Loss: 0.3708\n",
      "Epoch [1/1], Step [205/3504], Loss: 0.2657\n",
      "Epoch [1/1], Step [206/3504], Loss: 0.3518\n",
      "Epoch [1/1], Step [207/3504], Loss: 0.2676\n",
      "Epoch [1/1], Step [208/3504], Loss: 0.4277\n",
      "Epoch [1/1], Step [209/3504], Loss: 0.3229\n",
      "Epoch [1/1], Step [210/3504], Loss: 0.3430\n",
      "Epoch [1/1], Step [211/3504], Loss: 0.3001\n",
      "Epoch [1/1], Step [212/3504], Loss: 0.3484\n",
      "Epoch [1/1], Step [213/3504], Loss: 0.3831\n",
      "Epoch [1/1], Step [214/3504], Loss: 0.3156\n",
      "Epoch [1/1], Step [215/3504], Loss: 0.3050\n",
      "Epoch [1/1], Step [216/3504], Loss: 0.3305\n",
      "Epoch [1/1], Step [217/3504], Loss: 0.3757\n",
      "Epoch [1/1], Step [218/3504], Loss: 0.2578\n",
      "Epoch [1/1], Step [219/3504], Loss: 0.3450\n",
      "Epoch [1/1], Step [220/3504], Loss: 0.3627\n",
      "Epoch [1/1], Step [221/3504], Loss: 0.3264\n",
      "Epoch [1/1], Step [222/3504], Loss: 0.3268\n",
      "Epoch [1/1], Step [223/3504], Loss: 0.3378\n",
      "Epoch [1/1], Step [224/3504], Loss: 0.3972\n",
      "Epoch [1/1], Step [225/3504], Loss: 0.4426\n",
      "Epoch [1/1], Step [226/3504], Loss: 0.3343\n",
      "Epoch [1/1], Step [227/3504], Loss: 0.3455\n",
      "Epoch [1/1], Step [228/3504], Loss: 0.4398\n",
      "Epoch [1/1], Step [229/3504], Loss: 0.3392\n",
      "Epoch [1/1], Step [230/3504], Loss: 0.4033\n",
      "Epoch [1/1], Step [231/3504], Loss: 0.3463\n",
      "Epoch [1/1], Step [232/3504], Loss: 0.3506\n",
      "Epoch [1/1], Step [233/3504], Loss: 0.3322\n",
      "Epoch [1/1], Step [234/3504], Loss: 0.3208\n",
      "Epoch [1/1], Step [235/3504], Loss: 0.3088\n",
      "Epoch [1/1], Step [236/3504], Loss: 0.2935\n",
      "Epoch [1/1], Step [237/3504], Loss: 0.3979\n",
      "Epoch [1/1], Step [238/3504], Loss: 0.2682\n",
      "Epoch [1/1], Step [239/3504], Loss: 0.2728\n",
      "Epoch [1/1], Step [240/3504], Loss: 0.4318\n",
      "Epoch [1/1], Step [241/3504], Loss: 0.3959\n",
      "Epoch [1/1], Step [242/3504], Loss: 0.3150\n",
      "Epoch [1/1], Step [243/3504], Loss: 0.3388\n",
      "Epoch [1/1], Step [244/3504], Loss: 0.3899\n",
      "Epoch [1/1], Step [245/3504], Loss: 0.3200\n",
      "Epoch [1/1], Step [246/3504], Loss: 0.3769\n",
      "Epoch [1/1], Step [247/3504], Loss: 0.3896\n",
      "Epoch [1/1], Step [248/3504], Loss: 0.3318\n",
      "Epoch [1/1], Step [249/3504], Loss: 0.2900\n",
      "Epoch [1/1], Step [250/3504], Loss: 0.3016\n",
      "Epoch [1/1], Step [251/3504], Loss: 0.3538\n",
      "Epoch [1/1], Step [252/3504], Loss: 0.3169\n",
      "Epoch [1/1], Step [253/3504], Loss: 0.3686\n",
      "Epoch [1/1], Step [254/3504], Loss: 0.3476\n",
      "Epoch [1/1], Step [255/3504], Loss: 0.3481\n",
      "Epoch [1/1], Step [256/3504], Loss: 0.3277\n",
      "Epoch [1/1], Step [257/3504], Loss: 0.2920\n",
      "Epoch [1/1], Step [258/3504], Loss: 0.3161\n",
      "Epoch [1/1], Step [259/3504], Loss: 0.4222\n",
      "Epoch [1/1], Step [260/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [261/3504], Loss: 0.4450\n",
      "Epoch [1/1], Step [262/3504], Loss: 0.2981\n",
      "Epoch [1/1], Step [263/3504], Loss: 0.3681\n",
      "Epoch [1/1], Step [264/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [265/3504], Loss: 0.3258\n",
      "Epoch [1/1], Step [266/3504], Loss: 0.3925\n",
      "Epoch [1/1], Step [267/3504], Loss: 0.2695\n",
      "Epoch [1/1], Step [268/3504], Loss: 0.2732\n",
      "Epoch [1/1], Step [269/3504], Loss: 0.3405\n",
      "Epoch [1/1], Step [270/3504], Loss: 0.2652\n",
      "Epoch [1/1], Step [271/3504], Loss: 0.3355\n",
      "Epoch [1/1], Step [272/3504], Loss: 0.3947\n",
      "Epoch [1/1], Step [273/3504], Loss: 0.2678\n",
      "Epoch [1/1], Step [274/3504], Loss: 0.2870\n",
      "Epoch [1/1], Step [275/3504], Loss: 0.3296\n",
      "Epoch [1/1], Step [276/3504], Loss: 0.2780\n",
      "Epoch [1/1], Step [277/3504], Loss: 0.3686\n",
      "Epoch [1/1], Step [278/3504], Loss: 0.3711\n",
      "Epoch [1/1], Step [279/3504], Loss: 0.3694\n",
      "Epoch [1/1], Step [280/3504], Loss: 0.3206\n",
      "Epoch [1/1], Step [281/3504], Loss: 0.3159\n",
      "Epoch [1/1], Step [282/3504], Loss: 0.3999\n",
      "Epoch [1/1], Step [283/3504], Loss: 0.2979\n",
      "Epoch [1/1], Step [284/3504], Loss: 0.4087\n",
      "Epoch [1/1], Step [285/3504], Loss: 0.4761\n",
      "Epoch [1/1], Step [286/3504], Loss: 0.3971\n",
      "Epoch [1/1], Step [287/3504], Loss: 0.4199\n",
      "Epoch [1/1], Step [288/3504], Loss: 0.3353\n",
      "Epoch [1/1], Step [289/3504], Loss: 0.4212\n",
      "Epoch [1/1], Step [290/3504], Loss: 0.4106\n",
      "Epoch [1/1], Step [291/3504], Loss: 0.3818\n",
      "Epoch [1/1], Step [292/3504], Loss: 0.3412\n",
      "Epoch [1/1], Step [293/3504], Loss: 0.3293\n",
      "Epoch [1/1], Step [294/3504], Loss: 0.3310\n",
      "Epoch [1/1], Step [295/3504], Loss: 0.3174\n",
      "Epoch [1/1], Step [296/3504], Loss: 0.3894\n",
      "Epoch [1/1], Step [297/3504], Loss: 0.3066\n",
      "Epoch [1/1], Step [298/3504], Loss: 0.4055\n",
      "Epoch [1/1], Step [299/3504], Loss: 0.2818\n",
      "Epoch [1/1], Step [300/3504], Loss: 0.4019\n",
      "Epoch [1/1], Step [301/3504], Loss: 0.3547\n",
      "Epoch [1/1], Step [302/3504], Loss: 0.3690\n",
      "Epoch [1/1], Step [303/3504], Loss: 0.3303\n",
      "Epoch [1/1], Step [304/3504], Loss: 0.2666\n",
      "Epoch [1/1], Step [305/3504], Loss: 0.3362\n",
      "Epoch [1/1], Step [306/3504], Loss: 0.3378\n",
      "Epoch [1/1], Step [307/3504], Loss: 0.3045\n",
      "Epoch [1/1], Step [308/3504], Loss: 0.4260\n",
      "Epoch [1/1], Step [309/3504], Loss: 0.3820\n",
      "Epoch [1/1], Step [310/3504], Loss: 0.3204\n",
      "Epoch [1/1], Step [311/3504], Loss: 0.4006\n",
      "Epoch [1/1], Step [312/3504], Loss: 0.3060\n",
      "Epoch [1/1], Step [313/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [314/3504], Loss: 0.3131\n",
      "Epoch [1/1], Step [315/3504], Loss: 0.3193\n",
      "Epoch [1/1], Step [316/3504], Loss: 0.3657\n",
      "Epoch [1/1], Step [317/3504], Loss: 0.3150\n",
      "Epoch [1/1], Step [318/3504], Loss: 0.2711\n",
      "Epoch [1/1], Step [319/3504], Loss: 0.3661\n",
      "Epoch [1/1], Step [320/3504], Loss: 0.3556\n",
      "Epoch [1/1], Step [321/3504], Loss: 0.4066\n",
      "Epoch [1/1], Step [322/3504], Loss: 0.3027\n",
      "Epoch [1/1], Step [323/3504], Loss: 0.3531\n",
      "Epoch [1/1], Step [324/3504], Loss: 0.3334\n",
      "Epoch [1/1], Step [325/3504], Loss: 0.3795\n",
      "Epoch [1/1], Step [326/3504], Loss: 0.3393\n",
      "Epoch [1/1], Step [327/3504], Loss: 0.3609\n",
      "Epoch [1/1], Step [328/3504], Loss: 0.3432\n",
      "Epoch [1/1], Step [329/3504], Loss: 0.3492\n",
      "Epoch [1/1], Step [330/3504], Loss: 0.3307\n",
      "Epoch [1/1], Step [331/3504], Loss: 0.3682\n",
      "Epoch [1/1], Step [332/3504], Loss: 0.3926\n",
      "Epoch [1/1], Step [333/3504], Loss: 0.3494\n",
      "Epoch [1/1], Step [334/3504], Loss: 0.3065\n",
      "Epoch [1/1], Step [335/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [336/3504], Loss: 0.2978\n",
      "Epoch [1/1], Step [337/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [338/3504], Loss: 0.4007\n",
      "Epoch [1/1], Step [339/3504], Loss: 0.3159\n",
      "Epoch [1/1], Step [340/3504], Loss: 0.3017\n",
      "Epoch [1/1], Step [341/3504], Loss: 0.4006\n",
      "Epoch [1/1], Step [342/3504], Loss: 0.3408\n",
      "Epoch [1/1], Step [343/3504], Loss: 0.3806\n",
      "Epoch [1/1], Step [344/3504], Loss: 0.2852\n",
      "Epoch [1/1], Step [345/3504], Loss: 0.3657\n",
      "Epoch [1/1], Step [346/3504], Loss: 0.3934\n",
      "Epoch [1/1], Step [347/3504], Loss: 0.3433\n",
      "Epoch [1/1], Step [348/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [349/3504], Loss: 0.2547\n",
      "Epoch [1/1], Step [350/3504], Loss: 0.3325\n",
      "Epoch [1/1], Step [351/3504], Loss: 0.3194\n",
      "Epoch [1/1], Step [352/3504], Loss: 0.4286\n",
      "Epoch [1/1], Step [353/3504], Loss: 0.3639\n",
      "Epoch [1/1], Step [354/3504], Loss: 0.3886\n",
      "Epoch [1/1], Step [355/3504], Loss: 0.3502\n",
      "Epoch [1/1], Step [356/3504], Loss: 0.3283\n",
      "Epoch [1/1], Step [357/3504], Loss: 0.2742\n",
      "Epoch [1/1], Step [358/3504], Loss: 0.3143\n",
      "Epoch [1/1], Step [359/3504], Loss: 0.3173\n",
      "Epoch [1/1], Step [360/3504], Loss: 0.3461\n",
      "Epoch [1/1], Step [361/3504], Loss: 0.3735\n",
      "Epoch [1/1], Step [362/3504], Loss: 0.2660\n",
      "Epoch [1/1], Step [363/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [364/3504], Loss: 0.3387\n",
      "Epoch [1/1], Step [365/3504], Loss: 0.2767\n",
      "Epoch [1/1], Step [366/3504], Loss: 0.3854\n",
      "Epoch [1/1], Step [367/3504], Loss: 0.3216\n",
      "Epoch [1/1], Step [368/3504], Loss: 0.3355\n",
      "Epoch [1/1], Step [369/3504], Loss: 0.3125\n",
      "Epoch [1/1], Step [370/3504], Loss: 0.5132\n",
      "Epoch [1/1], Step [371/3504], Loss: 0.3524\n",
      "Epoch [1/1], Step [372/3504], Loss: 0.3278\n",
      "Epoch [1/1], Step [373/3504], Loss: 0.3965\n",
      "Epoch [1/1], Step [374/3504], Loss: 0.2440\n",
      "Epoch [1/1], Step [375/3504], Loss: 0.3618\n",
      "Epoch [1/1], Step [376/3504], Loss: 0.4598\n",
      "Epoch [1/1], Step [377/3504], Loss: 0.3212\n",
      "Epoch [1/1], Step [378/3504], Loss: 0.3571\n",
      "Epoch [1/1], Step [379/3504], Loss: 0.3835\n",
      "Epoch [1/1], Step [380/3504], Loss: 0.3273\n",
      "Epoch [1/1], Step [381/3504], Loss: 0.3105\n",
      "Epoch [1/1], Step [382/3504], Loss: 0.3000\n",
      "Epoch [1/1], Step [383/3504], Loss: 0.3022\n",
      "Epoch [1/1], Step [384/3504], Loss: 0.3935\n",
      "Epoch [1/1], Step [385/3504], Loss: 0.3272\n",
      "Epoch [1/1], Step [386/3504], Loss: 0.2755\n",
      "Epoch [1/1], Step [387/3504], Loss: 0.3184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [388/3504], Loss: 0.3039\n",
      "Epoch [1/1], Step [389/3504], Loss: 0.3192\n",
      "Epoch [1/1], Step [390/3504], Loss: 0.3802\n",
      "Epoch [1/1], Step [391/3504], Loss: 0.3298\n",
      "Epoch [1/1], Step [392/3504], Loss: 0.3450\n",
      "Epoch [1/1], Step [393/3504], Loss: 0.2780\n",
      "Epoch [1/1], Step [394/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [395/3504], Loss: 0.3506\n",
      "Epoch [1/1], Step [396/3504], Loss: 0.2932\n",
      "Epoch [1/1], Step [397/3504], Loss: 0.3783\n",
      "Epoch [1/1], Step [398/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [399/3504], Loss: 0.3330\n",
      "Epoch [1/1], Step [400/3504], Loss: 0.3814\n",
      "Epoch [1/1], Step [401/3504], Loss: 0.3208\n",
      "Epoch [1/1], Step [402/3504], Loss: 0.3085\n",
      "Epoch [1/1], Step [403/3504], Loss: 0.3466\n",
      "Epoch [1/1], Step [404/3504], Loss: 0.2676\n",
      "Epoch [1/1], Step [405/3504], Loss: 0.3432\n",
      "Epoch [1/1], Step [406/3504], Loss: 0.3165\n",
      "Epoch [1/1], Step [407/3504], Loss: 0.3453\n",
      "Epoch [1/1], Step [408/3504], Loss: 0.2958\n",
      "Epoch [1/1], Step [409/3504], Loss: 0.3783\n",
      "Epoch [1/1], Step [410/3504], Loss: 0.4123\n",
      "Epoch [1/1], Step [411/3504], Loss: 0.3500\n",
      "Epoch [1/1], Step [412/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [413/3504], Loss: 0.3218\n",
      "Epoch [1/1], Step [414/3504], Loss: 0.3325\n",
      "Epoch [1/1], Step [415/3504], Loss: 0.3835\n",
      "Epoch [1/1], Step [416/3504], Loss: 0.3348\n",
      "Epoch [1/1], Step [417/3504], Loss: 0.3902\n",
      "Epoch [1/1], Step [418/3504], Loss: 0.2652\n",
      "Epoch [1/1], Step [419/3504], Loss: 0.3044\n",
      "Epoch [1/1], Step [420/3504], Loss: 0.3316\n",
      "Epoch [1/1], Step [421/3504], Loss: 0.3362\n",
      "Epoch [1/1], Step [422/3504], Loss: 0.3531\n",
      "Epoch [1/1], Step [423/3504], Loss: 0.2629\n",
      "Epoch [1/1], Step [424/3504], Loss: 0.2870\n",
      "Epoch [1/1], Step [425/3504], Loss: 0.4175\n",
      "Epoch [1/1], Step [426/3504], Loss: 0.2872\n",
      "Epoch [1/1], Step [427/3504], Loss: 0.3795\n",
      "Epoch [1/1], Step [428/3504], Loss: 0.3603\n",
      "Epoch [1/1], Step [429/3504], Loss: 0.3380\n",
      "Epoch [1/1], Step [430/3504], Loss: 0.3282\n",
      "Epoch [1/1], Step [431/3504], Loss: 0.3039\n",
      "Epoch [1/1], Step [432/3504], Loss: 0.3939\n",
      "Epoch [1/1], Step [433/3504], Loss: 0.3320\n",
      "Epoch [1/1], Step [434/3504], Loss: 0.3131\n",
      "Epoch [1/1], Step [435/3504], Loss: 0.3612\n",
      "Epoch [1/1], Step [436/3504], Loss: 0.3726\n",
      "Epoch [1/1], Step [437/3504], Loss: 0.3784\n",
      "Epoch [1/1], Step [438/3504], Loss: 0.3306\n",
      "Epoch [1/1], Step [439/3504], Loss: 0.3636\n",
      "Epoch [1/1], Step [440/3504], Loss: 0.2779\n",
      "Epoch [1/1], Step [441/3504], Loss: 0.2298\n",
      "Epoch [1/1], Step [442/3504], Loss: 0.3645\n",
      "Epoch [1/1], Step [443/3504], Loss: 0.3665\n",
      "Epoch [1/1], Step [444/3504], Loss: 0.3351\n",
      "Epoch [1/1], Step [445/3504], Loss: 0.3810\n",
      "Epoch [1/1], Step [446/3504], Loss: 0.2140\n",
      "Epoch [1/1], Step [447/3504], Loss: 0.3887\n",
      "Epoch [1/1], Step [448/3504], Loss: 0.3701\n",
      "Epoch [1/1], Step [449/3504], Loss: 0.3091\n",
      "Epoch [1/1], Step [450/3504], Loss: 0.3432\n",
      "Epoch [1/1], Step [451/3504], Loss: 0.3147\n",
      "Epoch [1/1], Step [452/3504], Loss: 0.3032\n",
      "Epoch [1/1], Step [453/3504], Loss: 0.3348\n",
      "Epoch [1/1], Step [454/3504], Loss: 0.2968\n",
      "Epoch [1/1], Step [455/3504], Loss: 0.3412\n",
      "Epoch [1/1], Step [456/3504], Loss: 0.3426\n",
      "Epoch [1/1], Step [457/3504], Loss: 0.3015\n",
      "Epoch [1/1], Step [458/3504], Loss: 0.3417\n",
      "Epoch [1/1], Step [459/3504], Loss: 0.3349\n",
      "Epoch [1/1], Step [460/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [461/3504], Loss: 0.3475\n",
      "Epoch [1/1], Step [462/3504], Loss: 0.3316\n",
      "Epoch [1/1], Step [463/3504], Loss: 0.3162\n",
      "Epoch [1/1], Step [464/3504], Loss: 0.3306\n",
      "Epoch [1/1], Step [465/3504], Loss: 0.3598\n",
      "Epoch [1/1], Step [466/3504], Loss: 0.3563\n",
      "Epoch [1/1], Step [467/3504], Loss: 0.2985\n",
      "Epoch [1/1], Step [468/3504], Loss: 0.4028\n",
      "Epoch [1/1], Step [469/3504], Loss: 0.3859\n",
      "Epoch [1/1], Step [470/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [471/3504], Loss: 0.3755\n",
      "Epoch [1/1], Step [472/3504], Loss: 0.3724\n",
      "Epoch [1/1], Step [473/3504], Loss: 0.3107\n",
      "Epoch [1/1], Step [474/3504], Loss: 0.2745\n",
      "Epoch [1/1], Step [475/3504], Loss: 0.2678\n",
      "Epoch [1/1], Step [476/3504], Loss: 0.2996\n",
      "Epoch [1/1], Step [477/3504], Loss: 0.3046\n",
      "Epoch [1/1], Step [478/3504], Loss: 0.3536\n",
      "Epoch [1/1], Step [479/3504], Loss: 0.3636\n",
      "Epoch [1/1], Step [480/3504], Loss: 0.3513\n",
      "Epoch [1/1], Step [481/3504], Loss: 0.3189\n",
      "Epoch [1/1], Step [482/3504], Loss: 0.3197\n",
      "Epoch [1/1], Step [483/3504], Loss: 0.2919\n",
      "Epoch [1/1], Step [484/3504], Loss: 0.3118\n",
      "Epoch [1/1], Step [485/3504], Loss: 0.3922\n",
      "Epoch [1/1], Step [486/3504], Loss: 0.2934\n",
      "Epoch [1/1], Step [487/3504], Loss: 0.3207\n",
      "Epoch [1/1], Step [488/3504], Loss: 0.3189\n",
      "Epoch [1/1], Step [489/3504], Loss: 0.4020\n",
      "Epoch [1/1], Step [490/3504], Loss: 0.2887\n",
      "Epoch [1/1], Step [491/3504], Loss: 0.3544\n",
      "Epoch [1/1], Step [492/3504], Loss: 0.4004\n",
      "Epoch [1/1], Step [493/3504], Loss: 0.3401\n",
      "Epoch [1/1], Step [494/3504], Loss: 0.3096\n",
      "Epoch [1/1], Step [495/3504], Loss: 0.2855\n",
      "Epoch [1/1], Step [496/3504], Loss: 0.3197\n",
      "Epoch [1/1], Step [497/3504], Loss: 0.3241\n",
      "Epoch [1/1], Step [498/3504], Loss: 0.3907\n",
      "Epoch [1/1], Step [499/3504], Loss: 0.4267\n",
      "Epoch [1/1], Step [500/3504], Loss: 0.3065\n",
      "Epoch [1/1], Step [501/3504], Loss: 0.4041\n",
      "Epoch [1/1], Step [502/3504], Loss: 0.3065\n",
      "Epoch [1/1], Step [503/3504], Loss: 0.2845\n",
      "Epoch [1/1], Step [504/3504], Loss: 0.4101\n",
      "Epoch [1/1], Step [505/3504], Loss: 0.3673\n",
      "Epoch [1/1], Step [506/3504], Loss: 0.2995\n",
      "Epoch [1/1], Step [507/3504], Loss: 0.3187\n",
      "Epoch [1/1], Step [508/3504], Loss: 0.3762\n",
      "Epoch [1/1], Step [509/3504], Loss: 0.3469\n",
      "Epoch [1/1], Step [510/3504], Loss: 0.4241\n",
      "Epoch [1/1], Step [511/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [512/3504], Loss: 0.2982\n",
      "Epoch [1/1], Step [513/3504], Loss: 0.3704\n",
      "Epoch [1/1], Step [514/3504], Loss: 0.3871\n",
      "Epoch [1/1], Step [515/3504], Loss: 0.3464\n",
      "Epoch [1/1], Step [516/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [517/3504], Loss: 0.3503\n",
      "Epoch [1/1], Step [518/3504], Loss: 0.3962\n",
      "Epoch [1/1], Step [519/3504], Loss: 0.2835\n",
      "Epoch [1/1], Step [520/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [521/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [522/3504], Loss: 0.3325\n",
      "Epoch [1/1], Step [523/3504], Loss: 0.3160\n",
      "Epoch [1/1], Step [524/3504], Loss: 0.3965\n",
      "Epoch [1/1], Step [525/3504], Loss: 0.3580\n",
      "Epoch [1/1], Step [526/3504], Loss: 0.2618\n",
      "Epoch [1/1], Step [527/3504], Loss: 0.3619\n",
      "Epoch [1/1], Step [528/3504], Loss: 0.3392\n",
      "Epoch [1/1], Step [529/3504], Loss: 0.3635\n",
      "Epoch [1/1], Step [530/3504], Loss: 0.3334\n",
      "Epoch [1/1], Step [531/3504], Loss: 0.3240\n",
      "Epoch [1/1], Step [532/3504], Loss: 0.2970\n",
      "Epoch [1/1], Step [533/3504], Loss: 0.2681\n",
      "Epoch [1/1], Step [534/3504], Loss: 0.3956\n",
      "Epoch [1/1], Step [535/3504], Loss: 0.3811\n",
      "Epoch [1/1], Step [536/3504], Loss: 0.3202\n",
      "Epoch [1/1], Step [537/3504], Loss: 0.3250\n",
      "Epoch [1/1], Step [538/3504], Loss: 0.3524\n",
      "Epoch [1/1], Step [539/3504], Loss: 0.2838\n",
      "Epoch [1/1], Step [540/3504], Loss: 0.3986\n",
      "Epoch [1/1], Step [541/3504], Loss: 0.3003\n",
      "Epoch [1/1], Step [542/3504], Loss: 0.3006\n",
      "Epoch [1/1], Step [543/3504], Loss: 0.3510\n",
      "Epoch [1/1], Step [544/3504], Loss: 0.3230\n",
      "Epoch [1/1], Step [545/3504], Loss: 0.3033\n",
      "Epoch [1/1], Step [546/3504], Loss: 0.4086\n",
      "Epoch [1/1], Step [547/3504], Loss: 0.3824\n",
      "Epoch [1/1], Step [548/3504], Loss: 0.3763\n",
      "Epoch [1/1], Step [549/3504], Loss: 0.4093\n",
      "Epoch [1/1], Step [550/3504], Loss: 0.3478\n",
      "Epoch [1/1], Step [551/3504], Loss: 0.3445\n",
      "Epoch [1/1], Step [552/3504], Loss: 0.2770\n",
      "Epoch [1/1], Step [553/3504], Loss: 0.3652\n",
      "Epoch [1/1], Step [554/3504], Loss: 0.4007\n",
      "Epoch [1/1], Step [555/3504], Loss: 0.2861\n",
      "Epoch [1/1], Step [556/3504], Loss: 0.3351\n",
      "Epoch [1/1], Step [557/3504], Loss: 0.3258\n",
      "Epoch [1/1], Step [558/3504], Loss: 0.2826\n",
      "Epoch [1/1], Step [559/3504], Loss: 0.3906\n",
      "Epoch [1/1], Step [560/3504], Loss: 0.3382\n",
      "Epoch [1/1], Step [561/3504], Loss: 0.2587\n",
      "Epoch [1/1], Step [562/3504], Loss: 0.2519\n",
      "Epoch [1/1], Step [563/3504], Loss: 0.3015\n",
      "Epoch [1/1], Step [564/3504], Loss: 0.3571\n",
      "Epoch [1/1], Step [565/3504], Loss: 0.3504\n",
      "Epoch [1/1], Step [566/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [567/3504], Loss: 0.4131\n",
      "Epoch [1/1], Step [568/3504], Loss: 0.3143\n",
      "Epoch [1/1], Step [569/3504], Loss: 0.3253\n",
      "Epoch [1/1], Step [570/3504], Loss: 0.2847\n",
      "Epoch [1/1], Step [571/3504], Loss: 0.4418\n",
      "Epoch [1/1], Step [572/3504], Loss: 0.3785\n",
      "Epoch [1/1], Step [573/3504], Loss: 0.3615\n",
      "Epoch [1/1], Step [574/3504], Loss: 0.3482\n",
      "Epoch [1/1], Step [575/3504], Loss: 0.2951\n",
      "Epoch [1/1], Step [576/3504], Loss: 0.3041\n",
      "Epoch [1/1], Step [577/3504], Loss: 0.3680\n",
      "Epoch [1/1], Step [578/3504], Loss: 0.3126\n",
      "Epoch [1/1], Step [579/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [580/3504], Loss: 0.2842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [581/3504], Loss: 0.4537\n",
      "Epoch [1/1], Step [582/3504], Loss: 0.4248\n",
      "Epoch [1/1], Step [583/3504], Loss: 0.2886\n",
      "Epoch [1/1], Step [584/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [585/3504], Loss: 0.3816\n",
      "Epoch [1/1], Step [586/3504], Loss: 0.3979\n",
      "Epoch [1/1], Step [587/3504], Loss: 0.3902\n",
      "Epoch [1/1], Step [588/3504], Loss: 0.3962\n",
      "Epoch [1/1], Step [589/3504], Loss: 0.3162\n",
      "Epoch [1/1], Step [590/3504], Loss: 0.4139\n",
      "Epoch [1/1], Step [591/3504], Loss: 0.3593\n",
      "Epoch [1/1], Step [592/3504], Loss: 0.2822\n",
      "Epoch [1/1], Step [593/3504], Loss: 0.3679\n",
      "Epoch [1/1], Step [594/3504], Loss: 0.3964\n",
      "Epoch [1/1], Step [595/3504], Loss: 0.2807\n",
      "Epoch [1/1], Step [596/3504], Loss: 0.3189\n",
      "Epoch [1/1], Step [597/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [598/3504], Loss: 0.4055\n",
      "Epoch [1/1], Step [599/3504], Loss: 0.4249\n",
      "Epoch [1/1], Step [600/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [601/3504], Loss: 0.2906\n",
      "Epoch [1/1], Step [602/3504], Loss: 0.3025\n",
      "Epoch [1/1], Step [603/3504], Loss: 0.3640\n",
      "Epoch [1/1], Step [604/3504], Loss: 0.2910\n",
      "Epoch [1/1], Step [605/3504], Loss: 0.2897\n",
      "Epoch [1/1], Step [606/3504], Loss: 0.3724\n",
      "Epoch [1/1], Step [607/3504], Loss: 0.3556\n",
      "Epoch [1/1], Step [608/3504], Loss: 0.3504\n",
      "Epoch [1/1], Step [609/3504], Loss: 0.3278\n",
      "Epoch [1/1], Step [610/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [611/3504], Loss: 0.4063\n",
      "Epoch [1/1], Step [612/3504], Loss: 0.4170\n",
      "Epoch [1/1], Step [613/3504], Loss: 0.3796\n",
      "Epoch [1/1], Step [614/3504], Loss: 0.4221\n",
      "Epoch [1/1], Step [615/3504], Loss: 0.3613\n",
      "Epoch [1/1], Step [616/3504], Loss: 0.3479\n",
      "Epoch [1/1], Step [617/3504], Loss: 0.4050\n",
      "Epoch [1/1], Step [618/3504], Loss: 0.3879\n",
      "Epoch [1/1], Step [619/3504], Loss: 0.3178\n",
      "Epoch [1/1], Step [620/3504], Loss: 0.2774\n",
      "Epoch [1/1], Step [621/3504], Loss: 0.2930\n",
      "Epoch [1/1], Step [622/3504], Loss: 0.3619\n",
      "Epoch [1/1], Step [623/3504], Loss: 0.3819\n",
      "Epoch [1/1], Step [624/3504], Loss: 0.3151\n",
      "Epoch [1/1], Step [625/3504], Loss: 0.3047\n",
      "Epoch [1/1], Step [626/3504], Loss: 0.3493\n",
      "Epoch [1/1], Step [627/3504], Loss: 0.3549\n",
      "Epoch [1/1], Step [628/3504], Loss: 0.3487\n",
      "Epoch [1/1], Step [629/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [630/3504], Loss: 0.3672\n",
      "Epoch [1/1], Step [631/3504], Loss: 0.3198\n",
      "Epoch [1/1], Step [632/3504], Loss: 0.3493\n",
      "Epoch [1/1], Step [633/3504], Loss: 0.4233\n",
      "Epoch [1/1], Step [634/3504], Loss: 0.3766\n",
      "Epoch [1/1], Step [635/3504], Loss: 0.4030\n",
      "Epoch [1/1], Step [636/3504], Loss: 0.3192\n",
      "Epoch [1/1], Step [637/3504], Loss: 0.4573\n",
      "Epoch [1/1], Step [638/3504], Loss: 0.4159\n",
      "Epoch [1/1], Step [639/3504], Loss: 0.3916\n",
      "Epoch [1/1], Step [640/3504], Loss: 0.3485\n",
      "Epoch [1/1], Step [641/3504], Loss: 0.2446\n",
      "Epoch [1/1], Step [642/3504], Loss: 0.2922\n",
      "Epoch [1/1], Step [643/3504], Loss: 0.3403\n",
      "Epoch [1/1], Step [644/3504], Loss: 0.4115\n",
      "Epoch [1/1], Step [645/3504], Loss: 0.3553\n",
      "Epoch [1/1], Step [646/3504], Loss: 0.4086\n",
      "Epoch [1/1], Step [647/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [648/3504], Loss: 0.4079\n",
      "Epoch [1/1], Step [649/3504], Loss: 0.3986\n",
      "Epoch [1/1], Step [650/3504], Loss: 0.3694\n",
      "Epoch [1/1], Step [651/3504], Loss: 0.2969\n",
      "Epoch [1/1], Step [652/3504], Loss: 0.3539\n",
      "Epoch [1/1], Step [653/3504], Loss: 0.3674\n",
      "Epoch [1/1], Step [654/3504], Loss: 0.3333\n",
      "Epoch [1/1], Step [655/3504], Loss: 0.3871\n",
      "Epoch [1/1], Step [656/3504], Loss: 0.3182\n",
      "Epoch [1/1], Step [657/3504], Loss: 0.3876\n",
      "Epoch [1/1], Step [658/3504], Loss: 0.3935\n",
      "Epoch [1/1], Step [659/3504], Loss: 0.3722\n",
      "Epoch [1/1], Step [660/3504], Loss: 0.2997\n",
      "Epoch [1/1], Step [661/3504], Loss: 0.3465\n",
      "Epoch [1/1], Step [662/3504], Loss: 0.3426\n",
      "Epoch [1/1], Step [663/3504], Loss: 0.3622\n",
      "Epoch [1/1], Step [664/3504], Loss: 0.3494\n",
      "Epoch [1/1], Step [665/3504], Loss: 0.3151\n",
      "Epoch [1/1], Step [666/3504], Loss: 0.3304\n",
      "Epoch [1/1], Step [667/3504], Loss: 0.3261\n",
      "Epoch [1/1], Step [668/3504], Loss: 0.4103\n",
      "Epoch [1/1], Step [669/3504], Loss: 0.3259\n",
      "Epoch [1/1], Step [670/3504], Loss: 0.3584\n",
      "Epoch [1/1], Step [671/3504], Loss: 0.3725\n",
      "Epoch [1/1], Step [672/3504], Loss: 0.3310\n",
      "Epoch [1/1], Step [673/3504], Loss: 0.3124\n",
      "Epoch [1/1], Step [674/3504], Loss: 0.3089\n",
      "Epoch [1/1], Step [675/3504], Loss: 0.3017\n",
      "Epoch [1/1], Step [676/3504], Loss: 0.4305\n",
      "Epoch [1/1], Step [677/3504], Loss: 0.3080\n",
      "Epoch [1/1], Step [678/3504], Loss: 0.3461\n",
      "Epoch [1/1], Step [679/3504], Loss: 0.3033\n",
      "Epoch [1/1], Step [680/3504], Loss: 0.4162\n",
      "Epoch [1/1], Step [681/3504], Loss: 0.3961\n",
      "Epoch [1/1], Step [682/3504], Loss: 0.3509\n",
      "Epoch [1/1], Step [683/3504], Loss: 0.3158\n",
      "Epoch [1/1], Step [684/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [685/3504], Loss: 0.4579\n",
      "Epoch [1/1], Step [686/3504], Loss: 0.3401\n",
      "Epoch [1/1], Step [687/3504], Loss: 0.2926\n",
      "Epoch [1/1], Step [688/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [689/3504], Loss: 0.3846\n",
      "Epoch [1/1], Step [690/3504], Loss: 0.3332\n",
      "Epoch [1/1], Step [691/3504], Loss: 0.2886\n",
      "Epoch [1/1], Step [692/3504], Loss: 0.3215\n",
      "Epoch [1/1], Step [693/3504], Loss: 0.3898\n",
      "Epoch [1/1], Step [694/3504], Loss: 0.4200\n",
      "Epoch [1/1], Step [695/3504], Loss: 0.3952\n",
      "Epoch [1/1], Step [696/3504], Loss: 0.3464\n",
      "Epoch [1/1], Step [697/3504], Loss: 0.2978\n",
      "Epoch [1/1], Step [698/3504], Loss: 0.3183\n",
      "Epoch [1/1], Step [699/3504], Loss: 0.3589\n",
      "Epoch [1/1], Step [700/3504], Loss: 0.4170\n",
      "Epoch [1/1], Step [701/3504], Loss: 0.3057\n",
      "Epoch [1/1], Step [702/3504], Loss: 0.3507\n",
      "Epoch [1/1], Step [703/3504], Loss: 0.3525\n",
      "Epoch [1/1], Step [704/3504], Loss: 0.2807\n",
      "Epoch [1/1], Step [705/3504], Loss: 0.3363\n",
      "Epoch [1/1], Step [706/3504], Loss: 0.3403\n",
      "Epoch [1/1], Step [707/3504], Loss: 0.3208\n",
      "Epoch [1/1], Step [708/3504], Loss: 0.3878\n",
      "Epoch [1/1], Step [709/3504], Loss: 0.3957\n",
      "Epoch [1/1], Step [710/3504], Loss: 0.3091\n",
      "Epoch [1/1], Step [711/3504], Loss: 0.4058\n",
      "Epoch [1/1], Step [712/3504], Loss: 0.4437\n",
      "Epoch [1/1], Step [713/3504], Loss: 0.4179\n",
      "Epoch [1/1], Step [714/3504], Loss: 0.3071\n",
      "Epoch [1/1], Step [715/3504], Loss: 0.3659\n",
      "Epoch [1/1], Step [716/3504], Loss: 0.3468\n",
      "Epoch [1/1], Step [717/3504], Loss: 0.3460\n",
      "Epoch [1/1], Step [718/3504], Loss: 0.3082\n",
      "Epoch [1/1], Step [719/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [720/3504], Loss: 0.3981\n",
      "Epoch [1/1], Step [721/3504], Loss: 0.3605\n",
      "Epoch [1/1], Step [722/3504], Loss: 0.4112\n",
      "Epoch [1/1], Step [723/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [724/3504], Loss: 0.3206\n",
      "Epoch [1/1], Step [725/3504], Loss: 0.3383\n",
      "Epoch [1/1], Step [726/3504], Loss: 0.2646\n",
      "Epoch [1/1], Step [727/3504], Loss: 0.4288\n",
      "Epoch [1/1], Step [728/3504], Loss: 0.4685\n",
      "Epoch [1/1], Step [729/3504], Loss: 0.3931\n",
      "Epoch [1/1], Step [730/3504], Loss: 0.3946\n",
      "Epoch [1/1], Step [731/3504], Loss: 0.3344\n",
      "Epoch [1/1], Step [732/3504], Loss: 0.4047\n",
      "Epoch [1/1], Step [733/3504], Loss: 0.3643\n",
      "Epoch [1/1], Step [734/3504], Loss: 0.3004\n",
      "Epoch [1/1], Step [735/3504], Loss: 0.2807\n",
      "Epoch [1/1], Step [736/3504], Loss: 0.3369\n",
      "Epoch [1/1], Step [737/3504], Loss: 0.3178\n",
      "Epoch [1/1], Step [738/3504], Loss: 0.2554\n",
      "Epoch [1/1], Step [739/3504], Loss: 0.3707\n",
      "Epoch [1/1], Step [740/3504], Loss: 0.2848\n",
      "Epoch [1/1], Step [741/3504], Loss: 0.4145\n",
      "Epoch [1/1], Step [742/3504], Loss: 0.3094\n",
      "Epoch [1/1], Step [743/3504], Loss: 0.3276\n",
      "Epoch [1/1], Step [744/3504], Loss: 0.3663\n",
      "Epoch [1/1], Step [745/3504], Loss: 0.3502\n",
      "Epoch [1/1], Step [746/3504], Loss: 0.2891\n",
      "Epoch [1/1], Step [747/3504], Loss: 0.2935\n",
      "Epoch [1/1], Step [748/3504], Loss: 0.3797\n",
      "Epoch [1/1], Step [749/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [750/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [751/3504], Loss: 0.2671\n",
      "Epoch [1/1], Step [752/3504], Loss: 0.4177\n",
      "Epoch [1/1], Step [753/3504], Loss: 0.4262\n",
      "Epoch [1/1], Step [754/3504], Loss: 0.3136\n",
      "Epoch [1/1], Step [755/3504], Loss: 0.4128\n",
      "Epoch [1/1], Step [756/3504], Loss: 0.2771\n",
      "Epoch [1/1], Step [757/3504], Loss: 0.3349\n",
      "Epoch [1/1], Step [758/3504], Loss: 0.3763\n",
      "Epoch [1/1], Step [759/3504], Loss: 0.3096\n",
      "Epoch [1/1], Step [760/3504], Loss: 0.3439\n",
      "Epoch [1/1], Step [761/3504], Loss: 0.3562\n",
      "Epoch [1/1], Step [762/3504], Loss: 0.3620\n",
      "Epoch [1/1], Step [763/3504], Loss: 0.2902\n",
      "Epoch [1/1], Step [764/3504], Loss: 0.3717\n",
      "Epoch [1/1], Step [765/3504], Loss: 0.3928\n",
      "Epoch [1/1], Step [766/3504], Loss: 0.4261\n",
      "Epoch [1/1], Step [767/3504], Loss: 0.3927\n",
      "Epoch [1/1], Step [768/3504], Loss: 0.3821\n",
      "Epoch [1/1], Step [769/3504], Loss: 0.3615\n",
      "Epoch [1/1], Step [770/3504], Loss: 0.3341\n",
      "Epoch [1/1], Step [771/3504], Loss: 0.3126\n",
      "Epoch [1/1], Step [772/3504], Loss: 0.4255\n",
      "Epoch [1/1], Step [773/3504], Loss: 0.3509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [774/3504], Loss: 0.3995\n",
      "Epoch [1/1], Step [775/3504], Loss: 0.3562\n",
      "Epoch [1/1], Step [776/3504], Loss: 0.3932\n",
      "Epoch [1/1], Step [777/3504], Loss: 0.4215\n",
      "Epoch [1/1], Step [778/3504], Loss: 0.2925\n",
      "Epoch [1/1], Step [779/3504], Loss: 0.3331\n",
      "Epoch [1/1], Step [780/3504], Loss: 0.4662\n",
      "Epoch [1/1], Step [781/3504], Loss: 0.3484\n",
      "Epoch [1/1], Step [782/3504], Loss: 0.2957\n",
      "Epoch [1/1], Step [783/3504], Loss: 0.2655\n",
      "Epoch [1/1], Step [784/3504], Loss: 0.2808\n",
      "Epoch [1/1], Step [785/3504], Loss: 0.3983\n",
      "Epoch [1/1], Step [786/3504], Loss: 0.3286\n",
      "Epoch [1/1], Step [787/3504], Loss: 0.3588\n",
      "Epoch [1/1], Step [788/3504], Loss: 0.3664\n",
      "Epoch [1/1], Step [789/3504], Loss: 0.4006\n",
      "Epoch [1/1], Step [790/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [791/3504], Loss: 0.3640\n",
      "Epoch [1/1], Step [792/3504], Loss: 0.4021\n",
      "Epoch [1/1], Step [793/3504], Loss: 0.3165\n",
      "Epoch [1/1], Step [794/3504], Loss: 0.3320\n",
      "Epoch [1/1], Step [795/3504], Loss: 0.3384\n",
      "Epoch [1/1], Step [796/3504], Loss: 0.3655\n",
      "Epoch [1/1], Step [797/3504], Loss: 0.3523\n",
      "Epoch [1/1], Step [798/3504], Loss: 0.3722\n",
      "Epoch [1/1], Step [799/3504], Loss: 0.3875\n",
      "Epoch [1/1], Step [800/3504], Loss: 0.2751\n",
      "Epoch [1/1], Step [801/3504], Loss: 0.3253\n",
      "Epoch [1/1], Step [802/3504], Loss: 0.3619\n",
      "Epoch [1/1], Step [803/3504], Loss: 0.3349\n",
      "Epoch [1/1], Step [804/3504], Loss: 0.3369\n",
      "Epoch [1/1], Step [805/3504], Loss: 0.3663\n",
      "Epoch [1/1], Step [806/3504], Loss: 0.3207\n",
      "Epoch [1/1], Step [807/3504], Loss: 0.2992\n",
      "Epoch [1/1], Step [808/3504], Loss: 0.4479\n",
      "Epoch [1/1], Step [809/3504], Loss: 0.4239\n",
      "Epoch [1/1], Step [810/3504], Loss: 0.3934\n",
      "Epoch [1/1], Step [811/3504], Loss: 0.3477\n",
      "Epoch [1/1], Step [812/3504], Loss: 0.4142\n",
      "Epoch [1/1], Step [813/3504], Loss: 0.3503\n",
      "Epoch [1/1], Step [814/3504], Loss: 0.2828\n",
      "Epoch [1/1], Step [815/3504], Loss: 0.3073\n",
      "Epoch [1/1], Step [816/3504], Loss: 0.4201\n",
      "Epoch [1/1], Step [817/3504], Loss: 0.4275\n",
      "Epoch [1/1], Step [818/3504], Loss: 0.3590\n",
      "Epoch [1/1], Step [819/3504], Loss: 0.3364\n",
      "Epoch [1/1], Step [820/3504], Loss: 0.3552\n",
      "Epoch [1/1], Step [821/3504], Loss: 0.3052\n",
      "Epoch [1/1], Step [822/3504], Loss: 0.2768\n",
      "Epoch [1/1], Step [823/3504], Loss: 0.4167\n",
      "Epoch [1/1], Step [824/3504], Loss: 0.3640\n",
      "Epoch [1/1], Step [825/3504], Loss: 0.4368\n",
      "Epoch [1/1], Step [826/3504], Loss: 0.2496\n",
      "Epoch [1/1], Step [827/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [828/3504], Loss: 0.3547\n",
      "Epoch [1/1], Step [829/3504], Loss: 0.3796\n",
      "Epoch [1/1], Step [830/3504], Loss: 0.2902\n",
      "Epoch [1/1], Step [831/3504], Loss: 0.3418\n",
      "Epoch [1/1], Step [832/3504], Loss: 0.2992\n",
      "Epoch [1/1], Step [833/3504], Loss: 0.4079\n",
      "Epoch [1/1], Step [834/3504], Loss: 0.4131\n",
      "Epoch [1/1], Step [835/3504], Loss: 0.3230\n",
      "Epoch [1/1], Step [836/3504], Loss: 0.3364\n",
      "Epoch [1/1], Step [837/3504], Loss: 0.3514\n",
      "Epoch [1/1], Step [838/3504], Loss: 0.3552\n",
      "Epoch [1/1], Step [839/3504], Loss: 0.3787\n",
      "Epoch [1/1], Step [840/3504], Loss: 0.3303\n",
      "Epoch [1/1], Step [841/3504], Loss: 0.3938\n",
      "Epoch [1/1], Step [842/3504], Loss: 0.3638\n",
      "Epoch [1/1], Step [843/3504], Loss: 0.2728\n",
      "Epoch [1/1], Step [844/3504], Loss: 0.2836\n",
      "Epoch [1/1], Step [845/3504], Loss: 0.3287\n",
      "Epoch [1/1], Step [846/3504], Loss: 0.3214\n",
      "Epoch [1/1], Step [847/3504], Loss: 0.4090\n",
      "Epoch [1/1], Step [848/3504], Loss: 0.3351\n",
      "Epoch [1/1], Step [849/3504], Loss: 0.2758\n",
      "Epoch [1/1], Step [850/3504], Loss: 0.2981\n",
      "Epoch [1/1], Step [851/3504], Loss: 0.3948\n",
      "Epoch [1/1], Step [852/3504], Loss: 0.4115\n",
      "Epoch [1/1], Step [853/3504], Loss: 0.4445\n",
      "Epoch [1/1], Step [854/3504], Loss: 0.3396\n",
      "Epoch [1/1], Step [855/3504], Loss: 0.2323\n",
      "Epoch [1/1], Step [856/3504], Loss: 0.3950\n",
      "Epoch [1/1], Step [857/3504], Loss: 0.2677\n",
      "Epoch [1/1], Step [858/3504], Loss: 0.2967\n",
      "Epoch [1/1], Step [859/3504], Loss: 0.3043\n",
      "Epoch [1/1], Step [860/3504], Loss: 0.2415\n",
      "Epoch [1/1], Step [861/3504], Loss: 0.3871\n",
      "Epoch [1/1], Step [862/3504], Loss: 0.4101\n",
      "Epoch [1/1], Step [863/3504], Loss: 0.3811\n",
      "Epoch [1/1], Step [864/3504], Loss: 0.3724\n",
      "Epoch [1/1], Step [865/3504], Loss: 0.3454\n",
      "Epoch [1/1], Step [866/3504], Loss: 0.3036\n",
      "Epoch [1/1], Step [867/3504], Loss: 0.4339\n",
      "Epoch [1/1], Step [868/3504], Loss: 0.3095\n",
      "Epoch [1/1], Step [869/3504], Loss: 0.2818\n",
      "Epoch [1/1], Step [870/3504], Loss: 0.4358\n",
      "Epoch [1/1], Step [871/3504], Loss: 0.3124\n",
      "Epoch [1/1], Step [872/3504], Loss: 0.4009\n",
      "Epoch [1/1], Step [873/3504], Loss: 0.3719\n",
      "Epoch [1/1], Step [874/3504], Loss: 0.3289\n",
      "Epoch [1/1], Step [875/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [876/3504], Loss: 0.3290\n",
      "Epoch [1/1], Step [877/3504], Loss: 0.3520\n",
      "Epoch [1/1], Step [878/3504], Loss: 0.3369\n",
      "Epoch [1/1], Step [879/3504], Loss: 0.3247\n",
      "Epoch [1/1], Step [880/3504], Loss: 0.3543\n",
      "Epoch [1/1], Step [881/3504], Loss: 0.4259\n",
      "Epoch [1/1], Step [882/3504], Loss: 0.3337\n",
      "Epoch [1/1], Step [883/3504], Loss: 0.3554\n",
      "Epoch [1/1], Step [884/3504], Loss: 0.3126\n",
      "Epoch [1/1], Step [885/3504], Loss: 0.4519\n",
      "Epoch [1/1], Step [886/3504], Loss: 0.2952\n",
      "Epoch [1/1], Step [887/3504], Loss: 0.2999\n",
      "Epoch [1/1], Step [888/3504], Loss: 0.3690\n",
      "Epoch [1/1], Step [889/3504], Loss: 0.3169\n",
      "Epoch [1/1], Step [890/3504], Loss: 0.3194\n",
      "Epoch [1/1], Step [891/3504], Loss: 0.3318\n",
      "Epoch [1/1], Step [892/3504], Loss: 0.3427\n",
      "Epoch [1/1], Step [893/3504], Loss: 0.3100\n",
      "Epoch [1/1], Step [894/3504], Loss: 0.3400\n",
      "Epoch [1/1], Step [895/3504], Loss: 0.3192\n",
      "Epoch [1/1], Step [896/3504], Loss: 0.3620\n",
      "Epoch [1/1], Step [897/3504], Loss: 0.3141\n",
      "Epoch [1/1], Step [898/3504], Loss: 0.4355\n",
      "Epoch [1/1], Step [899/3504], Loss: 0.2988\n",
      "Epoch [1/1], Step [900/3504], Loss: 0.3996\n",
      "Epoch [1/1], Step [901/3504], Loss: 0.3274\n",
      "Epoch [1/1], Step [902/3504], Loss: 0.3401\n",
      "Epoch [1/1], Step [903/3504], Loss: 0.2518\n",
      "Epoch [1/1], Step [904/3504], Loss: 0.3559\n",
      "Epoch [1/1], Step [905/3504], Loss: 0.4318\n",
      "Epoch [1/1], Step [906/3504], Loss: 0.4393\n",
      "Epoch [1/1], Step [907/3504], Loss: 0.3607\n",
      "Epoch [1/1], Step [908/3504], Loss: 0.3388\n",
      "Epoch [1/1], Step [909/3504], Loss: 0.3498\n",
      "Epoch [1/1], Step [910/3504], Loss: 0.3105\n",
      "Epoch [1/1], Step [911/3504], Loss: 0.2753\n",
      "Epoch [1/1], Step [912/3504], Loss: 0.4029\n",
      "Epoch [1/1], Step [913/3504], Loss: 0.2941\n",
      "Epoch [1/1], Step [914/3504], Loss: 0.3454\n",
      "Epoch [1/1], Step [915/3504], Loss: 0.3192\n",
      "Epoch [1/1], Step [916/3504], Loss: 0.3729\n",
      "Epoch [1/1], Step [917/3504], Loss: 0.3455\n",
      "Epoch [1/1], Step [918/3504], Loss: 0.4054\n",
      "Epoch [1/1], Step [919/3504], Loss: 0.3411\n",
      "Epoch [1/1], Step [920/3504], Loss: 0.3371\n",
      "Epoch [1/1], Step [921/3504], Loss: 0.2738\n",
      "Epoch [1/1], Step [922/3504], Loss: 0.3698\n",
      "Epoch [1/1], Step [923/3504], Loss: 0.2918\n",
      "Epoch [1/1], Step [924/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [925/3504], Loss: 0.4220\n",
      "Epoch [1/1], Step [926/3504], Loss: 0.3992\n",
      "Epoch [1/1], Step [927/3504], Loss: 0.4785\n",
      "Epoch [1/1], Step [928/3504], Loss: 0.2631\n",
      "Epoch [1/1], Step [929/3504], Loss: 0.2814\n",
      "Epoch [1/1], Step [930/3504], Loss: 0.3574\n",
      "Epoch [1/1], Step [931/3504], Loss: 0.2977\n",
      "Epoch [1/1], Step [932/3504], Loss: 0.2671\n",
      "Epoch [1/1], Step [933/3504], Loss: 0.3513\n",
      "Epoch [1/1], Step [934/3504], Loss: 0.3559\n",
      "Epoch [1/1], Step [935/3504], Loss: 0.3101\n",
      "Epoch [1/1], Step [936/3504], Loss: 0.4243\n",
      "Epoch [1/1], Step [937/3504], Loss: 0.4090\n",
      "Epoch [1/1], Step [938/3504], Loss: 0.3258\n",
      "Epoch [1/1], Step [939/3504], Loss: 0.3744\n",
      "Epoch [1/1], Step [940/3504], Loss: 0.2978\n",
      "Epoch [1/1], Step [941/3504], Loss: 0.3077\n",
      "Epoch [1/1], Step [942/3504], Loss: 0.3960\n",
      "Epoch [1/1], Step [943/3504], Loss: 0.3768\n",
      "Epoch [1/1], Step [944/3504], Loss: 0.3741\n",
      "Epoch [1/1], Step [945/3504], Loss: 0.2776\n",
      "Epoch [1/1], Step [946/3504], Loss: 0.3348\n",
      "Epoch [1/1], Step [947/3504], Loss: 0.3630\n",
      "Epoch [1/1], Step [948/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [949/3504], Loss: 0.3735\n",
      "Epoch [1/1], Step [950/3504], Loss: 0.4258\n",
      "Epoch [1/1], Step [951/3504], Loss: 0.3841\n",
      "Epoch [1/1], Step [952/3504], Loss: 0.4584\n",
      "Epoch [1/1], Step [953/3504], Loss: 0.2416\n",
      "Epoch [1/1], Step [954/3504], Loss: 0.3188\n",
      "Epoch [1/1], Step [955/3504], Loss: 0.3889\n",
      "Epoch [1/1], Step [956/3504], Loss: 0.3348\n",
      "Epoch [1/1], Step [957/3504], Loss: 0.2169\n",
      "Epoch [1/1], Step [958/3504], Loss: 0.2548\n",
      "Epoch [1/1], Step [959/3504], Loss: 0.3234\n",
      "Epoch [1/1], Step [960/3504], Loss: 0.4082\n",
      "Epoch [1/1], Step [961/3504], Loss: 0.4321\n",
      "Epoch [1/1], Step [962/3504], Loss: 0.3222\n",
      "Epoch [1/1], Step [963/3504], Loss: 0.2951\n",
      "Epoch [1/1], Step [964/3504], Loss: 0.3076\n",
      "Epoch [1/1], Step [965/3504], Loss: 0.3247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [966/3504], Loss: 0.3503\n",
      "Epoch [1/1], Step [967/3504], Loss: 0.4158\n",
      "Epoch [1/1], Step [968/3504], Loss: 0.4236\n",
      "Epoch [1/1], Step [969/3504], Loss: 0.3591\n",
      "Epoch [1/1], Step [970/3504], Loss: 0.3533\n",
      "Epoch [1/1], Step [971/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [972/3504], Loss: 0.3828\n",
      "Epoch [1/1], Step [973/3504], Loss: 0.3390\n",
      "Epoch [1/1], Step [974/3504], Loss: 0.3651\n",
      "Epoch [1/1], Step [975/3504], Loss: 0.2869\n",
      "Epoch [1/1], Step [976/3504], Loss: 0.4959\n",
      "Epoch [1/1], Step [977/3504], Loss: 0.3502\n",
      "Epoch [1/1], Step [978/3504], Loss: 0.3703\n",
      "Epoch [1/1], Step [979/3504], Loss: 0.3876\n",
      "Epoch [1/1], Step [980/3504], Loss: 0.2852\n",
      "Epoch [1/1], Step [981/3504], Loss: 0.3671\n",
      "Epoch [1/1], Step [982/3504], Loss: 0.3621\n",
      "Epoch [1/1], Step [983/3504], Loss: 0.3306\n",
      "Epoch [1/1], Step [984/3504], Loss: 0.3053\n",
      "Epoch [1/1], Step [985/3504], Loss: 0.2779\n",
      "Epoch [1/1], Step [986/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [987/3504], Loss: 0.3243\n",
      "Epoch [1/1], Step [988/3504], Loss: 0.3382\n",
      "Epoch [1/1], Step [989/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [990/3504], Loss: 0.3272\n",
      "Epoch [1/1], Step [991/3504], Loss: 0.2925\n",
      "Epoch [1/1], Step [992/3504], Loss: 0.4264\n",
      "Epoch [1/1], Step [993/3504], Loss: 0.3098\n",
      "Epoch [1/1], Step [994/3504], Loss: 0.3679\n",
      "Epoch [1/1], Step [995/3504], Loss: 0.4372\n",
      "Epoch [1/1], Step [996/3504], Loss: 0.4057\n",
      "Epoch [1/1], Step [997/3504], Loss: 0.2405\n",
      "Epoch [1/1], Step [998/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [999/3504], Loss: 0.2855\n",
      "Epoch [1/1], Step [1000/3504], Loss: 0.2776\n",
      "Epoch [1/1], Step [1001/3504], Loss: 0.2927\n",
      "Epoch [1/1], Step [1002/3504], Loss: 0.3479\n",
      "Epoch [1/1], Step [1003/3504], Loss: 0.3655\n",
      "Epoch [1/1], Step [1004/3504], Loss: 0.3869\n",
      "Epoch [1/1], Step [1005/3504], Loss: 0.3138\n",
      "Epoch [1/1], Step [1006/3504], Loss: 0.3474\n",
      "Epoch [1/1], Step [1007/3504], Loss: 0.3691\n",
      "Epoch [1/1], Step [1008/3504], Loss: 0.3020\n",
      "Epoch [1/1], Step [1009/3504], Loss: 0.3827\n",
      "Epoch [1/1], Step [1010/3504], Loss: 0.3373\n",
      "Epoch [1/1], Step [1011/3504], Loss: 0.3390\n",
      "Epoch [1/1], Step [1012/3504], Loss: 0.4451\n",
      "Epoch [1/1], Step [1013/3504], Loss: 0.4447\n",
      "Epoch [1/1], Step [1014/3504], Loss: 0.4229\n",
      "Epoch [1/1], Step [1015/3504], Loss: 0.3320\n",
      "Epoch [1/1], Step [1016/3504], Loss: 0.2960\n",
      "Epoch [1/1], Step [1017/3504], Loss: 0.3326\n",
      "Epoch [1/1], Step [1018/3504], Loss: 0.5425\n",
      "Epoch [1/1], Step [1019/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [1020/3504], Loss: 0.2969\n",
      "Epoch [1/1], Step [1021/3504], Loss: 0.2536\n",
      "Epoch [1/1], Step [1022/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [1023/3504], Loss: 0.3628\n",
      "Epoch [1/1], Step [1024/3504], Loss: 0.3528\n",
      "Epoch [1/1], Step [1025/3504], Loss: 0.3433\n",
      "Epoch [1/1], Step [1026/3504], Loss: 0.3379\n",
      "Epoch [1/1], Step [1027/3504], Loss: 0.3040\n",
      "Epoch [1/1], Step [1028/3504], Loss: 0.3134\n",
      "Epoch [1/1], Step [1029/3504], Loss: 0.3257\n",
      "Epoch [1/1], Step [1030/3504], Loss: 0.2907\n",
      "Epoch [1/1], Step [1031/3504], Loss: 0.2941\n",
      "Epoch [1/1], Step [1032/3504], Loss: 0.4835\n",
      "Epoch [1/1], Step [1033/3504], Loss: 0.3768\n",
      "Epoch [1/1], Step [1034/3504], Loss: 0.4757\n",
      "Epoch [1/1], Step [1035/3504], Loss: 0.3466\n",
      "Epoch [1/1], Step [1036/3504], Loss: 0.2762\n",
      "Epoch [1/1], Step [1037/3504], Loss: 0.2672\n",
      "Epoch [1/1], Step [1038/3504], Loss: 0.3169\n",
      "Epoch [1/1], Step [1039/3504], Loss: 0.3196\n",
      "Epoch [1/1], Step [1040/3504], Loss: 0.3952\n",
      "Epoch [1/1], Step [1041/3504], Loss: 0.3161\n",
      "Epoch [1/1], Step [1042/3504], Loss: 0.4209\n",
      "Epoch [1/1], Step [1043/3504], Loss: 0.3972\n",
      "Epoch [1/1], Step [1044/3504], Loss: 0.3355\n",
      "Epoch [1/1], Step [1045/3504], Loss: 0.3702\n",
      "Epoch [1/1], Step [1046/3504], Loss: 0.4157\n",
      "Epoch [1/1], Step [1047/3504], Loss: 0.3561\n",
      "Epoch [1/1], Step [1048/3504], Loss: 0.3483\n",
      "Epoch [1/1], Step [1049/3504], Loss: 0.3401\n",
      "Epoch [1/1], Step [1050/3504], Loss: 0.3965\n",
      "Epoch [1/1], Step [1051/3504], Loss: 0.3177\n",
      "Epoch [1/1], Step [1052/3504], Loss: 0.2957\n",
      "Epoch [1/1], Step [1053/3504], Loss: 0.3276\n",
      "Epoch [1/1], Step [1054/3504], Loss: 0.3288\n",
      "Epoch [1/1], Step [1055/3504], Loss: 0.3351\n",
      "Epoch [1/1], Step [1056/3504], Loss: 0.3689\n",
      "Epoch [1/1], Step [1057/3504], Loss: 0.3534\n",
      "Epoch [1/1], Step [1058/3504], Loss: 0.2993\n",
      "Epoch [1/1], Step [1059/3504], Loss: 0.3937\n",
      "Epoch [1/1], Step [1060/3504], Loss: 0.3289\n",
      "Epoch [1/1], Step [1061/3504], Loss: 0.2950\n",
      "Epoch [1/1], Step [1062/3504], Loss: 0.3167\n",
      "Epoch [1/1], Step [1063/3504], Loss: 0.4335\n",
      "Epoch [1/1], Step [1064/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [1065/3504], Loss: 0.3729\n",
      "Epoch [1/1], Step [1066/3504], Loss: 0.3406\n",
      "Epoch [1/1], Step [1067/3504], Loss: 0.4166\n",
      "Epoch [1/1], Step [1068/3504], Loss: 0.3745\n",
      "Epoch [1/1], Step [1069/3504], Loss: 0.3253\n",
      "Epoch [1/1], Step [1070/3504], Loss: 0.2608\n",
      "Epoch [1/1], Step [1071/3504], Loss: 0.3147\n",
      "Epoch [1/1], Step [1072/3504], Loss: 0.2568\n",
      "Epoch [1/1], Step [1073/3504], Loss: 0.2772\n",
      "Epoch [1/1], Step [1074/3504], Loss: 0.4284\n",
      "Epoch [1/1], Step [1075/3504], Loss: 0.4753\n",
      "Epoch [1/1], Step [1076/3504], Loss: 0.3917\n",
      "Epoch [1/1], Step [1077/3504], Loss: 0.2898\n",
      "Epoch [1/1], Step [1078/3504], Loss: 0.3095\n",
      "Epoch [1/1], Step [1079/3504], Loss: 0.3820\n",
      "Epoch [1/1], Step [1080/3504], Loss: 0.2981\n",
      "Epoch [1/1], Step [1081/3504], Loss: 0.2791\n",
      "Epoch [1/1], Step [1082/3504], Loss: 0.3387\n",
      "Epoch [1/1], Step [1083/3504], Loss: 0.4347\n",
      "Epoch [1/1], Step [1084/3504], Loss: 0.4077\n",
      "Epoch [1/1], Step [1085/3504], Loss: 0.3888\n",
      "Epoch [1/1], Step [1086/3504], Loss: 0.3889\n",
      "Epoch [1/1], Step [1087/3504], Loss: 0.3300\n",
      "Epoch [1/1], Step [1088/3504], Loss: 0.2740\n",
      "Epoch [1/1], Step [1089/3504], Loss: 0.4559\n",
      "Epoch [1/1], Step [1090/3504], Loss: 0.3069\n",
      "Epoch [1/1], Step [1091/3504], Loss: 0.3646\n",
      "Epoch [1/1], Step [1092/3504], Loss: 0.3807\n",
      "Epoch [1/1], Step [1093/3504], Loss: 0.3946\n",
      "Epoch [1/1], Step [1094/3504], Loss: 0.4160\n",
      "Epoch [1/1], Step [1095/3504], Loss: 0.3774\n",
      "Epoch [1/1], Step [1096/3504], Loss: 0.3120\n",
      "Epoch [1/1], Step [1097/3504], Loss: 0.3721\n",
      "Epoch [1/1], Step [1098/3504], Loss: 0.3265\n",
      "Epoch [1/1], Step [1099/3504], Loss: 0.2870\n",
      "Epoch [1/1], Step [1100/3504], Loss: 0.4302\n",
      "Epoch [1/1], Step [1101/3504], Loss: 0.3871\n",
      "Epoch [1/1], Step [1102/3504], Loss: 0.2817\n",
      "Epoch [1/1], Step [1103/3504], Loss: 0.3153\n",
      "Epoch [1/1], Step [1104/3504], Loss: 0.3933\n",
      "Epoch [1/1], Step [1105/3504], Loss: 0.3762\n",
      "Epoch [1/1], Step [1106/3504], Loss: 0.3148\n",
      "Epoch [1/1], Step [1107/3504], Loss: 0.3727\n",
      "Epoch [1/1], Step [1108/3504], Loss: 0.2877\n",
      "Epoch [1/1], Step [1109/3504], Loss: 0.3271\n",
      "Epoch [1/1], Step [1110/3504], Loss: 0.3868\n",
      "Epoch [1/1], Step [1111/3504], Loss: 0.3595\n",
      "Epoch [1/1], Step [1112/3504], Loss: 0.3741\n",
      "Epoch [1/1], Step [1113/3504], Loss: 0.3634\n",
      "Epoch [1/1], Step [1114/3504], Loss: 0.4576\n",
      "Epoch [1/1], Step [1115/3504], Loss: 0.3932\n",
      "Epoch [1/1], Step [1116/3504], Loss: 0.4351\n",
      "Epoch [1/1], Step [1117/3504], Loss: 0.4164\n",
      "Epoch [1/1], Step [1118/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [1119/3504], Loss: 0.3521\n",
      "Epoch [1/1], Step [1120/3504], Loss: 0.3794\n",
      "Epoch [1/1], Step [1121/3504], Loss: 0.4689\n",
      "Epoch [1/1], Step [1122/3504], Loss: 0.3460\n",
      "Epoch [1/1], Step [1123/3504], Loss: 0.3618\n",
      "Epoch [1/1], Step [1124/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [1125/3504], Loss: 0.2849\n",
      "Epoch [1/1], Step [1126/3504], Loss: 0.3542\n",
      "Epoch [1/1], Step [1127/3504], Loss: 0.3698\n",
      "Epoch [1/1], Step [1128/3504], Loss: 0.2999\n",
      "Epoch [1/1], Step [1129/3504], Loss: 0.3265\n",
      "Epoch [1/1], Step [1130/3504], Loss: 0.3167\n",
      "Epoch [1/1], Step [1131/3504], Loss: 0.3123\n",
      "Epoch [1/1], Step [1132/3504], Loss: 0.3190\n",
      "Epoch [1/1], Step [1133/3504], Loss: 0.3556\n",
      "Epoch [1/1], Step [1134/3504], Loss: 0.3306\n",
      "Epoch [1/1], Step [1135/3504], Loss: 0.3580\n",
      "Epoch [1/1], Step [1136/3504], Loss: 0.3948\n",
      "Epoch [1/1], Step [1137/3504], Loss: 0.3310\n",
      "Epoch [1/1], Step [1138/3504], Loss: 0.3658\n",
      "Epoch [1/1], Step [1139/3504], Loss: 0.3067\n",
      "Epoch [1/1], Step [1140/3504], Loss: 0.4495\n",
      "Epoch [1/1], Step [1141/3504], Loss: 0.3188\n",
      "Epoch [1/1], Step [1142/3504], Loss: 0.3558\n",
      "Epoch [1/1], Step [1143/3504], Loss: 0.4402\n",
      "Epoch [1/1], Step [1144/3504], Loss: 0.3096\n",
      "Epoch [1/1], Step [1145/3504], Loss: 0.3183\n",
      "Epoch [1/1], Step [1146/3504], Loss: 0.3292\n",
      "Epoch [1/1], Step [1147/3504], Loss: 0.3210\n",
      "Epoch [1/1], Step [1148/3504], Loss: 0.3219\n",
      "Epoch [1/1], Step [1149/3504], Loss: 0.4251\n",
      "Epoch [1/1], Step [1150/3504], Loss: 0.3102\n",
      "Epoch [1/1], Step [1151/3504], Loss: 0.3119\n",
      "Epoch [1/1], Step [1152/3504], Loss: 0.4145\n",
      "Epoch [1/1], Step [1153/3504], Loss: 0.3549\n",
      "Epoch [1/1], Step [1154/3504], Loss: 0.3802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1155/3504], Loss: 0.4017\n",
      "Epoch [1/1], Step [1156/3504], Loss: 0.4554\n",
      "Epoch [1/1], Step [1157/3504], Loss: 0.3294\n",
      "Epoch [1/1], Step [1158/3504], Loss: 0.4770\n",
      "Epoch [1/1], Step [1159/3504], Loss: 0.2883\n",
      "Epoch [1/1], Step [1160/3504], Loss: 0.3801\n",
      "Epoch [1/1], Step [1161/3504], Loss: 0.3714\n",
      "Epoch [1/1], Step [1162/3504], Loss: 0.3044\n",
      "Epoch [1/1], Step [1163/3504], Loss: 0.2752\n",
      "Epoch [1/1], Step [1164/3504], Loss: 0.3771\n",
      "Epoch [1/1], Step [1165/3504], Loss: 0.3853\n",
      "Epoch [1/1], Step [1166/3504], Loss: 0.2888\n",
      "Epoch [1/1], Step [1167/3504], Loss: 0.3521\n",
      "Epoch [1/1], Step [1168/3504], Loss: 0.3394\n",
      "Epoch [1/1], Step [1169/3504], Loss: 0.3595\n",
      "Epoch [1/1], Step [1170/3504], Loss: 0.3244\n",
      "Epoch [1/1], Step [1171/3504], Loss: 0.3056\n",
      "Epoch [1/1], Step [1172/3504], Loss: 0.4077\n",
      "Epoch [1/1], Step [1173/3504], Loss: 0.3817\n",
      "Epoch [1/1], Step [1174/3504], Loss: 0.4495\n",
      "Epoch [1/1], Step [1175/3504], Loss: 0.2800\n",
      "Epoch [1/1], Step [1176/3504], Loss: 0.3877\n",
      "Epoch [1/1], Step [1177/3504], Loss: 0.3406\n",
      "Epoch [1/1], Step [1178/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [1179/3504], Loss: 0.3506\n",
      "Epoch [1/1], Step [1180/3504], Loss: 0.3460\n",
      "Epoch [1/1], Step [1181/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [1182/3504], Loss: 0.3358\n",
      "Epoch [1/1], Step [1183/3504], Loss: 0.3298\n",
      "Epoch [1/1], Step [1184/3504], Loss: 0.4175\n",
      "Epoch [1/1], Step [1185/3504], Loss: 0.3126\n",
      "Epoch [1/1], Step [1186/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [1187/3504], Loss: 0.2698\n",
      "Epoch [1/1], Step [1188/3504], Loss: 0.3466\n",
      "Epoch [1/1], Step [1189/3504], Loss: 0.3312\n",
      "Epoch [1/1], Step [1190/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [1191/3504], Loss: 0.3212\n",
      "Epoch [1/1], Step [1192/3504], Loss: 0.2949\n",
      "Epoch [1/1], Step [1193/3504], Loss: 0.3070\n",
      "Epoch [1/1], Step [1194/3504], Loss: 0.2727\n",
      "Epoch [1/1], Step [1195/3504], Loss: 0.3145\n",
      "Epoch [1/1], Step [1196/3504], Loss: 0.4320\n",
      "Epoch [1/1], Step [1197/3504], Loss: 0.2986\n",
      "Epoch [1/1], Step [1198/3504], Loss: 0.4792\n",
      "Epoch [1/1], Step [1199/3504], Loss: 0.4128\n",
      "Epoch [1/1], Step [1200/3504], Loss: 0.3382\n",
      "Epoch [1/1], Step [1201/3504], Loss: 0.3409\n",
      "Epoch [1/1], Step [1202/3504], Loss: 0.2988\n",
      "Epoch [1/1], Step [1203/3504], Loss: 0.3714\n",
      "Epoch [1/1], Step [1204/3504], Loss: 0.3112\n",
      "Epoch [1/1], Step [1205/3504], Loss: 0.3855\n",
      "Epoch [1/1], Step [1206/3504], Loss: 0.3413\n",
      "Epoch [1/1], Step [1207/3504], Loss: 0.3622\n",
      "Epoch [1/1], Step [1208/3504], Loss: 0.2890\n",
      "Epoch [1/1], Step [1209/3504], Loss: 0.3112\n",
      "Epoch [1/1], Step [1210/3504], Loss: 0.3011\n",
      "Epoch [1/1], Step [1211/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [1212/3504], Loss: 0.2844\n",
      "Epoch [1/1], Step [1213/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [1214/3504], Loss: 0.2812\n",
      "Epoch [1/1], Step [1215/3504], Loss: 0.3822\n",
      "Epoch [1/1], Step [1216/3504], Loss: 0.3848\n",
      "Epoch [1/1], Step [1217/3504], Loss: 0.3459\n",
      "Epoch [1/1], Step [1218/3504], Loss: 0.2686\n",
      "Epoch [1/1], Step [1219/3504], Loss: 0.3444\n",
      "Epoch [1/1], Step [1220/3504], Loss: 0.3157\n",
      "Epoch [1/1], Step [1221/3504], Loss: 0.3948\n",
      "Epoch [1/1], Step [1222/3504], Loss: 0.3243\n",
      "Epoch [1/1], Step [1223/3504], Loss: 0.2800\n",
      "Epoch [1/1], Step [1224/3504], Loss: 0.4139\n",
      "Epoch [1/1], Step [1225/3504], Loss: 0.3129\n",
      "Epoch [1/1], Step [1226/3504], Loss: 0.3659\n",
      "Epoch [1/1], Step [1227/3504], Loss: 0.3627\n",
      "Epoch [1/1], Step [1228/3504], Loss: 0.3498\n",
      "Epoch [1/1], Step [1229/3504], Loss: 0.3646\n",
      "Epoch [1/1], Step [1230/3504], Loss: 0.3194\n",
      "Epoch [1/1], Step [1231/3504], Loss: 0.2951\n",
      "Epoch [1/1], Step [1232/3504], Loss: 0.3790\n",
      "Epoch [1/1], Step [1233/3504], Loss: 0.3657\n",
      "Epoch [1/1], Step [1234/3504], Loss: 0.2768\n",
      "Epoch [1/1], Step [1235/3504], Loss: 0.3294\n",
      "Epoch [1/1], Step [1236/3504], Loss: 0.3204\n",
      "Epoch [1/1], Step [1237/3504], Loss: 0.3458\n",
      "Epoch [1/1], Step [1238/3504], Loss: 0.3175\n",
      "Epoch [1/1], Step [1239/3504], Loss: 0.3194\n",
      "Epoch [1/1], Step [1240/3504], Loss: 0.2613\n",
      "Epoch [1/1], Step [1241/3504], Loss: 0.2557\n",
      "Epoch [1/1], Step [1242/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [1243/3504], Loss: 0.2671\n",
      "Epoch [1/1], Step [1244/3504], Loss: 0.3531\n",
      "Epoch [1/1], Step [1245/3504], Loss: 0.3439\n",
      "Epoch [1/1], Step [1246/3504], Loss: 0.4294\n",
      "Epoch [1/1], Step [1247/3504], Loss: 0.2589\n",
      "Epoch [1/1], Step [1248/3504], Loss: 0.2712\n",
      "Epoch [1/1], Step [1249/3504], Loss: 0.2800\n",
      "Epoch [1/1], Step [1250/3504], Loss: 0.3339\n",
      "Epoch [1/1], Step [1251/3504], Loss: 0.3292\n",
      "Epoch [1/1], Step [1252/3504], Loss: 0.3115\n",
      "Epoch [1/1], Step [1253/3504], Loss: 0.3443\n",
      "Epoch [1/1], Step [1254/3504], Loss: 0.3300\n",
      "Epoch [1/1], Step [1255/3504], Loss: 0.3459\n",
      "Epoch [1/1], Step [1256/3504], Loss: 0.4352\n",
      "Epoch [1/1], Step [1257/3504], Loss: 0.3589\n",
      "Epoch [1/1], Step [1258/3504], Loss: 0.3732\n",
      "Epoch [1/1], Step [1259/3504], Loss: 0.3359\n",
      "Epoch [1/1], Step [1260/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [1261/3504], Loss: 0.3621\n",
      "Epoch [1/1], Step [1262/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [1263/3504], Loss: 0.3163\n",
      "Epoch [1/1], Step [1264/3504], Loss: 0.3923\n",
      "Epoch [1/1], Step [1265/3504], Loss: 0.3202\n",
      "Epoch [1/1], Step [1266/3504], Loss: 0.3185\n",
      "Epoch [1/1], Step [1267/3504], Loss: 0.3463\n",
      "Epoch [1/1], Step [1268/3504], Loss: 0.3428\n",
      "Epoch [1/1], Step [1269/3504], Loss: 0.3396\n",
      "Epoch [1/1], Step [1270/3504], Loss: 0.2909\n",
      "Epoch [1/1], Step [1271/3504], Loss: 0.3726\n",
      "Epoch [1/1], Step [1272/3504], Loss: 0.3446\n",
      "Epoch [1/1], Step [1273/3504], Loss: 0.3342\n",
      "Epoch [1/1], Step [1274/3504], Loss: 0.3631\n",
      "Epoch [1/1], Step [1275/3504], Loss: 0.3125\n",
      "Epoch [1/1], Step [1276/3504], Loss: 0.4064\n",
      "Epoch [1/1], Step [1277/3504], Loss: 0.2832\n",
      "Epoch [1/1], Step [1278/3504], Loss: 0.3350\n",
      "Epoch [1/1], Step [1279/3504], Loss: 0.3388\n",
      "Epoch [1/1], Step [1280/3504], Loss: 0.3867\n",
      "Epoch [1/1], Step [1281/3504], Loss: 0.2439\n",
      "Epoch [1/1], Step [1282/3504], Loss: 0.3375\n",
      "Epoch [1/1], Step [1283/3504], Loss: 0.3157\n",
      "Epoch [1/1], Step [1284/3504], Loss: 0.3237\n",
      "Epoch [1/1], Step [1285/3504], Loss: 0.3658\n",
      "Epoch [1/1], Step [1286/3504], Loss: 0.3856\n",
      "Epoch [1/1], Step [1287/3504], Loss: 0.4015\n",
      "Epoch [1/1], Step [1288/3504], Loss: 0.3022\n",
      "Epoch [1/1], Step [1289/3504], Loss: 0.2701\n",
      "Epoch [1/1], Step [1290/3504], Loss: 0.3280\n",
      "Epoch [1/1], Step [1291/3504], Loss: 0.2862\n",
      "Epoch [1/1], Step [1292/3504], Loss: 0.3381\n",
      "Epoch [1/1], Step [1293/3504], Loss: 0.5412\n",
      "Epoch [1/1], Step [1294/3504], Loss: 0.2768\n",
      "Epoch [1/1], Step [1295/3504], Loss: 0.3750\n",
      "Epoch [1/1], Step [1296/3504], Loss: 0.3577\n",
      "Epoch [1/1], Step [1297/3504], Loss: 0.3612\n",
      "Epoch [1/1], Step [1298/3504], Loss: 0.3131\n",
      "Epoch [1/1], Step [1299/3504], Loss: 0.2780\n",
      "Epoch [1/1], Step [1300/3504], Loss: 0.2689\n",
      "Epoch [1/1], Step [1301/3504], Loss: 0.3489\n",
      "Epoch [1/1], Step [1302/3504], Loss: 0.3315\n",
      "Epoch [1/1], Step [1303/3504], Loss: 0.3189\n",
      "Epoch [1/1], Step [1304/3504], Loss: 0.3268\n",
      "Epoch [1/1], Step [1305/3504], Loss: 0.4460\n",
      "Epoch [1/1], Step [1306/3504], Loss: 0.3203\n",
      "Epoch [1/1], Step [1307/3504], Loss: 0.3081\n",
      "Epoch [1/1], Step [1308/3504], Loss: 0.3512\n",
      "Epoch [1/1], Step [1309/3504], Loss: 0.3423\n",
      "Epoch [1/1], Step [1310/3504], Loss: 0.3058\n",
      "Epoch [1/1], Step [1311/3504], Loss: 0.4088\n",
      "Epoch [1/1], Step [1312/3504], Loss: 0.4433\n",
      "Epoch [1/1], Step [1313/3504], Loss: 0.2923\n",
      "Epoch [1/1], Step [1314/3504], Loss: 0.4106\n",
      "Epoch [1/1], Step [1315/3504], Loss: 0.3368\n",
      "Epoch [1/1], Step [1316/3504], Loss: 0.3948\n",
      "Epoch [1/1], Step [1317/3504], Loss: 0.2450\n",
      "Epoch [1/1], Step [1318/3504], Loss: 0.3327\n",
      "Epoch [1/1], Step [1319/3504], Loss: 0.3119\n",
      "Epoch [1/1], Step [1320/3504], Loss: 0.3352\n",
      "Epoch [1/1], Step [1321/3504], Loss: 0.4132\n",
      "Epoch [1/1], Step [1322/3504], Loss: 0.3555\n",
      "Epoch [1/1], Step [1323/3504], Loss: 0.4121\n",
      "Epoch [1/1], Step [1324/3504], Loss: 0.3264\n",
      "Epoch [1/1], Step [1325/3504], Loss: 0.3265\n",
      "Epoch [1/1], Step [1326/3504], Loss: 0.3478\n",
      "Epoch [1/1], Step [1327/3504], Loss: 0.3356\n",
      "Epoch [1/1], Step [1328/3504], Loss: 0.3854\n",
      "Epoch [1/1], Step [1329/3504], Loss: 0.3757\n",
      "Epoch [1/1], Step [1330/3504], Loss: 0.3751\n",
      "Epoch [1/1], Step [1331/3504], Loss: 0.3119\n",
      "Epoch [1/1], Step [1332/3504], Loss: 0.2926\n",
      "Epoch [1/1], Step [1333/3504], Loss: 0.3106\n",
      "Epoch [1/1], Step [1334/3504], Loss: 0.3017\n",
      "Epoch [1/1], Step [1335/3504], Loss: 0.3232\n",
      "Epoch [1/1], Step [1336/3504], Loss: 0.3497\n",
      "Epoch [1/1], Step [1337/3504], Loss: 0.2731\n",
      "Epoch [1/1], Step [1338/3504], Loss: 0.2855\n",
      "Epoch [1/1], Step [1339/3504], Loss: 0.3866\n",
      "Epoch [1/1], Step [1340/3504], Loss: 0.3925\n",
      "Epoch [1/1], Step [1341/3504], Loss: 0.3826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1342/3504], Loss: 0.3701\n",
      "Epoch [1/1], Step [1343/3504], Loss: 0.3952\n",
      "Epoch [1/1], Step [1344/3504], Loss: 0.3493\n",
      "Epoch [1/1], Step [1345/3504], Loss: 0.3807\n",
      "Epoch [1/1], Step [1346/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [1347/3504], Loss: 0.2566\n",
      "Epoch [1/1], Step [1348/3504], Loss: 0.3761\n",
      "Epoch [1/1], Step [1349/3504], Loss: 0.3778\n",
      "Epoch [1/1], Step [1350/3504], Loss: 0.2967\n",
      "Epoch [1/1], Step [1351/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [1352/3504], Loss: 0.3384\n",
      "Epoch [1/1], Step [1353/3504], Loss: 0.4492\n",
      "Epoch [1/1], Step [1354/3504], Loss: 0.4048\n",
      "Epoch [1/1], Step [1355/3504], Loss: 0.3940\n",
      "Epoch [1/1], Step [1356/3504], Loss: 0.3266\n",
      "Epoch [1/1], Step [1357/3504], Loss: 0.3730\n",
      "Epoch [1/1], Step [1358/3504], Loss: 0.3909\n",
      "Epoch [1/1], Step [1359/3504], Loss: 0.3784\n",
      "Epoch [1/1], Step [1360/3504], Loss: 0.3137\n",
      "Epoch [1/1], Step [1361/3504], Loss: 0.3074\n",
      "Epoch [1/1], Step [1362/3504], Loss: 0.2988\n",
      "Epoch [1/1], Step [1363/3504], Loss: 0.3726\n",
      "Epoch [1/1], Step [1364/3504], Loss: 0.4342\n",
      "Epoch [1/1], Step [1365/3504], Loss: 0.3268\n",
      "Epoch [1/1], Step [1366/3504], Loss: 0.3707\n",
      "Epoch [1/1], Step [1367/3504], Loss: 0.3357\n",
      "Epoch [1/1], Step [1368/3504], Loss: 0.2778\n",
      "Epoch [1/1], Step [1369/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [1370/3504], Loss: 0.3308\n",
      "Epoch [1/1], Step [1371/3504], Loss: 0.2800\n",
      "Epoch [1/1], Step [1372/3504], Loss: 0.2922\n",
      "Epoch [1/1], Step [1373/3504], Loss: 0.3721\n",
      "Epoch [1/1], Step [1374/3504], Loss: 0.3466\n",
      "Epoch [1/1], Step [1375/3504], Loss: 0.3939\n",
      "Epoch [1/1], Step [1376/3504], Loss: 0.3171\n",
      "Epoch [1/1], Step [1377/3504], Loss: 0.3105\n",
      "Epoch [1/1], Step [1378/3504], Loss: 0.4295\n",
      "Epoch [1/1], Step [1379/3504], Loss: 0.3288\n",
      "Epoch [1/1], Step [1380/3504], Loss: 0.3067\n",
      "Epoch [1/1], Step [1381/3504], Loss: 0.3345\n",
      "Epoch [1/1], Step [1382/3504], Loss: 0.3599\n",
      "Epoch [1/1], Step [1383/3504], Loss: 0.3683\n",
      "Epoch [1/1], Step [1384/3504], Loss: 0.3205\n",
      "Epoch [1/1], Step [1385/3504], Loss: 0.3271\n",
      "Epoch [1/1], Step [1386/3504], Loss: 0.3823\n",
      "Epoch [1/1], Step [1387/3504], Loss: 0.3278\n",
      "Epoch [1/1], Step [1388/3504], Loss: 0.3372\n",
      "Epoch [1/1], Step [1389/3504], Loss: 0.3261\n",
      "Epoch [1/1], Step [1390/3504], Loss: 0.3893\n",
      "Epoch [1/1], Step [1391/3504], Loss: 0.3392\n",
      "Epoch [1/1], Step [1392/3504], Loss: 0.2773\n",
      "Epoch [1/1], Step [1393/3504], Loss: 0.4574\n",
      "Epoch [1/1], Step [1394/3504], Loss: 0.3524\n",
      "Epoch [1/1], Step [1395/3504], Loss: 0.3725\n",
      "Epoch [1/1], Step [1396/3504], Loss: 0.2856\n",
      "Epoch [1/1], Step [1397/3504], Loss: 0.2867\n",
      "Epoch [1/1], Step [1398/3504], Loss: 0.3789\n",
      "Epoch [1/1], Step [1399/3504], Loss: 0.3645\n",
      "Epoch [1/1], Step [1400/3504], Loss: 0.2680\n",
      "Epoch [1/1], Step [1401/3504], Loss: 0.2940\n",
      "Epoch [1/1], Step [1402/3504], Loss: 0.4081\n",
      "Epoch [1/1], Step [1403/3504], Loss: 0.4282\n",
      "Epoch [1/1], Step [1404/3504], Loss: 0.3104\n",
      "Epoch [1/1], Step [1405/3504], Loss: 0.3402\n",
      "Epoch [1/1], Step [1406/3504], Loss: 0.3189\n",
      "Epoch [1/1], Step [1407/3504], Loss: 0.2967\n",
      "Epoch [1/1], Step [1408/3504], Loss: 0.3730\n",
      "Epoch [1/1], Step [1409/3504], Loss: 0.2938\n",
      "Epoch [1/1], Step [1410/3504], Loss: 0.4337\n",
      "Epoch [1/1], Step [1411/3504], Loss: 0.3519\n",
      "Epoch [1/1], Step [1412/3504], Loss: 0.4018\n",
      "Epoch [1/1], Step [1413/3504], Loss: 0.4107\n",
      "Epoch [1/1], Step [1414/3504], Loss: 0.2818\n",
      "Epoch [1/1], Step [1415/3504], Loss: 0.4172\n",
      "Epoch [1/1], Step [1416/3504], Loss: 0.3352\n",
      "Epoch [1/1], Step [1417/3504], Loss: 0.4337\n",
      "Epoch [1/1], Step [1418/3504], Loss: 0.3936\n",
      "Epoch [1/1], Step [1419/3504], Loss: 0.3726\n",
      "Epoch [1/1], Step [1420/3504], Loss: 0.3588\n",
      "Epoch [1/1], Step [1421/3504], Loss: 0.3631\n",
      "Epoch [1/1], Step [1422/3504], Loss: 0.4000\n",
      "Epoch [1/1], Step [1423/3504], Loss: 0.2892\n",
      "Epoch [1/1], Step [1424/3504], Loss: 0.3434\n",
      "Epoch [1/1], Step [1425/3504], Loss: 0.4021\n",
      "Epoch [1/1], Step [1426/3504], Loss: 0.4128\n",
      "Epoch [1/1], Step [1427/3504], Loss: 0.3868\n",
      "Epoch [1/1], Step [1428/3504], Loss: 0.4099\n",
      "Epoch [1/1], Step [1429/3504], Loss: 0.2708\n",
      "Epoch [1/1], Step [1430/3504], Loss: 0.3632\n",
      "Epoch [1/1], Step [1431/3504], Loss: 0.3202\n",
      "Epoch [1/1], Step [1432/3504], Loss: 0.3754\n",
      "Epoch [1/1], Step [1433/3504], Loss: 0.3181\n",
      "Epoch [1/1], Step [1434/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [1435/3504], Loss: 0.3160\n",
      "Epoch [1/1], Step [1436/3504], Loss: 0.2784\n",
      "Epoch [1/1], Step [1437/3504], Loss: 0.3839\n",
      "Epoch [1/1], Step [1438/3504], Loss: 0.4069\n",
      "Epoch [1/1], Step [1439/3504], Loss: 0.2769\n",
      "Epoch [1/1], Step [1440/3504], Loss: 0.3595\n",
      "Epoch [1/1], Step [1441/3504], Loss: 0.3078\n",
      "Epoch [1/1], Step [1442/3504], Loss: 0.4187\n",
      "Epoch [1/1], Step [1443/3504], Loss: 0.2941\n",
      "Epoch [1/1], Step [1444/3504], Loss: 0.3542\n",
      "Epoch [1/1], Step [1445/3504], Loss: 0.3355\n",
      "Epoch [1/1], Step [1446/3504], Loss: 0.4334\n",
      "Epoch [1/1], Step [1447/3504], Loss: 0.3305\n",
      "Epoch [1/1], Step [1448/3504], Loss: 0.3603\n",
      "Epoch [1/1], Step [1449/3504], Loss: 0.3553\n",
      "Epoch [1/1], Step [1450/3504], Loss: 0.3163\n",
      "Epoch [1/1], Step [1451/3504], Loss: 0.2659\n",
      "Epoch [1/1], Step [1452/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [1453/3504], Loss: 0.3159\n",
      "Epoch [1/1], Step [1454/3504], Loss: 0.3571\n",
      "Epoch [1/1], Step [1455/3504], Loss: 0.3901\n",
      "Epoch [1/1], Step [1456/3504], Loss: 0.3194\n",
      "Epoch [1/1], Step [1457/3504], Loss: 0.2983\n",
      "Epoch [1/1], Step [1458/3504], Loss: 0.3860\n",
      "Epoch [1/1], Step [1459/3504], Loss: 0.4143\n",
      "Epoch [1/1], Step [1460/3504], Loss: 0.3776\n",
      "Epoch [1/1], Step [1461/3504], Loss: 0.3955\n",
      "Epoch [1/1], Step [1462/3504], Loss: 0.3052\n",
      "Epoch [1/1], Step [1463/3504], Loss: 0.3576\n",
      "Epoch [1/1], Step [1464/3504], Loss: 0.4406\n",
      "Epoch [1/1], Step [1465/3504], Loss: 0.5319\n",
      "Epoch [1/1], Step [1466/3504], Loss: 0.3544\n",
      "Epoch [1/1], Step [1467/3504], Loss: 0.3112\n",
      "Epoch [1/1], Step [1468/3504], Loss: 0.4510\n",
      "Epoch [1/1], Step [1469/3504], Loss: 0.3956\n",
      "Epoch [1/1], Step [1470/3504], Loss: 0.4073\n",
      "Epoch [1/1], Step [1471/3504], Loss: 0.3827\n",
      "Epoch [1/1], Step [1472/3504], Loss: 0.3674\n",
      "Epoch [1/1], Step [1473/3504], Loss: 0.3222\n",
      "Epoch [1/1], Step [1474/3504], Loss: 0.3430\n",
      "Epoch [1/1], Step [1475/3504], Loss: 0.3898\n",
      "Epoch [1/1], Step [1476/3504], Loss: 0.3498\n",
      "Epoch [1/1], Step [1477/3504], Loss: 0.3411\n",
      "Epoch [1/1], Step [1478/3504], Loss: 0.3537\n",
      "Epoch [1/1], Step [1479/3504], Loss: 0.2772\n",
      "Epoch [1/1], Step [1480/3504], Loss: 0.2991\n",
      "Epoch [1/1], Step [1481/3504], Loss: 0.3523\n",
      "Epoch [1/1], Step [1482/3504], Loss: 0.3622\n",
      "Epoch [1/1], Step [1483/3504], Loss: 0.3347\n",
      "Epoch [1/1], Step [1484/3504], Loss: 0.2685\n",
      "Epoch [1/1], Step [1485/3504], Loss: 0.4451\n",
      "Epoch [1/1], Step [1486/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [1487/3504], Loss: 0.3988\n",
      "Epoch [1/1], Step [1488/3504], Loss: 0.4150\n",
      "Epoch [1/1], Step [1489/3504], Loss: 0.2888\n",
      "Epoch [1/1], Step [1490/3504], Loss: 0.2825\n",
      "Epoch [1/1], Step [1491/3504], Loss: 0.3638\n",
      "Epoch [1/1], Step [1492/3504], Loss: 0.2813\n",
      "Epoch [1/1], Step [1493/3504], Loss: 0.3751\n",
      "Epoch [1/1], Step [1494/3504], Loss: 0.3149\n",
      "Epoch [1/1], Step [1495/3504], Loss: 0.3821\n",
      "Epoch [1/1], Step [1496/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [1497/3504], Loss: 0.4279\n",
      "Epoch [1/1], Step [1498/3504], Loss: 0.3255\n",
      "Epoch [1/1], Step [1499/3504], Loss: 0.3433\n",
      "Epoch [1/1], Step [1500/3504], Loss: 0.2877\n",
      "Epoch [1/1], Step [1501/3504], Loss: 0.3421\n",
      "Epoch [1/1], Step [1502/3504], Loss: 0.2965\n",
      "Epoch [1/1], Step [1503/3504], Loss: 0.3706\n",
      "Epoch [1/1], Step [1504/3504], Loss: 0.3143\n",
      "Epoch [1/1], Step [1505/3504], Loss: 0.3664\n",
      "Epoch [1/1], Step [1506/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [1507/3504], Loss: 0.3665\n",
      "Epoch [1/1], Step [1508/3504], Loss: 0.4057\n",
      "Epoch [1/1], Step [1509/3504], Loss: 0.3065\n",
      "Epoch [1/1], Step [1510/3504], Loss: 0.3559\n",
      "Epoch [1/1], Step [1511/3504], Loss: 0.3849\n",
      "Epoch [1/1], Step [1512/3504], Loss: 0.2800\n",
      "Epoch [1/1], Step [1513/3504], Loss: 0.3294\n",
      "Epoch [1/1], Step [1514/3504], Loss: 0.2946\n",
      "Epoch [1/1], Step [1515/3504], Loss: 0.2625\n",
      "Epoch [1/1], Step [1516/3504], Loss: 0.3290\n",
      "Epoch [1/1], Step [1517/3504], Loss: 0.2670\n",
      "Epoch [1/1], Step [1518/3504], Loss: 0.3171\n",
      "Epoch [1/1], Step [1519/3504], Loss: 0.3479\n",
      "Epoch [1/1], Step [1520/3504], Loss: 0.3388\n",
      "Epoch [1/1], Step [1521/3504], Loss: 0.3343\n",
      "Epoch [1/1], Step [1522/3504], Loss: 0.2998\n",
      "Epoch [1/1], Step [1523/3504], Loss: 0.4111\n",
      "Epoch [1/1], Step [1524/3504], Loss: 0.3970\n",
      "Epoch [1/1], Step [1525/3504], Loss: 0.3039\n",
      "Epoch [1/1], Step [1526/3504], Loss: 0.2837\n",
      "Epoch [1/1], Step [1527/3504], Loss: 0.3902\n",
      "Epoch [1/1], Step [1528/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [1529/3504], Loss: 0.3467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1530/3504], Loss: 0.3340\n",
      "Epoch [1/1], Step [1531/3504], Loss: 0.3634\n",
      "Epoch [1/1], Step [1532/3504], Loss: 0.2974\n",
      "Epoch [1/1], Step [1533/3504], Loss: 0.3423\n",
      "Epoch [1/1], Step [1534/3504], Loss: 0.3142\n",
      "Epoch [1/1], Step [1535/3504], Loss: 0.3278\n",
      "Epoch [1/1], Step [1536/3504], Loss: 0.3853\n",
      "Epoch [1/1], Step [1537/3504], Loss: 0.3411\n",
      "Epoch [1/1], Step [1538/3504], Loss: 0.3063\n",
      "Epoch [1/1], Step [1539/3504], Loss: 0.3197\n",
      "Epoch [1/1], Step [1540/3504], Loss: 0.3084\n",
      "Epoch [1/1], Step [1541/3504], Loss: 0.3007\n",
      "Epoch [1/1], Step [1542/3504], Loss: 0.4451\n",
      "Epoch [1/1], Step [1543/3504], Loss: 0.3562\n",
      "Epoch [1/1], Step [1544/3504], Loss: 0.3960\n",
      "Epoch [1/1], Step [1545/3504], Loss: 0.3087\n",
      "Epoch [1/1], Step [1546/3504], Loss: 0.3105\n",
      "Epoch [1/1], Step [1547/3504], Loss: 0.3947\n",
      "Epoch [1/1], Step [1548/3504], Loss: 0.3747\n",
      "Epoch [1/1], Step [1549/3504], Loss: 0.4472\n",
      "Epoch [1/1], Step [1550/3504], Loss: 0.4243\n",
      "Epoch [1/1], Step [1551/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [1552/3504], Loss: 0.3031\n",
      "Epoch [1/1], Step [1553/3504], Loss: 0.3961\n",
      "Epoch [1/1], Step [1554/3504], Loss: 0.2600\n",
      "Epoch [1/1], Step [1555/3504], Loss: 0.4597\n",
      "Epoch [1/1], Step [1556/3504], Loss: 0.2976\n",
      "Epoch [1/1], Step [1557/3504], Loss: 0.3246\n",
      "Epoch [1/1], Step [1558/3504], Loss: 0.3544\n",
      "Epoch [1/1], Step [1559/3504], Loss: 0.3475\n",
      "Epoch [1/1], Step [1560/3504], Loss: 0.3757\n",
      "Epoch [1/1], Step [1561/3504], Loss: 0.3003\n",
      "Epoch [1/1], Step [1562/3504], Loss: 0.3083\n",
      "Epoch [1/1], Step [1563/3504], Loss: 0.3375\n",
      "Epoch [1/1], Step [1564/3504], Loss: 0.3958\n",
      "Epoch [1/1], Step [1565/3504], Loss: 0.3245\n",
      "Epoch [1/1], Step [1566/3504], Loss: 0.3712\n",
      "Epoch [1/1], Step [1567/3504], Loss: 0.3450\n",
      "Epoch [1/1], Step [1568/3504], Loss: 0.4514\n",
      "Epoch [1/1], Step [1569/3504], Loss: 0.4415\n",
      "Epoch [1/1], Step [1570/3504], Loss: 0.2801\n",
      "Epoch [1/1], Step [1571/3504], Loss: 0.2777\n",
      "Epoch [1/1], Step [1572/3504], Loss: 0.4880\n",
      "Epoch [1/1], Step [1573/3504], Loss: 0.3505\n",
      "Epoch [1/1], Step [1574/3504], Loss: 0.3580\n",
      "Epoch [1/1], Step [1575/3504], Loss: 0.3606\n",
      "Epoch [1/1], Step [1576/3504], Loss: 0.3375\n",
      "Epoch [1/1], Step [1577/3504], Loss: 0.3210\n",
      "Epoch [1/1], Step [1578/3504], Loss: 0.3086\n",
      "Epoch [1/1], Step [1579/3504], Loss: 0.3212\n",
      "Epoch [1/1], Step [1580/3504], Loss: 0.4142\n",
      "Epoch [1/1], Step [1581/3504], Loss: 0.3919\n",
      "Epoch [1/1], Step [1582/3504], Loss: 0.4560\n",
      "Epoch [1/1], Step [1583/3504], Loss: 0.3022\n",
      "Epoch [1/1], Step [1584/3504], Loss: 0.2741\n",
      "Epoch [1/1], Step [1585/3504], Loss: 0.3894\n",
      "Epoch [1/1], Step [1586/3504], Loss: 0.3088\n",
      "Epoch [1/1], Step [1587/3504], Loss: 0.2448\n",
      "Epoch [1/1], Step [1588/3504], Loss: 0.4069\n",
      "Epoch [1/1], Step [1589/3504], Loss: 0.3396\n",
      "Epoch [1/1], Step [1590/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [1591/3504], Loss: 0.4906\n",
      "Epoch [1/1], Step [1592/3504], Loss: 0.3709\n",
      "Epoch [1/1], Step [1593/3504], Loss: 0.2966\n",
      "Epoch [1/1], Step [1594/3504], Loss: 0.3082\n",
      "Epoch [1/1], Step [1595/3504], Loss: 0.4397\n",
      "Epoch [1/1], Step [1596/3504], Loss: 0.3334\n",
      "Epoch [1/1], Step [1597/3504], Loss: 0.3969\n",
      "Epoch [1/1], Step [1598/3504], Loss: 0.3969\n",
      "Epoch [1/1], Step [1599/3504], Loss: 0.3617\n",
      "Epoch [1/1], Step [1600/3504], Loss: 0.3516\n",
      "Epoch [1/1], Step [1601/3504], Loss: 0.3394\n",
      "Epoch [1/1], Step [1602/3504], Loss: 0.3348\n",
      "Epoch [1/1], Step [1603/3504], Loss: 0.3777\n",
      "Epoch [1/1], Step [1604/3504], Loss: 0.4133\n",
      "Epoch [1/1], Step [1605/3504], Loss: 0.3038\n",
      "Epoch [1/1], Step [1606/3504], Loss: 0.3348\n",
      "Epoch [1/1], Step [1607/3504], Loss: 0.3610\n",
      "Epoch [1/1], Step [1608/3504], Loss: 0.3328\n",
      "Epoch [1/1], Step [1609/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [1610/3504], Loss: 0.3083\n",
      "Epoch [1/1], Step [1611/3504], Loss: 0.3159\n",
      "Epoch [1/1], Step [1612/3504], Loss: 0.2582\n",
      "Epoch [1/1], Step [1613/3504], Loss: 0.2858\n",
      "Epoch [1/1], Step [1614/3504], Loss: 0.3814\n",
      "Epoch [1/1], Step [1615/3504], Loss: 0.4183\n",
      "Epoch [1/1], Step [1616/3504], Loss: 0.2691\n",
      "Epoch [1/1], Step [1617/3504], Loss: 0.4015\n",
      "Epoch [1/1], Step [1618/3504], Loss: 0.3276\n",
      "Epoch [1/1], Step [1619/3504], Loss: 0.3381\n",
      "Epoch [1/1], Step [1620/3504], Loss: 0.3169\n",
      "Epoch [1/1], Step [1621/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [1622/3504], Loss: 0.4068\n",
      "Epoch [1/1], Step [1623/3504], Loss: 0.4295\n",
      "Epoch [1/1], Step [1624/3504], Loss: 0.3613\n",
      "Epoch [1/1], Step [1625/3504], Loss: 0.3949\n",
      "Epoch [1/1], Step [1626/3504], Loss: 0.3888\n",
      "Epoch [1/1], Step [1627/3504], Loss: 0.3896\n",
      "Epoch [1/1], Step [1628/3504], Loss: 0.3208\n",
      "Epoch [1/1], Step [1629/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [1630/3504], Loss: 0.3964\n",
      "Epoch [1/1], Step [1631/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [1632/3504], Loss: 0.3742\n",
      "Epoch [1/1], Step [1633/3504], Loss: 0.3801\n",
      "Epoch [1/1], Step [1634/3504], Loss: 0.2981\n",
      "Epoch [1/1], Step [1635/3504], Loss: 0.3941\n",
      "Epoch [1/1], Step [1636/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [1637/3504], Loss: 0.2918\n",
      "Epoch [1/1], Step [1638/3504], Loss: 0.3688\n",
      "Epoch [1/1], Step [1639/3504], Loss: 0.3176\n",
      "Epoch [1/1], Step [1640/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [1641/3504], Loss: 0.4100\n",
      "Epoch [1/1], Step [1642/3504], Loss: 0.3001\n",
      "Epoch [1/1], Step [1643/3504], Loss: 0.3330\n",
      "Epoch [1/1], Step [1644/3504], Loss: 0.2985\n",
      "Epoch [1/1], Step [1645/3504], Loss: 0.3735\n",
      "Epoch [1/1], Step [1646/3504], Loss: 0.4798\n",
      "Epoch [1/1], Step [1647/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [1648/3504], Loss: 0.2980\n",
      "Epoch [1/1], Step [1649/3504], Loss: 0.3075\n",
      "Epoch [1/1], Step [1650/3504], Loss: 0.3595\n",
      "Epoch [1/1], Step [1651/3504], Loss: 0.4051\n",
      "Epoch [1/1], Step [1652/3504], Loss: 0.4070\n",
      "Epoch [1/1], Step [1653/3504], Loss: 0.4516\n",
      "Epoch [1/1], Step [1654/3504], Loss: 0.3389\n",
      "Epoch [1/1], Step [1655/3504], Loss: 0.3947\n",
      "Epoch [1/1], Step [1656/3504], Loss: 0.3422\n",
      "Epoch [1/1], Step [1657/3504], Loss: 0.3982\n",
      "Epoch [1/1], Step [1658/3504], Loss: 0.3714\n",
      "Epoch [1/1], Step [1659/3504], Loss: 0.3403\n",
      "Epoch [1/1], Step [1660/3504], Loss: 0.3347\n",
      "Epoch [1/1], Step [1661/3504], Loss: 0.3108\n",
      "Epoch [1/1], Step [1662/3504], Loss: 0.3849\n",
      "Epoch [1/1], Step [1663/3504], Loss: 0.3455\n",
      "Epoch [1/1], Step [1664/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [1665/3504], Loss: 0.3137\n",
      "Epoch [1/1], Step [1666/3504], Loss: 0.4445\n",
      "Epoch [1/1], Step [1667/3504], Loss: 0.3411\n",
      "Epoch [1/1], Step [1668/3504], Loss: 0.3919\n",
      "Epoch [1/1], Step [1669/3504], Loss: 0.3802\n",
      "Epoch [1/1], Step [1670/3504], Loss: 0.3153\n",
      "Epoch [1/1], Step [1671/3504], Loss: 0.4034\n",
      "Epoch [1/1], Step [1672/3504], Loss: 0.2897\n",
      "Epoch [1/1], Step [1673/3504], Loss: 0.3575\n",
      "Epoch [1/1], Step [1674/3504], Loss: 0.3264\n",
      "Epoch [1/1], Step [1675/3504], Loss: 0.4052\n",
      "Epoch [1/1], Step [1676/3504], Loss: 0.3869\n",
      "Epoch [1/1], Step [1677/3504], Loss: 0.3520\n",
      "Epoch [1/1], Step [1678/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [1679/3504], Loss: 0.4008\n",
      "Epoch [1/1], Step [1680/3504], Loss: 0.2960\n",
      "Epoch [1/1], Step [1681/3504], Loss: 0.3183\n",
      "Epoch [1/1], Step [1682/3504], Loss: 0.4597\n",
      "Epoch [1/1], Step [1683/3504], Loss: 0.3634\n",
      "Epoch [1/1], Step [1684/3504], Loss: 0.3322\n",
      "Epoch [1/1], Step [1685/3504], Loss: 0.4148\n",
      "Epoch [1/1], Step [1686/3504], Loss: 0.3470\n",
      "Epoch [1/1], Step [1687/3504], Loss: 0.3313\n",
      "Epoch [1/1], Step [1688/3504], Loss: 0.3173\n",
      "Epoch [1/1], Step [1689/3504], Loss: 0.4470\n",
      "Epoch [1/1], Step [1690/3504], Loss: 0.3378\n",
      "Epoch [1/1], Step [1691/3504], Loss: 0.2522\n",
      "Epoch [1/1], Step [1692/3504], Loss: 0.3665\n",
      "Epoch [1/1], Step [1693/3504], Loss: 0.2885\n",
      "Epoch [1/1], Step [1694/3504], Loss: 0.4072\n",
      "Epoch [1/1], Step [1695/3504], Loss: 0.4031\n",
      "Epoch [1/1], Step [1696/3504], Loss: 0.4313\n",
      "Epoch [1/1], Step [1697/3504], Loss: 0.2773\n",
      "Epoch [1/1], Step [1698/3504], Loss: 0.3152\n",
      "Epoch [1/1], Step [1699/3504], Loss: 0.3206\n",
      "Epoch [1/1], Step [1700/3504], Loss: 0.3377\n",
      "Epoch [1/1], Step [1701/3504], Loss: 0.3250\n",
      "Epoch [1/1], Step [1702/3504], Loss: 0.3859\n",
      "Epoch [1/1], Step [1703/3504], Loss: 0.2825\n",
      "Epoch [1/1], Step [1704/3504], Loss: 0.2468\n",
      "Epoch [1/1], Step [1705/3504], Loss: 0.2998\n",
      "Epoch [1/1], Step [1706/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [1707/3504], Loss: 0.4015\n",
      "Epoch [1/1], Step [1708/3504], Loss: 0.3657\n",
      "Epoch [1/1], Step [1709/3504], Loss: 0.2801\n",
      "Epoch [1/1], Step [1710/3504], Loss: 0.3648\n",
      "Epoch [1/1], Step [1711/3504], Loss: 0.3167\n",
      "Epoch [1/1], Step [1712/3504], Loss: 0.4462\n",
      "Epoch [1/1], Step [1713/3504], Loss: 0.3090\n",
      "Epoch [1/1], Step [1714/3504], Loss: 0.3335\n",
      "Epoch [1/1], Step [1715/3504], Loss: 0.3123\n",
      "Epoch [1/1], Step [1716/3504], Loss: 0.4267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1717/3504], Loss: 0.2617\n",
      "Epoch [1/1], Step [1718/3504], Loss: 0.2985\n",
      "Epoch [1/1], Step [1719/3504], Loss: 0.3980\n",
      "Epoch [1/1], Step [1720/3504], Loss: 0.3543\n",
      "Epoch [1/1], Step [1721/3504], Loss: 0.3869\n",
      "Epoch [1/1], Step [1722/3504], Loss: 0.3054\n",
      "Epoch [1/1], Step [1723/3504], Loss: 0.4233\n",
      "Epoch [1/1], Step [1724/3504], Loss: 0.2834\n",
      "Epoch [1/1], Step [1725/3504], Loss: 0.3362\n",
      "Epoch [1/1], Step [1726/3504], Loss: 0.2413\n",
      "Epoch [1/1], Step [1727/3504], Loss: 0.3893\n",
      "Epoch [1/1], Step [1728/3504], Loss: 0.3290\n",
      "Epoch [1/1], Step [1729/3504], Loss: 0.3576\n",
      "Epoch [1/1], Step [1730/3504], Loss: 0.3564\n",
      "Epoch [1/1], Step [1731/3504], Loss: 0.3148\n",
      "Epoch [1/1], Step [1732/3504], Loss: 0.4016\n",
      "Epoch [1/1], Step [1733/3504], Loss: 0.2913\n",
      "Epoch [1/1], Step [1734/3504], Loss: 0.3283\n",
      "Epoch [1/1], Step [1735/3504], Loss: 0.4820\n",
      "Epoch [1/1], Step [1736/3504], Loss: 0.2979\n",
      "Epoch [1/1], Step [1737/3504], Loss: 0.3649\n",
      "Epoch [1/1], Step [1738/3504], Loss: 0.3876\n",
      "Epoch [1/1], Step [1739/3504], Loss: 0.4150\n",
      "Epoch [1/1], Step [1740/3504], Loss: 0.4453\n",
      "Epoch [1/1], Step [1741/3504], Loss: 0.2964\n",
      "Epoch [1/1], Step [1742/3504], Loss: 0.3582\n",
      "Epoch [1/1], Step [1743/3504], Loss: 0.3233\n",
      "Epoch [1/1], Step [1744/3504], Loss: 0.2496\n",
      "Epoch [1/1], Step [1745/3504], Loss: 0.2844\n",
      "Epoch [1/1], Step [1746/3504], Loss: 0.3649\n",
      "Epoch [1/1], Step [1747/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [1748/3504], Loss: 0.3623\n",
      "Epoch [1/1], Step [1749/3504], Loss: 0.4782\n",
      "Epoch [1/1], Step [1750/3504], Loss: 0.3216\n",
      "Epoch [1/1], Step [1751/3504], Loss: 0.4293\n",
      "Epoch [1/1], Step [1752/3504], Loss: 0.2639\n",
      "Epoch [1/1], Step [1753/3504], Loss: 0.2429\n",
      "Epoch [1/1], Step [1754/3504], Loss: 0.3800\n",
      "Epoch [1/1], Step [1755/3504], Loss: 0.3287\n",
      "Epoch [1/1], Step [1756/3504], Loss: 0.2914\n",
      "Epoch [1/1], Step [1757/3504], Loss: 0.3538\n",
      "Epoch [1/1], Step [1758/3504], Loss: 0.4062\n",
      "Epoch [1/1], Step [1759/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [1760/3504], Loss: 0.3523\n",
      "Epoch [1/1], Step [1761/3504], Loss: 0.4509\n",
      "Epoch [1/1], Step [1762/3504], Loss: 0.2701\n",
      "Epoch [1/1], Step [1763/3504], Loss: 0.3607\n",
      "Epoch [1/1], Step [1764/3504], Loss: 0.3270\n",
      "Epoch [1/1], Step [1765/3504], Loss: 0.3165\n",
      "Epoch [1/1], Step [1766/3504], Loss: 0.4425\n",
      "Epoch [1/1], Step [1767/3504], Loss: 0.3691\n",
      "Epoch [1/1], Step [1768/3504], Loss: 0.3538\n",
      "Epoch [1/1], Step [1769/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [1770/3504], Loss: 0.3447\n",
      "Epoch [1/1], Step [1771/3504], Loss: 0.3459\n",
      "Epoch [1/1], Step [1772/3504], Loss: 0.2902\n",
      "Epoch [1/1], Step [1773/3504], Loss: 0.4025\n",
      "Epoch [1/1], Step [1774/3504], Loss: 0.3053\n",
      "Epoch [1/1], Step [1775/3504], Loss: 0.2803\n",
      "Epoch [1/1], Step [1776/3504], Loss: 0.3103\n",
      "Epoch [1/1], Step [1777/3504], Loss: 0.3827\n",
      "Epoch [1/1], Step [1778/3504], Loss: 0.3951\n",
      "Epoch [1/1], Step [1779/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [1780/3504], Loss: 0.2917\n",
      "Epoch [1/1], Step [1781/3504], Loss: 0.3632\n",
      "Epoch [1/1], Step [1782/3504], Loss: 0.4513\n",
      "Epoch [1/1], Step [1783/3504], Loss: 0.2904\n",
      "Epoch [1/1], Step [1784/3504], Loss: 0.3979\n",
      "Epoch [1/1], Step [1785/3504], Loss: 0.3176\n",
      "Epoch [1/1], Step [1786/3504], Loss: 0.4506\n",
      "Epoch [1/1], Step [1787/3504], Loss: 0.3983\n",
      "Epoch [1/1], Step [1788/3504], Loss: 0.3089\n",
      "Epoch [1/1], Step [1789/3504], Loss: 0.4628\n",
      "Epoch [1/1], Step [1790/3504], Loss: 0.3634\n",
      "Epoch [1/1], Step [1791/3504], Loss: 0.3553\n",
      "Epoch [1/1], Step [1792/3504], Loss: 0.2905\n",
      "Epoch [1/1], Step [1793/3504], Loss: 0.3279\n",
      "Epoch [1/1], Step [1794/3504], Loss: 0.3256\n",
      "Epoch [1/1], Step [1795/3504], Loss: 0.3376\n",
      "Epoch [1/1], Step [1796/3504], Loss: 0.4073\n",
      "Epoch [1/1], Step [1797/3504], Loss: 0.3012\n",
      "Epoch [1/1], Step [1798/3504], Loss: 0.3757\n",
      "Epoch [1/1], Step [1799/3504], Loss: 0.3121\n",
      "Epoch [1/1], Step [1800/3504], Loss: 0.3553\n",
      "Epoch [1/1], Step [1801/3504], Loss: 0.4396\n",
      "Epoch [1/1], Step [1802/3504], Loss: 0.2929\n",
      "Epoch [1/1], Step [1803/3504], Loss: 0.2534\n",
      "Epoch [1/1], Step [1804/3504], Loss: 0.3430\n",
      "Epoch [1/1], Step [1805/3504], Loss: 0.3243\n",
      "Epoch [1/1], Step [1806/3504], Loss: 0.3849\n",
      "Epoch [1/1], Step [1807/3504], Loss: 0.4460\n",
      "Epoch [1/1], Step [1808/3504], Loss: 0.4262\n",
      "Epoch [1/1], Step [1809/3504], Loss: 0.3532\n",
      "Epoch [1/1], Step [1810/3504], Loss: 0.2593\n",
      "Epoch [1/1], Step [1811/3504], Loss: 0.4295\n",
      "Epoch [1/1], Step [1812/3504], Loss: 0.4169\n",
      "Epoch [1/1], Step [1813/3504], Loss: 0.3899\n",
      "Epoch [1/1], Step [1814/3504], Loss: 0.4855\n",
      "Epoch [1/1], Step [1815/3504], Loss: 0.3435\n",
      "Epoch [1/1], Step [1816/3504], Loss: 0.2756\n",
      "Epoch [1/1], Step [1817/3504], Loss: 0.3741\n",
      "Epoch [1/1], Step [1818/3504], Loss: 0.2253\n",
      "Epoch [1/1], Step [1819/3504], Loss: 0.3066\n",
      "Epoch [1/1], Step [1820/3504], Loss: 0.3828\n",
      "Epoch [1/1], Step [1821/3504], Loss: 0.3022\n",
      "Epoch [1/1], Step [1822/3504], Loss: 0.3517\n",
      "Epoch [1/1], Step [1823/3504], Loss: 0.3702\n",
      "Epoch [1/1], Step [1824/3504], Loss: 0.3837\n",
      "Epoch [1/1], Step [1825/3504], Loss: 0.3252\n",
      "Epoch [1/1], Step [1826/3504], Loss: 0.2242\n",
      "Epoch [1/1], Step [1827/3504], Loss: 0.3281\n",
      "Epoch [1/1], Step [1828/3504], Loss: 0.3968\n",
      "Epoch [1/1], Step [1829/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [1830/3504], Loss: 0.3717\n",
      "Epoch [1/1], Step [1831/3504], Loss: 0.3307\n",
      "Epoch [1/1], Step [1832/3504], Loss: 0.3540\n",
      "Epoch [1/1], Step [1833/3504], Loss: 0.3436\n",
      "Epoch [1/1], Step [1834/3504], Loss: 0.3264\n",
      "Epoch [1/1], Step [1835/3504], Loss: 0.3511\n",
      "Epoch [1/1], Step [1836/3504], Loss: 0.4054\n",
      "Epoch [1/1], Step [1837/3504], Loss: 0.3740\n",
      "Epoch [1/1], Step [1838/3504], Loss: 0.3927\n",
      "Epoch [1/1], Step [1839/3504], Loss: 0.3344\n",
      "Epoch [1/1], Step [1840/3504], Loss: 0.3378\n",
      "Epoch [1/1], Step [1841/3504], Loss: 0.3154\n",
      "Epoch [1/1], Step [1842/3504], Loss: 0.3433\n",
      "Epoch [1/1], Step [1843/3504], Loss: 0.3074\n",
      "Epoch [1/1], Step [1844/3504], Loss: 0.2692\n",
      "Epoch [1/1], Step [1845/3504], Loss: 0.3930\n",
      "Epoch [1/1], Step [1846/3504], Loss: 0.2521\n",
      "Epoch [1/1], Step [1847/3504], Loss: 0.4131\n",
      "Epoch [1/1], Step [1848/3504], Loss: 0.3999\n",
      "Epoch [1/1], Step [1849/3504], Loss: 0.3942\n",
      "Epoch [1/1], Step [1850/3504], Loss: 0.3070\n",
      "Epoch [1/1], Step [1851/3504], Loss: 0.3725\n",
      "Epoch [1/1], Step [1852/3504], Loss: 0.3335\n",
      "Epoch [1/1], Step [1853/3504], Loss: 0.3697\n",
      "Epoch [1/1], Step [1854/3504], Loss: 0.3330\n",
      "Epoch [1/1], Step [1855/3504], Loss: 0.2927\n",
      "Epoch [1/1], Step [1856/3504], Loss: 0.3158\n",
      "Epoch [1/1], Step [1857/3504], Loss: 0.3386\n",
      "Epoch [1/1], Step [1858/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [1859/3504], Loss: 0.3227\n",
      "Epoch [1/1], Step [1860/3504], Loss: 0.3620\n",
      "Epoch [1/1], Step [1861/3504], Loss: 0.4685\n",
      "Epoch [1/1], Step [1862/3504], Loss: 0.4100\n",
      "Epoch [1/1], Step [1863/3504], Loss: 0.3827\n",
      "Epoch [1/1], Step [1864/3504], Loss: 0.3271\n",
      "Epoch [1/1], Step [1865/3504], Loss: 0.3924\n",
      "Epoch [1/1], Step [1866/3504], Loss: 0.3315\n",
      "Epoch [1/1], Step [1867/3504], Loss: 0.4122\n",
      "Epoch [1/1], Step [1868/3504], Loss: 0.3148\n",
      "Epoch [1/1], Step [1869/3504], Loss: 0.3985\n",
      "Epoch [1/1], Step [1870/3504], Loss: 0.2696\n",
      "Epoch [1/1], Step [1871/3504], Loss: 0.3612\n",
      "Epoch [1/1], Step [1872/3504], Loss: 0.2790\n",
      "Epoch [1/1], Step [1873/3504], Loss: 0.3773\n",
      "Epoch [1/1], Step [1874/3504], Loss: 0.3915\n",
      "Epoch [1/1], Step [1875/3504], Loss: 0.2677\n",
      "Epoch [1/1], Step [1876/3504], Loss: 0.4662\n",
      "Epoch [1/1], Step [1877/3504], Loss: 0.4068\n",
      "Epoch [1/1], Step [1878/3504], Loss: 0.3883\n",
      "Epoch [1/1], Step [1879/3504], Loss: 0.3932\n",
      "Epoch [1/1], Step [1880/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [1881/3504], Loss: 0.3606\n",
      "Epoch [1/1], Step [1882/3504], Loss: 0.3441\n",
      "Epoch [1/1], Step [1883/3504], Loss: 0.3358\n",
      "Epoch [1/1], Step [1884/3504], Loss: 0.3011\n",
      "Epoch [1/1], Step [1885/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [1886/3504], Loss: 0.3339\n",
      "Epoch [1/1], Step [1887/3504], Loss: 0.3797\n",
      "Epoch [1/1], Step [1888/3504], Loss: 0.3949\n",
      "Epoch [1/1], Step [1889/3504], Loss: 0.2947\n",
      "Epoch [1/1], Step [1890/3504], Loss: 0.3343\n",
      "Epoch [1/1], Step [1891/3504], Loss: 0.3115\n",
      "Epoch [1/1], Step [1892/3504], Loss: 0.3365\n",
      "Epoch [1/1], Step [1893/3504], Loss: 0.2635\n",
      "Epoch [1/1], Step [1894/3504], Loss: 0.3575\n",
      "Epoch [1/1], Step [1895/3504], Loss: 0.3264\n",
      "Epoch [1/1], Step [1896/3504], Loss: 0.3464\n",
      "Epoch [1/1], Step [1897/3504], Loss: 0.3716\n",
      "Epoch [1/1], Step [1898/3504], Loss: 0.2971\n",
      "Epoch [1/1], Step [1899/3504], Loss: 0.3710\n",
      "Epoch [1/1], Step [1900/3504], Loss: 0.3109\n",
      "Epoch [1/1], Step [1901/3504], Loss: 0.3065\n",
      "Epoch [1/1], Step [1902/3504], Loss: 0.3085\n",
      "Epoch [1/1], Step [1903/3504], Loss: 0.3462\n",
      "Epoch [1/1], Step [1904/3504], Loss: 0.3405\n",
      "Epoch [1/1], Step [1905/3504], Loss: 0.2886\n",
      "Epoch [1/1], Step [1906/3504], Loss: 0.4586\n",
      "Epoch [1/1], Step [1907/3504], Loss: 0.3053\n",
      "Epoch [1/1], Step [1908/3504], Loss: 0.2836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1909/3504], Loss: 0.3027\n",
      "Epoch [1/1], Step [1910/3504], Loss: 0.4696\n",
      "Epoch [1/1], Step [1911/3504], Loss: 0.4532\n",
      "Epoch [1/1], Step [1912/3504], Loss: 0.3574\n",
      "Epoch [1/1], Step [1913/3504], Loss: 0.3790\n",
      "Epoch [1/1], Step [1914/3504], Loss: 0.2729\n",
      "Epoch [1/1], Step [1915/3504], Loss: 0.3155\n",
      "Epoch [1/1], Step [1916/3504], Loss: 0.4327\n",
      "Epoch [1/1], Step [1917/3504], Loss: 0.3114\n",
      "Epoch [1/1], Step [1918/3504], Loss: 0.3179\n",
      "Epoch [1/1], Step [1919/3504], Loss: 0.3703\n",
      "Epoch [1/1], Step [1920/3504], Loss: 0.3630\n",
      "Epoch [1/1], Step [1921/3504], Loss: 0.3962\n",
      "Epoch [1/1], Step [1922/3504], Loss: 0.3373\n",
      "Epoch [1/1], Step [1923/3504], Loss: 0.3238\n",
      "Epoch [1/1], Step [1924/3504], Loss: 0.2872\n",
      "Epoch [1/1], Step [1925/3504], Loss: 0.4163\n",
      "Epoch [1/1], Step [1926/3504], Loss: 0.2784\n",
      "Epoch [1/1], Step [1927/3504], Loss: 0.3316\n",
      "Epoch [1/1], Step [1928/3504], Loss: 0.4062\n",
      "Epoch [1/1], Step [1929/3504], Loss: 0.3079\n",
      "Epoch [1/1], Step [1930/3504], Loss: 0.4478\n",
      "Epoch [1/1], Step [1931/3504], Loss: 0.3876\n",
      "Epoch [1/1], Step [1932/3504], Loss: 0.3149\n",
      "Epoch [1/1], Step [1933/3504], Loss: 0.3966\n",
      "Epoch [1/1], Step [1934/3504], Loss: 0.3056\n",
      "Epoch [1/1], Step [1935/3504], Loss: 0.3189\n",
      "Epoch [1/1], Step [1936/3504], Loss: 0.3621\n",
      "Epoch [1/1], Step [1937/3504], Loss: 0.3542\n",
      "Epoch [1/1], Step [1938/3504], Loss: 0.3214\n",
      "Epoch [1/1], Step [1939/3504], Loss: 0.3445\n",
      "Epoch [1/1], Step [1940/3504], Loss: 0.3062\n",
      "Epoch [1/1], Step [1941/3504], Loss: 0.3748\n",
      "Epoch [1/1], Step [1942/3504], Loss: 0.4056\n",
      "Epoch [1/1], Step [1943/3504], Loss: 0.2903\n",
      "Epoch [1/1], Step [1944/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [1945/3504], Loss: 0.3036\n",
      "Epoch [1/1], Step [1946/3504], Loss: 0.3650\n",
      "Epoch [1/1], Step [1947/3504], Loss: 0.2996\n",
      "Epoch [1/1], Step [1948/3504], Loss: 0.2983\n",
      "Epoch [1/1], Step [1949/3504], Loss: 0.2710\n",
      "Epoch [1/1], Step [1950/3504], Loss: 0.3883\n",
      "Epoch [1/1], Step [1951/3504], Loss: 0.4932\n",
      "Epoch [1/1], Step [1952/3504], Loss: 0.2867\n",
      "Epoch [1/1], Step [1953/3504], Loss: 0.3124\n",
      "Epoch [1/1], Step [1954/3504], Loss: 0.3148\n",
      "Epoch [1/1], Step [1955/3504], Loss: 0.3100\n",
      "Epoch [1/1], Step [1956/3504], Loss: 0.4193\n",
      "Epoch [1/1], Step [1957/3504], Loss: 0.3098\n",
      "Epoch [1/1], Step [1958/3504], Loss: 0.3828\n",
      "Epoch [1/1], Step [1959/3504], Loss: 0.3548\n",
      "Epoch [1/1], Step [1960/3504], Loss: 0.3178\n",
      "Epoch [1/1], Step [1961/3504], Loss: 0.2452\n",
      "Epoch [1/1], Step [1962/3504], Loss: 0.4186\n",
      "Epoch [1/1], Step [1963/3504], Loss: 0.2962\n",
      "Epoch [1/1], Step [1964/3504], Loss: 0.3618\n",
      "Epoch [1/1], Step [1965/3504], Loss: 0.2399\n",
      "Epoch [1/1], Step [1966/3504], Loss: 0.3355\n",
      "Epoch [1/1], Step [1967/3504], Loss: 0.3642\n",
      "Epoch [1/1], Step [1968/3504], Loss: 0.3508\n",
      "Epoch [1/1], Step [1969/3504], Loss: 0.3187\n",
      "Epoch [1/1], Step [1970/3504], Loss: 0.3604\n",
      "Epoch [1/1], Step [1971/3504], Loss: 0.3838\n",
      "Epoch [1/1], Step [1972/3504], Loss: 0.3057\n",
      "Epoch [1/1], Step [1973/3504], Loss: 0.2865\n",
      "Epoch [1/1], Step [1974/3504], Loss: 0.2893\n",
      "Epoch [1/1], Step [1975/3504], Loss: 0.3700\n",
      "Epoch [1/1], Step [1976/3504], Loss: 0.2919\n",
      "Epoch [1/1], Step [1977/3504], Loss: 0.2914\n",
      "Epoch [1/1], Step [1978/3504], Loss: 0.3730\n",
      "Epoch [1/1], Step [1979/3504], Loss: 0.3798\n",
      "Epoch [1/1], Step [1980/3504], Loss: 0.3544\n",
      "Epoch [1/1], Step [1981/3504], Loss: 0.3707\n",
      "Epoch [1/1], Step [1982/3504], Loss: 0.2772\n",
      "Epoch [1/1], Step [1983/3504], Loss: 0.4107\n",
      "Epoch [1/1], Step [1984/3504], Loss: 0.3338\n",
      "Epoch [1/1], Step [1985/3504], Loss: 0.2931\n",
      "Epoch [1/1], Step [1986/3504], Loss: 0.2762\n",
      "Epoch [1/1], Step [1987/3504], Loss: 0.3843\n",
      "Epoch [1/1], Step [1988/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [1989/3504], Loss: 0.3662\n",
      "Epoch [1/1], Step [1990/3504], Loss: 0.3900\n",
      "Epoch [1/1], Step [1991/3504], Loss: 0.3283\n",
      "Epoch [1/1], Step [1992/3504], Loss: 0.3221\n",
      "Epoch [1/1], Step [1993/3504], Loss: 0.4026\n",
      "Epoch [1/1], Step [1994/3504], Loss: 0.3458\n",
      "Epoch [1/1], Step [1995/3504], Loss: 0.3651\n",
      "Epoch [1/1], Step [1996/3504], Loss: 0.4192\n",
      "Epoch [1/1], Step [1997/3504], Loss: 0.3252\n",
      "Epoch [1/1], Step [1998/3504], Loss: 0.3373\n",
      "Epoch [1/1], Step [1999/3504], Loss: 0.3631\n",
      "Epoch [1/1], Step [2000/3504], Loss: 0.3274\n",
      "Epoch [1/1], Step [2001/3504], Loss: 0.3304\n",
      "Epoch [1/1], Step [2002/3504], Loss: 0.3723\n",
      "Epoch [1/1], Step [2003/3504], Loss: 0.2926\n",
      "Epoch [1/1], Step [2004/3504], Loss: 0.4217\n",
      "Epoch [1/1], Step [2005/3504], Loss: 0.3393\n",
      "Epoch [1/1], Step [2006/3504], Loss: 0.3417\n",
      "Epoch [1/1], Step [2007/3504], Loss: 0.3362\n",
      "Epoch [1/1], Step [2008/3504], Loss: 0.3653\n",
      "Epoch [1/1], Step [2009/3504], Loss: 0.2620\n",
      "Epoch [1/1], Step [2010/3504], Loss: 0.3948\n",
      "Epoch [1/1], Step [2011/3504], Loss: 0.3210\n",
      "Epoch [1/1], Step [2012/3504], Loss: 0.4042\n",
      "Epoch [1/1], Step [2013/3504], Loss: 0.2949\n",
      "Epoch [1/1], Step [2014/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [2015/3504], Loss: 0.4027\n",
      "Epoch [1/1], Step [2016/3504], Loss: 0.4059\n",
      "Epoch [1/1], Step [2017/3504], Loss: 0.2947\n",
      "Epoch [1/1], Step [2018/3504], Loss: 0.3466\n",
      "Epoch [1/1], Step [2019/3504], Loss: 0.3279\n",
      "Epoch [1/1], Step [2020/3504], Loss: 0.3558\n",
      "Epoch [1/1], Step [2021/3504], Loss: 0.4536\n",
      "Epoch [1/1], Step [2022/3504], Loss: 0.3680\n",
      "Epoch [1/1], Step [2023/3504], Loss: 0.2562\n",
      "Epoch [1/1], Step [2024/3504], Loss: 0.3022\n",
      "Epoch [1/1], Step [2025/3504], Loss: 0.3705\n",
      "Epoch [1/1], Step [2026/3504], Loss: 0.3909\n",
      "Epoch [1/1], Step [2027/3504], Loss: 0.3417\n",
      "Epoch [1/1], Step [2028/3504], Loss: 0.2877\n",
      "Epoch [1/1], Step [2029/3504], Loss: 0.3173\n",
      "Epoch [1/1], Step [2030/3504], Loss: 0.3668\n",
      "Epoch [1/1], Step [2031/3504], Loss: 0.2636\n",
      "Epoch [1/1], Step [2032/3504], Loss: 0.2978\n",
      "Epoch [1/1], Step [2033/3504], Loss: 0.3579\n",
      "Epoch [1/1], Step [2034/3504], Loss: 0.3242\n",
      "Epoch [1/1], Step [2035/3504], Loss: 0.3177\n",
      "Epoch [1/1], Step [2036/3504], Loss: 0.3315\n",
      "Epoch [1/1], Step [2037/3504], Loss: 0.2916\n",
      "Epoch [1/1], Step [2038/3504], Loss: 0.3550\n",
      "Epoch [1/1], Step [2039/3504], Loss: 0.3391\n",
      "Epoch [1/1], Step [2040/3504], Loss: 0.4183\n",
      "Epoch [1/1], Step [2041/3504], Loss: 0.3564\n",
      "Epoch [1/1], Step [2042/3504], Loss: 0.2987\n",
      "Epoch [1/1], Step [2043/3504], Loss: 0.3996\n",
      "Epoch [1/1], Step [2044/3504], Loss: 0.3549\n",
      "Epoch [1/1], Step [2045/3504], Loss: 0.3445\n",
      "Epoch [1/1], Step [2046/3504], Loss: 0.3716\n",
      "Epoch [1/1], Step [2047/3504], Loss: 0.3355\n",
      "Epoch [1/1], Step [2048/3504], Loss: 0.3471\n",
      "Epoch [1/1], Step [2049/3504], Loss: 0.2881\n",
      "Epoch [1/1], Step [2050/3504], Loss: 0.3517\n",
      "Epoch [1/1], Step [2051/3504], Loss: 0.2880\n",
      "Epoch [1/1], Step [2052/3504], Loss: 0.4330\n",
      "Epoch [1/1], Step [2053/3504], Loss: 0.4397\n",
      "Epoch [1/1], Step [2054/3504], Loss: 0.4003\n",
      "Epoch [1/1], Step [2055/3504], Loss: 0.3518\n",
      "Epoch [1/1], Step [2056/3504], Loss: 0.3144\n",
      "Epoch [1/1], Step [2057/3504], Loss: 0.3415\n",
      "Epoch [1/1], Step [2058/3504], Loss: 0.3472\n",
      "Epoch [1/1], Step [2059/3504], Loss: 0.4042\n",
      "Epoch [1/1], Step [2060/3504], Loss: 0.3711\n",
      "Epoch [1/1], Step [2061/3504], Loss: 0.3720\n",
      "Epoch [1/1], Step [2062/3504], Loss: 0.3618\n",
      "Epoch [1/1], Step [2063/3504], Loss: 0.3123\n",
      "Epoch [1/1], Step [2064/3504], Loss: 0.3033\n",
      "Epoch [1/1], Step [2065/3504], Loss: 0.4585\n",
      "Epoch [1/1], Step [2066/3504], Loss: 0.2959\n",
      "Epoch [1/1], Step [2067/3504], Loss: 0.2966\n",
      "Epoch [1/1], Step [2068/3504], Loss: 0.3066\n",
      "Epoch [1/1], Step [2069/3504], Loss: 0.3487\n",
      "Epoch [1/1], Step [2070/3504], Loss: 0.4345\n",
      "Epoch [1/1], Step [2071/3504], Loss: 0.3358\n",
      "Epoch [1/1], Step [2072/3504], Loss: 0.3616\n",
      "Epoch [1/1], Step [2073/3504], Loss: 0.3833\n",
      "Epoch [1/1], Step [2074/3504], Loss: 0.3900\n",
      "Epoch [1/1], Step [2075/3504], Loss: 0.2764\n",
      "Epoch [1/1], Step [2076/3504], Loss: 0.4445\n",
      "Epoch [1/1], Step [2077/3504], Loss: 0.4122\n",
      "Epoch [1/1], Step [2078/3504], Loss: 0.2780\n",
      "Epoch [1/1], Step [2079/3504], Loss: 0.3866\n",
      "Epoch [1/1], Step [2080/3504], Loss: 0.2381\n",
      "Epoch [1/1], Step [2081/3504], Loss: 0.3019\n",
      "Epoch [1/1], Step [2082/3504], Loss: 0.4328\n",
      "Epoch [1/1], Step [2083/3504], Loss: 0.2776\n",
      "Epoch [1/1], Step [2084/3504], Loss: 0.3237\n",
      "Epoch [1/1], Step [2085/3504], Loss: 0.3178\n",
      "Epoch [1/1], Step [2086/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [2087/3504], Loss: 0.3707\n",
      "Epoch [1/1], Step [2088/3504], Loss: 0.3170\n",
      "Epoch [1/1], Step [2089/3504], Loss: 0.4161\n",
      "Epoch [1/1], Step [2090/3504], Loss: 0.3066\n",
      "Epoch [1/1], Step [2091/3504], Loss: 0.3662\n",
      "Epoch [1/1], Step [2092/3504], Loss: 0.3055\n",
      "Epoch [1/1], Step [2093/3504], Loss: 0.4183\n",
      "Epoch [1/1], Step [2094/3504], Loss: 0.3164\n",
      "Epoch [1/1], Step [2095/3504], Loss: 0.2432\n",
      "Epoch [1/1], Step [2096/3504], Loss: 0.3728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [2097/3504], Loss: 0.3431\n",
      "Epoch [1/1], Step [2098/3504], Loss: 0.3486\n",
      "Epoch [1/1], Step [2099/3504], Loss: 0.4018\n",
      "Epoch [1/1], Step [2100/3504], Loss: 0.3414\n",
      "Epoch [1/1], Step [2101/3504], Loss: 0.4044\n",
      "Epoch [1/1], Step [2102/3504], Loss: 0.3265\n",
      "Epoch [1/1], Step [2103/3504], Loss: 0.4124\n",
      "Epoch [1/1], Step [2104/3504], Loss: 0.2776\n",
      "Epoch [1/1], Step [2105/3504], Loss: 0.2748\n",
      "Epoch [1/1], Step [2106/3504], Loss: 0.3757\n",
      "Epoch [1/1], Step [2107/3504], Loss: 0.3233\n",
      "Epoch [1/1], Step [2108/3504], Loss: 0.3837\n",
      "Epoch [1/1], Step [2109/3504], Loss: 0.2919\n",
      "Epoch [1/1], Step [2110/3504], Loss: 0.4177\n",
      "Epoch [1/1], Step [2111/3504], Loss: 0.2965\n",
      "Epoch [1/1], Step [2112/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [2113/3504], Loss: 0.4966\n",
      "Epoch [1/1], Step [2114/3504], Loss: 0.4069\n",
      "Epoch [1/1], Step [2115/3504], Loss: 0.2944\n",
      "Epoch [1/1], Step [2116/3504], Loss: 0.2988\n",
      "Epoch [1/1], Step [2117/3504], Loss: 0.2977\n",
      "Epoch [1/1], Step [2118/3504], Loss: 0.2755\n",
      "Epoch [1/1], Step [2119/3504], Loss: 0.3104\n",
      "Epoch [1/1], Step [2120/3504], Loss: 0.2947\n",
      "Epoch [1/1], Step [2121/3504], Loss: 0.3664\n",
      "Epoch [1/1], Step [2122/3504], Loss: 0.2378\n",
      "Epoch [1/1], Step [2123/3504], Loss: 0.3631\n",
      "Epoch [1/1], Step [2124/3504], Loss: 0.3319\n",
      "Epoch [1/1], Step [2125/3504], Loss: 0.2328\n",
      "Epoch [1/1], Step [2126/3504], Loss: 0.2350\n",
      "Epoch [1/1], Step [2127/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [2128/3504], Loss: 0.3461\n",
      "Epoch [1/1], Step [2129/3504], Loss: 0.3976\n",
      "Epoch [1/1], Step [2130/3504], Loss: 0.3215\n",
      "Epoch [1/1], Step [2131/3504], Loss: 0.3393\n",
      "Epoch [1/1], Step [2132/3504], Loss: 0.3785\n",
      "Epoch [1/1], Step [2133/3504], Loss: 0.4228\n",
      "Epoch [1/1], Step [2134/3504], Loss: 0.3444\n",
      "Epoch [1/1], Step [2135/3504], Loss: 0.4098\n",
      "Epoch [1/1], Step [2136/3504], Loss: 0.3870\n",
      "Epoch [1/1], Step [2137/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [2138/3504], Loss: 0.2809\n",
      "Epoch [1/1], Step [2139/3504], Loss: 0.3419\n",
      "Epoch [1/1], Step [2140/3504], Loss: 0.3806\n",
      "Epoch [1/1], Step [2141/3504], Loss: 0.4306\n",
      "Epoch [1/1], Step [2142/3504], Loss: 0.3772\n",
      "Epoch [1/1], Step [2143/3504], Loss: 0.3551\n",
      "Epoch [1/1], Step [2144/3504], Loss: 0.3338\n",
      "Epoch [1/1], Step [2145/3504], Loss: 0.3825\n",
      "Epoch [1/1], Step [2146/3504], Loss: 0.3624\n",
      "Epoch [1/1], Step [2147/3504], Loss: 0.2823\n",
      "Epoch [1/1], Step [2148/3504], Loss: 0.3819\n",
      "Epoch [1/1], Step [2149/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [2150/3504], Loss: 0.3686\n",
      "Epoch [1/1], Step [2151/3504], Loss: 0.3506\n",
      "Epoch [1/1], Step [2152/3504], Loss: 0.3837\n",
      "Epoch [1/1], Step [2153/3504], Loss: 0.4048\n",
      "Epoch [1/1], Step [2154/3504], Loss: 0.3797\n",
      "Epoch [1/1], Step [2155/3504], Loss: 0.2996\n",
      "Epoch [1/1], Step [2156/3504], Loss: 0.3713\n",
      "Epoch [1/1], Step [2157/3504], Loss: 0.3512\n",
      "Epoch [1/1], Step [2158/3504], Loss: 0.3443\n",
      "Epoch [1/1], Step [2159/3504], Loss: 0.3825\n",
      "Epoch [1/1], Step [2160/3504], Loss: 0.3570\n",
      "Epoch [1/1], Step [2161/3504], Loss: 0.3800\n",
      "Epoch [1/1], Step [2162/3504], Loss: 0.3271\n",
      "Epoch [1/1], Step [2163/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [2164/3504], Loss: 0.3191\n",
      "Epoch [1/1], Step [2165/3504], Loss: 0.3051\n",
      "Epoch [1/1], Step [2166/3504], Loss: 0.2761\n",
      "Epoch [1/1], Step [2167/3504], Loss: 0.4251\n",
      "Epoch [1/1], Step [2168/3504], Loss: 0.2743\n",
      "Epoch [1/1], Step [2169/3504], Loss: 0.3108\n",
      "Epoch [1/1], Step [2170/3504], Loss: 0.2859\n",
      "Epoch [1/1], Step [2171/3504], Loss: 0.3617\n",
      "Epoch [1/1], Step [2172/3504], Loss: 0.3488\n",
      "Epoch [1/1], Step [2173/3504], Loss: 0.3702\n",
      "Epoch [1/1], Step [2174/3504], Loss: 0.3121\n",
      "Epoch [1/1], Step [2175/3504], Loss: 0.2817\n",
      "Epoch [1/1], Step [2176/3504], Loss: 0.3768\n",
      "Epoch [1/1], Step [2177/3504], Loss: 0.2895\n",
      "Epoch [1/1], Step [2178/3504], Loss: 0.4837\n",
      "Epoch [1/1], Step [2179/3504], Loss: 0.4697\n",
      "Epoch [1/1], Step [2180/3504], Loss: 0.3870\n",
      "Epoch [1/1], Step [2181/3504], Loss: 0.3915\n",
      "Epoch [1/1], Step [2182/3504], Loss: 0.3993\n",
      "Epoch [1/1], Step [2183/3504], Loss: 0.3574\n",
      "Epoch [1/1], Step [2184/3504], Loss: 0.3045\n",
      "Epoch [1/1], Step [2185/3504], Loss: 0.3052\n",
      "Epoch [1/1], Step [2186/3504], Loss: 0.2837\n",
      "Epoch [1/1], Step [2187/3504], Loss: 0.2572\n",
      "Epoch [1/1], Step [2188/3504], Loss: 0.3735\n",
      "Epoch [1/1], Step [2189/3504], Loss: 0.3776\n",
      "Epoch [1/1], Step [2190/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [2191/3504], Loss: 0.3026\n",
      "Epoch [1/1], Step [2192/3504], Loss: 0.2944\n",
      "Epoch [1/1], Step [2193/3504], Loss: 0.3530\n",
      "Epoch [1/1], Step [2194/3504], Loss: 0.3138\n",
      "Epoch [1/1], Step [2195/3504], Loss: 0.3953\n",
      "Epoch [1/1], Step [2196/3504], Loss: 0.3687\n",
      "Epoch [1/1], Step [2197/3504], Loss: 0.3325\n",
      "Epoch [1/1], Step [2198/3504], Loss: 0.3684\n",
      "Epoch [1/1], Step [2199/3504], Loss: 0.3604\n",
      "Epoch [1/1], Step [2200/3504], Loss: 0.3497\n",
      "Epoch [1/1], Step [2201/3504], Loss: 0.4276\n",
      "Epoch [1/1], Step [2202/3504], Loss: 0.3538\n",
      "Epoch [1/1], Step [2203/3504], Loss: 0.2441\n",
      "Epoch [1/1], Step [2204/3504], Loss: 0.4392\n",
      "Epoch [1/1], Step [2205/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [2206/3504], Loss: 0.3444\n",
      "Epoch [1/1], Step [2207/3504], Loss: 0.3590\n",
      "Epoch [1/1], Step [2208/3504], Loss: 0.4184\n",
      "Epoch [1/1], Step [2209/3504], Loss: 0.3722\n",
      "Epoch [1/1], Step [2210/3504], Loss: 0.2629\n",
      "Epoch [1/1], Step [2211/3504], Loss: 0.3708\n",
      "Epoch [1/1], Step [2212/3504], Loss: 0.2900\n",
      "Epoch [1/1], Step [2213/3504], Loss: 0.3313\n",
      "Epoch [1/1], Step [2214/3504], Loss: 0.2966\n",
      "Epoch [1/1], Step [2215/3504], Loss: 0.3722\n",
      "Epoch [1/1], Step [2216/3504], Loss: 0.3027\n",
      "Epoch [1/1], Step [2217/3504], Loss: 0.3238\n",
      "Epoch [1/1], Step [2218/3504], Loss: 0.4065\n",
      "Epoch [1/1], Step [2219/3504], Loss: 0.3859\n",
      "Epoch [1/1], Step [2220/3504], Loss: 0.3066\n",
      "Epoch [1/1], Step [2221/3504], Loss: 0.3792\n",
      "Epoch [1/1], Step [2222/3504], Loss: 0.3291\n",
      "Epoch [1/1], Step [2223/3504], Loss: 0.2790\n",
      "Epoch [1/1], Step [2224/3504], Loss: 0.4553\n",
      "Epoch [1/1], Step [2225/3504], Loss: 0.3206\n",
      "Epoch [1/1], Step [2226/3504], Loss: 0.3438\n",
      "Epoch [1/1], Step [2227/3504], Loss: 0.3966\n",
      "Epoch [1/1], Step [2228/3504], Loss: 0.3082\n",
      "Epoch [1/1], Step [2229/3504], Loss: 0.2919\n",
      "Epoch [1/1], Step [2230/3504], Loss: 0.3884\n",
      "Epoch [1/1], Step [2231/3504], Loss: 0.3684\n",
      "Epoch [1/1], Step [2232/3504], Loss: 0.3490\n",
      "Epoch [1/1], Step [2233/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [2234/3504], Loss: 0.3415\n",
      "Epoch [1/1], Step [2235/3504], Loss: 0.3011\n",
      "Epoch [1/1], Step [2236/3504], Loss: 0.3397\n",
      "Epoch [1/1], Step [2237/3504], Loss: 0.3220\n",
      "Epoch [1/1], Step [2238/3504], Loss: 0.3270\n",
      "Epoch [1/1], Step [2239/3504], Loss: 0.3526\n",
      "Epoch [1/1], Step [2240/3504], Loss: 0.2976\n",
      "Epoch [1/1], Step [2241/3504], Loss: 0.2922\n",
      "Epoch [1/1], Step [2242/3504], Loss: 0.8041\n",
      "Epoch [1/1], Step [2243/3504], Loss: 0.3724\n",
      "Epoch [1/1], Step [2244/3504], Loss: 0.3817\n",
      "Epoch [1/1], Step [2245/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [2246/3504], Loss: 0.2857\n",
      "Epoch [1/1], Step [2247/3504], Loss: 0.3320\n",
      "Epoch [1/1], Step [2248/3504], Loss: 0.3690\n",
      "Epoch [1/1], Step [2249/3504], Loss: 0.3569\n",
      "Epoch [1/1], Step [2250/3504], Loss: 0.3364\n",
      "Epoch [1/1], Step [2251/3504], Loss: 0.4886\n",
      "Epoch [1/1], Step [2252/3504], Loss: 0.2954\n",
      "Epoch [1/1], Step [2253/3504], Loss: 0.3145\n",
      "Epoch [1/1], Step [2254/3504], Loss: 0.3214\n",
      "Epoch [1/1], Step [2255/3504], Loss: 0.3424\n",
      "Epoch [1/1], Step [2256/3504], Loss: 0.3535\n",
      "Epoch [1/1], Step [2257/3504], Loss: 0.3399\n",
      "Epoch [1/1], Step [2258/3504], Loss: 0.2767\n",
      "Epoch [1/1], Step [2259/3504], Loss: 0.3644\n",
      "Epoch [1/1], Step [2260/3504], Loss: 0.4318\n",
      "Epoch [1/1], Step [2261/3504], Loss: 0.2931\n",
      "Epoch [1/1], Step [2262/3504], Loss: 0.2812\n",
      "Epoch [1/1], Step [2263/3504], Loss: 0.4449\n",
      "Epoch [1/1], Step [2264/3504], Loss: 0.4157\n",
      "Epoch [1/1], Step [2265/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [2266/3504], Loss: 0.2836\n",
      "Epoch [1/1], Step [2267/3504], Loss: 0.3280\n",
      "Epoch [1/1], Step [2268/3504], Loss: 0.3266\n",
      "Epoch [1/1], Step [2269/3504], Loss: 0.3615\n",
      "Epoch [1/1], Step [2270/3504], Loss: 0.4019\n",
      "Epoch [1/1], Step [2271/3504], Loss: 0.4419\n",
      "Epoch [1/1], Step [2272/3504], Loss: 0.3582\n",
      "Epoch [1/1], Step [2273/3504], Loss: 0.4296\n",
      "Epoch [1/1], Step [2274/3504], Loss: 0.3120\n",
      "Epoch [1/1], Step [2275/3504], Loss: 0.3276\n",
      "Epoch [1/1], Step [2276/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [2277/3504], Loss: 0.3173\n",
      "Epoch [1/1], Step [2278/3504], Loss: 0.3681\n",
      "Epoch [1/1], Step [2279/3504], Loss: 0.3862\n",
      "Epoch [1/1], Step [2280/3504], Loss: 0.3848\n",
      "Epoch [1/1], Step [2281/3504], Loss: 0.3028\n",
      "Epoch [1/1], Step [2282/3504], Loss: 0.3253\n",
      "Epoch [1/1], Step [2283/3504], Loss: 0.3519\n",
      "Epoch [1/1], Step [2284/3504], Loss: 0.3679\n",
      "Epoch [1/1], Step [2285/3504], Loss: 0.3421\n",
      "Epoch [1/1], Step [2286/3504], Loss: 0.2589\n",
      "Epoch [1/1], Step [2287/3504], Loss: 0.3337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [2288/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [2289/3504], Loss: 0.2655\n",
      "Epoch [1/1], Step [2290/3504], Loss: 0.3242\n",
      "Epoch [1/1], Step [2291/3504], Loss: 0.3177\n",
      "Epoch [1/1], Step [2292/3504], Loss: 0.3188\n",
      "Epoch [1/1], Step [2293/3504], Loss: 0.3872\n",
      "Epoch [1/1], Step [2294/3504], Loss: 0.3695\n",
      "Epoch [1/1], Step [2295/3504], Loss: 0.2489\n",
      "Epoch [1/1], Step [2296/3504], Loss: 0.3979\n",
      "Epoch [1/1], Step [2297/3504], Loss: 0.3259\n",
      "Epoch [1/1], Step [2298/3504], Loss: 0.3986\n",
      "Epoch [1/1], Step [2299/3504], Loss: 0.3723\n",
      "Epoch [1/1], Step [2300/3504], Loss: 0.3592\n",
      "Epoch [1/1], Step [2301/3504], Loss: 0.3674\n",
      "Epoch [1/1], Step [2302/3504], Loss: 0.3835\n",
      "Epoch [1/1], Step [2303/3504], Loss: 0.2609\n",
      "Epoch [1/1], Step [2304/3504], Loss: 0.3931\n",
      "Epoch [1/1], Step [2305/3504], Loss: 0.3430\n",
      "Epoch [1/1], Step [2306/3504], Loss: 0.2531\n",
      "Epoch [1/1], Step [2307/3504], Loss: 0.3820\n",
      "Epoch [1/1], Step [2308/3504], Loss: 0.3710\n",
      "Epoch [1/1], Step [2309/3504], Loss: 0.3827\n",
      "Epoch [1/1], Step [2310/3504], Loss: 0.3551\n",
      "Epoch [1/1], Step [2311/3504], Loss: 0.2794\n",
      "Epoch [1/1], Step [2312/3504], Loss: 0.3913\n",
      "Epoch [1/1], Step [2313/3504], Loss: 0.3560\n",
      "Epoch [1/1], Step [2314/3504], Loss: 0.3318\n",
      "Epoch [1/1], Step [2315/3504], Loss: 0.2401\n",
      "Epoch [1/1], Step [2316/3504], Loss: 0.4124\n",
      "Epoch [1/1], Step [2317/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [2318/3504], Loss: 0.4035\n",
      "Epoch [1/1], Step [2319/3504], Loss: 0.3983\n",
      "Epoch [1/1], Step [2320/3504], Loss: 0.2712\n",
      "Epoch [1/1], Step [2321/3504], Loss: 0.2791\n",
      "Epoch [1/1], Step [2322/3504], Loss: 0.3373\n",
      "Epoch [1/1], Step [2323/3504], Loss: 0.3796\n",
      "Epoch [1/1], Step [2324/3504], Loss: 0.4088\n",
      "Epoch [1/1], Step [2325/3504], Loss: 0.3921\n",
      "Epoch [1/1], Step [2326/3504], Loss: 0.3010\n",
      "Epoch [1/1], Step [2327/3504], Loss: 0.3576\n",
      "Epoch [1/1], Step [2328/3504], Loss: 0.4469\n",
      "Epoch [1/1], Step [2329/3504], Loss: 0.3942\n",
      "Epoch [1/1], Step [2330/3504], Loss: 0.2917\n",
      "Epoch [1/1], Step [2331/3504], Loss: 0.3497\n",
      "Epoch [1/1], Step [2332/3504], Loss: 0.4191\n",
      "Epoch [1/1], Step [2333/3504], Loss: 0.3556\n",
      "Epoch [1/1], Step [2334/3504], Loss: 0.3323\n",
      "Epoch [1/1], Step [2335/3504], Loss: 0.2632\n",
      "Epoch [1/1], Step [2336/3504], Loss: 0.3284\n",
      "Epoch [1/1], Step [2337/3504], Loss: 0.3462\n",
      "Epoch [1/1], Step [2338/3504], Loss: 0.2938\n",
      "Epoch [1/1], Step [2339/3504], Loss: 0.2811\n",
      "Epoch [1/1], Step [2340/3504], Loss: 0.3446\n",
      "Epoch [1/1], Step [2341/3504], Loss: 0.4879\n",
      "Epoch [1/1], Step [2342/3504], Loss: 0.3609\n",
      "Epoch [1/1], Step [2343/3504], Loss: 0.3268\n",
      "Epoch [1/1], Step [2344/3504], Loss: 0.3037\n",
      "Epoch [1/1], Step [2345/3504], Loss: 0.4663\n",
      "Epoch [1/1], Step [2346/3504], Loss: 0.2949\n",
      "Epoch [1/1], Step [2347/3504], Loss: 0.3838\n",
      "Epoch [1/1], Step [2348/3504], Loss: 0.3038\n",
      "Epoch [1/1], Step [2349/3504], Loss: 0.3844\n",
      "Epoch [1/1], Step [2350/3504], Loss: 0.3602\n",
      "Epoch [1/1], Step [2351/3504], Loss: 0.3338\n",
      "Epoch [1/1], Step [2352/3504], Loss: 0.3755\n",
      "Epoch [1/1], Step [2353/3504], Loss: 0.3036\n",
      "Epoch [1/1], Step [2354/3504], Loss: 0.2998\n",
      "Epoch [1/1], Step [2355/3504], Loss: 0.3209\n",
      "Epoch [1/1], Step [2356/3504], Loss: 0.4007\n",
      "Epoch [1/1], Step [2357/3504], Loss: 0.3339\n",
      "Epoch [1/1], Step [2358/3504], Loss: 0.3339\n",
      "Epoch [1/1], Step [2359/3504], Loss: 0.2350\n",
      "Epoch [1/1], Step [2360/3504], Loss: 0.3275\n",
      "Epoch [1/1], Step [2361/3504], Loss: 0.4250\n",
      "Epoch [1/1], Step [2362/3504], Loss: 0.3277\n",
      "Epoch [1/1], Step [2363/3504], Loss: 0.4271\n",
      "Epoch [1/1], Step [2364/3504], Loss: 0.4260\n",
      "Epoch [1/1], Step [2365/3504], Loss: 0.2900\n",
      "Epoch [1/1], Step [2366/3504], Loss: 0.3461\n",
      "Epoch [1/1], Step [2367/3504], Loss: 0.4520\n",
      "Epoch [1/1], Step [2368/3504], Loss: 0.2883\n",
      "Epoch [1/1], Step [2369/3504], Loss: 0.3616\n",
      "Epoch [1/1], Step [2370/3504], Loss: 0.3685\n",
      "Epoch [1/1], Step [2371/3504], Loss: 0.3992\n",
      "Epoch [1/1], Step [2372/3504], Loss: 0.3628\n",
      "Epoch [1/1], Step [2373/3504], Loss: 0.3902\n",
      "Epoch [1/1], Step [2374/3504], Loss: 0.2898\n",
      "Epoch [1/1], Step [2375/3504], Loss: 0.3143\n",
      "Epoch [1/1], Step [2376/3504], Loss: 0.3620\n",
      "Epoch [1/1], Step [2377/3504], Loss: 0.3400\n",
      "Epoch [1/1], Step [2378/3504], Loss: 0.3513\n",
      "Epoch [1/1], Step [2379/3504], Loss: 0.3484\n",
      "Epoch [1/1], Step [2380/3504], Loss: 0.2289\n",
      "Epoch [1/1], Step [2381/3504], Loss: 0.3855\n",
      "Epoch [1/1], Step [2382/3504], Loss: 0.3789\n",
      "Epoch [1/1], Step [2383/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [2384/3504], Loss: 0.3251\n",
      "Epoch [1/1], Step [2385/3504], Loss: 0.3529\n",
      "Epoch [1/1], Step [2386/3504], Loss: 0.2749\n",
      "Epoch [1/1], Step [2387/3504], Loss: 0.4143\n",
      "Epoch [1/1], Step [2388/3504], Loss: 0.4049\n",
      "Epoch [1/1], Step [2389/3504], Loss: 0.3277\n",
      "Epoch [1/1], Step [2390/3504], Loss: 0.3587\n",
      "Epoch [1/1], Step [2391/3504], Loss: 0.3436\n",
      "Epoch [1/1], Step [2392/3504], Loss: 0.3510\n",
      "Epoch [1/1], Step [2393/3504], Loss: 0.3703\n",
      "Epoch [1/1], Step [2394/3504], Loss: 0.3038\n",
      "Epoch [1/1], Step [2395/3504], Loss: 0.3943\n",
      "Epoch [1/1], Step [2396/3504], Loss: 0.3850\n",
      "Epoch [1/1], Step [2397/3504], Loss: 0.3804\n",
      "Epoch [1/1], Step [2398/3504], Loss: 0.3834\n",
      "Epoch [1/1], Step [2399/3504], Loss: 0.3304\n",
      "Epoch [1/1], Step [2400/3504], Loss: 0.3820\n",
      "Epoch [1/1], Step [2401/3504], Loss: 0.4269\n",
      "Epoch [1/1], Step [2402/3504], Loss: 0.3627\n",
      "Epoch [1/1], Step [2403/3504], Loss: 0.3428\n",
      "Epoch [1/1], Step [2404/3504], Loss: 0.4359\n",
      "Epoch [1/1], Step [2405/3504], Loss: 0.3212\n",
      "Epoch [1/1], Step [2406/3504], Loss: 0.2927\n",
      "Epoch [1/1], Step [2407/3504], Loss: 0.3805\n",
      "Epoch [1/1], Step [2408/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [2409/3504], Loss: 0.2980\n",
      "Epoch [1/1], Step [2410/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [2411/3504], Loss: 0.3676\n",
      "Epoch [1/1], Step [2412/3504], Loss: 0.4759\n",
      "Epoch [1/1], Step [2413/3504], Loss: 0.3555\n",
      "Epoch [1/1], Step [2414/3504], Loss: 0.4297\n",
      "Epoch [1/1], Step [2415/3504], Loss: 0.2981\n",
      "Epoch [1/1], Step [2416/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [2417/3504], Loss: 0.3436\n",
      "Epoch [1/1], Step [2418/3504], Loss: 0.2940\n",
      "Epoch [1/1], Step [2419/3504], Loss: 0.3645\n",
      "Epoch [1/1], Step [2420/3504], Loss: 0.2769\n",
      "Epoch [1/1], Step [2421/3504], Loss: 0.3770\n",
      "Epoch [1/1], Step [2422/3504], Loss: 0.2850\n",
      "Epoch [1/1], Step [2423/3504], Loss: 0.2823\n",
      "Epoch [1/1], Step [2424/3504], Loss: 0.3361\n",
      "Epoch [1/1], Step [2425/3504], Loss: 0.3671\n",
      "Epoch [1/1], Step [2426/3504], Loss: 0.3053\n",
      "Epoch [1/1], Step [2427/3504], Loss: 0.3148\n",
      "Epoch [1/1], Step [2428/3504], Loss: 0.3092\n",
      "Epoch [1/1], Step [2429/3504], Loss: 0.3089\n",
      "Epoch [1/1], Step [2430/3504], Loss: 0.3537\n",
      "Epoch [1/1], Step [2431/3504], Loss: 0.3273\n",
      "Epoch [1/1], Step [2432/3504], Loss: 0.3516\n",
      "Epoch [1/1], Step [2433/3504], Loss: 0.4083\n",
      "Epoch [1/1], Step [2434/3504], Loss: 0.3309\n",
      "Epoch [1/1], Step [2435/3504], Loss: 0.3693\n",
      "Epoch [1/1], Step [2436/3504], Loss: 0.4030\n",
      "Epoch [1/1], Step [2437/3504], Loss: 0.3598\n",
      "Epoch [1/1], Step [2438/3504], Loss: 0.3193\n",
      "Epoch [1/1], Step [2439/3504], Loss: 0.3090\n",
      "Epoch [1/1], Step [2440/3504], Loss: 0.3923\n",
      "Epoch [1/1], Step [2441/3504], Loss: 0.3344\n",
      "Epoch [1/1], Step [2442/3504], Loss: 0.4522\n",
      "Epoch [1/1], Step [2443/3504], Loss: 0.3627\n",
      "Epoch [1/1], Step [2444/3504], Loss: 0.3499\n",
      "Epoch [1/1], Step [2445/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [2446/3504], Loss: 0.3230\n",
      "Epoch [1/1], Step [2447/3504], Loss: 0.3323\n",
      "Epoch [1/1], Step [2448/3504], Loss: 0.6995\n",
      "Epoch [1/1], Step [2449/3504], Loss: 0.3306\n",
      "Epoch [1/1], Step [2450/3504], Loss: 0.2882\n",
      "Epoch [1/1], Step [2451/3504], Loss: 0.3803\n",
      "Epoch [1/1], Step [2452/3504], Loss: 0.3145\n",
      "Epoch [1/1], Step [2453/3504], Loss: 0.3248\n",
      "Epoch [1/1], Step [2454/3504], Loss: 0.3685\n",
      "Epoch [1/1], Step [2455/3504], Loss: 0.3773\n",
      "Epoch [1/1], Step [2456/3504], Loss: 0.3053\n",
      "Epoch [1/1], Step [2457/3504], Loss: 0.3331\n",
      "Epoch [1/1], Step [2458/3504], Loss: 0.2829\n",
      "Epoch [1/1], Step [2459/3504], Loss: 0.3729\n",
      "Epoch [1/1], Step [2460/3504], Loss: 0.3474\n",
      "Epoch [1/1], Step [2461/3504], Loss: 0.3909\n",
      "Epoch [1/1], Step [2462/3504], Loss: 0.3454\n",
      "Epoch [1/1], Step [2463/3504], Loss: 0.2896\n",
      "Epoch [1/1], Step [2464/3504], Loss: 0.3257\n",
      "Epoch [1/1], Step [2465/3504], Loss: 0.3243\n",
      "Epoch [1/1], Step [2466/3504], Loss: 0.3365\n",
      "Epoch [1/1], Step [2467/3504], Loss: 0.2578\n",
      "Epoch [1/1], Step [2468/3504], Loss: 0.3563\n",
      "Epoch [1/1], Step [2469/3504], Loss: 0.2947\n",
      "Epoch [1/1], Step [2470/3504], Loss: 0.3075\n",
      "Epoch [1/1], Step [2471/3504], Loss: 0.3667\n",
      "Epoch [1/1], Step [2472/3504], Loss: 0.3097\n",
      "Epoch [1/1], Step [2473/3504], Loss: 0.3333\n",
      "Epoch [1/1], Step [2474/3504], Loss: 0.3180\n",
      "Epoch [1/1], Step [2475/3504], Loss: 0.3563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [2476/3504], Loss: 0.2767\n",
      "Epoch [1/1], Step [2477/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [2478/3504], Loss: 0.3388\n",
      "Epoch [1/1], Step [2479/3504], Loss: 0.3854\n",
      "Epoch [1/1], Step [2480/3504], Loss: 0.3442\n",
      "Epoch [1/1], Step [2481/3504], Loss: 0.3822\n",
      "Epoch [1/1], Step [2482/3504], Loss: 0.4042\n",
      "Epoch [1/1], Step [2483/3504], Loss: 0.2947\n",
      "Epoch [1/1], Step [2484/3504], Loss: 0.2831\n",
      "Epoch [1/1], Step [2485/3504], Loss: 0.2851\n",
      "Epoch [1/1], Step [2486/3504], Loss: 0.2225\n",
      "Epoch [1/1], Step [2487/3504], Loss: 0.3122\n",
      "Epoch [1/1], Step [2488/3504], Loss: 0.3198\n",
      "Epoch [1/1], Step [2489/3504], Loss: 0.3286\n",
      "Epoch [1/1], Step [2490/3504], Loss: 0.3813\n",
      "Epoch [1/1], Step [2491/3504], Loss: 0.2859\n",
      "Epoch [1/1], Step [2492/3504], Loss: 0.3035\n",
      "Epoch [1/1], Step [2493/3504], Loss: 0.4213\n",
      "Epoch [1/1], Step [2494/3504], Loss: 0.3218\n",
      "Epoch [1/1], Step [2495/3504], Loss: 0.2643\n",
      "Epoch [1/1], Step [2496/3504], Loss: 0.3524\n",
      "Epoch [1/1], Step [2497/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [2498/3504], Loss: 0.4607\n",
      "Epoch [1/1], Step [2499/3504], Loss: 0.3094\n",
      "Epoch [1/1], Step [2500/3504], Loss: 0.3547\n",
      "Epoch [1/1], Step [2501/3504], Loss: 0.2925\n",
      "Epoch [1/1], Step [2502/3504], Loss: 0.4337\n",
      "Epoch [1/1], Step [2503/3504], Loss: 0.3259\n",
      "Epoch [1/1], Step [2504/3504], Loss: 0.3645\n",
      "Epoch [1/1], Step [2505/3504], Loss: 0.3495\n",
      "Epoch [1/1], Step [2506/3504], Loss: 0.3965\n",
      "Epoch [1/1], Step [2507/3504], Loss: 0.3869\n",
      "Epoch [1/1], Step [2508/3504], Loss: 0.3940\n",
      "Epoch [1/1], Step [2509/3504], Loss: 0.2728\n",
      "Epoch [1/1], Step [2510/3504], Loss: 0.3613\n",
      "Epoch [1/1], Step [2511/3504], Loss: 0.2924\n",
      "Epoch [1/1], Step [2512/3504], Loss: 0.3619\n",
      "Epoch [1/1], Step [2513/3504], Loss: 0.2839\n",
      "Epoch [1/1], Step [2514/3504], Loss: 0.3589\n",
      "Epoch [1/1], Step [2515/3504], Loss: 0.3301\n",
      "Epoch [1/1], Step [2516/3504], Loss: 0.3081\n",
      "Epoch [1/1], Step [2517/3504], Loss: 0.3374\n",
      "Epoch [1/1], Step [2518/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [2519/3504], Loss: 0.3699\n",
      "Epoch [1/1], Step [2520/3504], Loss: 0.3183\n",
      "Epoch [1/1], Step [2521/3504], Loss: 0.3217\n",
      "Epoch [1/1], Step [2522/3504], Loss: 0.3938\n",
      "Epoch [1/1], Step [2523/3504], Loss: 0.2919\n",
      "Epoch [1/1], Step [2524/3504], Loss: 0.3073\n",
      "Epoch [1/1], Step [2525/3504], Loss: 0.3420\n",
      "Epoch [1/1], Step [2526/3504], Loss: 0.3682\n",
      "Epoch [1/1], Step [2527/3504], Loss: 0.3226\n",
      "Epoch [1/1], Step [2528/3504], Loss: 0.2894\n",
      "Epoch [1/1], Step [2529/3504], Loss: 0.3802\n",
      "Epoch [1/1], Step [2530/3504], Loss: 0.4027\n",
      "Epoch [1/1], Step [2531/3504], Loss: 0.4230\n",
      "Epoch [1/1], Step [2532/3504], Loss: 0.2097\n",
      "Epoch [1/1], Step [2533/3504], Loss: 0.2518\n",
      "Epoch [1/1], Step [2534/3504], Loss: 0.3546\n",
      "Epoch [1/1], Step [2535/3504], Loss: 0.3663\n",
      "Epoch [1/1], Step [2536/3504], Loss: 0.3673\n",
      "Epoch [1/1], Step [2537/3504], Loss: 0.3066\n",
      "Epoch [1/1], Step [2538/3504], Loss: 0.2579\n",
      "Epoch [1/1], Step [2539/3504], Loss: 0.3411\n",
      "Epoch [1/1], Step [2540/3504], Loss: 0.3851\n",
      "Epoch [1/1], Step [2541/3504], Loss: 0.2876\n",
      "Epoch [1/1], Step [2542/3504], Loss: 0.3246\n",
      "Epoch [1/1], Step [2543/3504], Loss: 0.3004\n",
      "Epoch [1/1], Step [2544/3504], Loss: 0.3471\n",
      "Epoch [1/1], Step [2545/3504], Loss: 0.3617\n",
      "Epoch [1/1], Step [2546/3504], Loss: 0.3853\n",
      "Epoch [1/1], Step [2547/3504], Loss: 0.3567\n",
      "Epoch [1/1], Step [2548/3504], Loss: 0.3951\n",
      "Epoch [1/1], Step [2549/3504], Loss: 0.3589\n",
      "Epoch [1/1], Step [2550/3504], Loss: 0.3224\n",
      "Epoch [1/1], Step [2551/3504], Loss: 0.4059\n",
      "Epoch [1/1], Step [2552/3504], Loss: 0.3408\n",
      "Epoch [1/1], Step [2553/3504], Loss: 0.3428\n",
      "Epoch [1/1], Step [2554/3504], Loss: 0.4370\n",
      "Epoch [1/1], Step [2555/3504], Loss: 0.3205\n",
      "Epoch [1/1], Step [2556/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [2557/3504], Loss: 0.3817\n",
      "Epoch [1/1], Step [2558/3504], Loss: 0.3227\n",
      "Epoch [1/1], Step [2559/3504], Loss: 0.3716\n",
      "Epoch [1/1], Step [2560/3504], Loss: 0.4350\n",
      "Epoch [1/1], Step [2561/3504], Loss: 0.2273\n",
      "Epoch [1/1], Step [2562/3504], Loss: 0.3186\n",
      "Epoch [1/1], Step [2563/3504], Loss: 0.3977\n",
      "Epoch [1/1], Step [2564/3504], Loss: 0.3758\n",
      "Epoch [1/1], Step [2565/3504], Loss: 0.3493\n",
      "Epoch [1/1], Step [2566/3504], Loss: 0.3404\n",
      "Epoch [1/1], Step [2567/3504], Loss: 0.3267\n",
      "Epoch [1/1], Step [2568/3504], Loss: 0.3079\n",
      "Epoch [1/1], Step [2569/3504], Loss: 0.4060\n",
      "Epoch [1/1], Step [2570/3504], Loss: 0.3619\n",
      "Epoch [1/1], Step [2571/3504], Loss: 0.3069\n",
      "Epoch [1/1], Step [2572/3504], Loss: 0.2917\n",
      "Epoch [1/1], Step [2573/3504], Loss: 0.3604\n",
      "Epoch [1/1], Step [2574/3504], Loss: 0.3146\n",
      "Epoch [1/1], Step [2575/3504], Loss: 0.3880\n",
      "Epoch [1/1], Step [2576/3504], Loss: 0.3311\n",
      "Epoch [1/1], Step [2577/3504], Loss: 0.3091\n",
      "Epoch [1/1], Step [2578/3504], Loss: 0.2941\n",
      "Epoch [1/1], Step [2579/3504], Loss: 0.4496\n",
      "Epoch [1/1], Step [2580/3504], Loss: 0.4029\n",
      "Epoch [1/1], Step [2581/3504], Loss: 0.3101\n",
      "Epoch [1/1], Step [2582/3504], Loss: 0.3418\n",
      "Epoch [1/1], Step [2583/3504], Loss: 0.3603\n",
      "Epoch [1/1], Step [2584/3504], Loss: 0.6134\n",
      "Epoch [1/1], Step [2585/3504], Loss: 0.3082\n",
      "Epoch [1/1], Step [2586/3504], Loss: 0.2649\n",
      "Epoch [1/1], Step [2587/3504], Loss: 0.4274\n",
      "Epoch [1/1], Step [2588/3504], Loss: 0.2835\n",
      "Epoch [1/1], Step [2589/3504], Loss: 0.2970\n",
      "Epoch [1/1], Step [2590/3504], Loss: 0.3953\n",
      "Epoch [1/1], Step [2591/3504], Loss: 0.3187\n",
      "Epoch [1/1], Step [2592/3504], Loss: 0.4244\n",
      "Epoch [1/1], Step [2593/3504], Loss: 0.3349\n",
      "Epoch [1/1], Step [2594/3504], Loss: 0.2361\n",
      "Epoch [1/1], Step [2595/3504], Loss: 0.3494\n",
      "Epoch [1/1], Step [2596/3504], Loss: 0.4774\n",
      "Epoch [1/1], Step [2597/3504], Loss: 0.3774\n",
      "Epoch [1/1], Step [2598/3504], Loss: 0.3442\n",
      "Epoch [1/1], Step [2599/3504], Loss: 0.3814\n",
      "Epoch [1/1], Step [2600/3504], Loss: 0.2694\n",
      "Epoch [1/1], Step [2601/3504], Loss: 0.3263\n",
      "Epoch [1/1], Step [2602/3504], Loss: 0.3500\n",
      "Epoch [1/1], Step [2603/3504], Loss: 0.3127\n",
      "Epoch [1/1], Step [2604/3504], Loss: 0.2656\n",
      "Epoch [1/1], Step [2605/3504], Loss: 0.3707\n",
      "Epoch [1/1], Step [2606/3504], Loss: 0.4052\n",
      "Epoch [1/1], Step [2607/3504], Loss: 0.3048\n",
      "Epoch [1/1], Step [2608/3504], Loss: 0.3475\n",
      "Epoch [1/1], Step [2609/3504], Loss: 0.3458\n",
      "Epoch [1/1], Step [2610/3504], Loss: 0.3185\n",
      "Epoch [1/1], Step [2611/3504], Loss: 0.3736\n",
      "Epoch [1/1], Step [2612/3504], Loss: 0.3689\n",
      "Epoch [1/1], Step [2613/3504], Loss: 0.3685\n",
      "Epoch [1/1], Step [2614/3504], Loss: 0.3990\n",
      "Epoch [1/1], Step [2615/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [2616/3504], Loss: 0.2806\n",
      "Epoch [1/1], Step [2617/3504], Loss: 0.4128\n",
      "Epoch [1/1], Step [2618/3504], Loss: 0.3321\n",
      "Epoch [1/1], Step [2619/3504], Loss: 0.2843\n",
      "Epoch [1/1], Step [2620/3504], Loss: 0.2704\n",
      "Epoch [1/1], Step [2621/3504], Loss: 0.2863\n",
      "Epoch [1/1], Step [2622/3504], Loss: 0.3215\n",
      "Epoch [1/1], Step [2623/3504], Loss: 0.3423\n",
      "Epoch [1/1], Step [2624/3504], Loss: 0.3723\n",
      "Epoch [1/1], Step [2625/3504], Loss: 0.3882\n",
      "Epoch [1/1], Step [2626/3504], Loss: 0.3767\n",
      "Epoch [1/1], Step [2627/3504], Loss: 0.2817\n",
      "Epoch [1/1], Step [2628/3504], Loss: 0.2549\n",
      "Epoch [1/1], Step [2629/3504], Loss: 0.3345\n",
      "Epoch [1/1], Step [2630/3504], Loss: 0.4089\n",
      "Epoch [1/1], Step [2631/3504], Loss: 0.2926\n",
      "Epoch [1/1], Step [2632/3504], Loss: 0.3598\n",
      "Epoch [1/1], Step [2633/3504], Loss: 0.3042\n",
      "Epoch [1/1], Step [2634/3504], Loss: 0.3794\n",
      "Epoch [1/1], Step [2635/3504], Loss: 0.3536\n",
      "Epoch [1/1], Step [2636/3504], Loss: 0.4119\n",
      "Epoch [1/1], Step [2637/3504], Loss: 0.2980\n",
      "Epoch [1/1], Step [2638/3504], Loss: 0.3022\n",
      "Epoch [1/1], Step [2639/3504], Loss: 0.3746\n",
      "Epoch [1/1], Step [2640/3504], Loss: 0.2973\n",
      "Epoch [1/1], Step [2641/3504], Loss: 0.2541\n",
      "Epoch [1/1], Step [2642/3504], Loss: 0.3565\n",
      "Epoch [1/1], Step [2643/3504], Loss: 0.2529\n",
      "Epoch [1/1], Step [2644/3504], Loss: 0.3261\n",
      "Epoch [1/1], Step [2645/3504], Loss: 0.3582\n",
      "Epoch [1/1], Step [2646/3504], Loss: 0.3081\n",
      "Epoch [1/1], Step [2647/3504], Loss: 0.3439\n",
      "Epoch [1/1], Step [2648/3504], Loss: 0.2947\n",
      "Epoch [1/1], Step [2649/3504], Loss: 0.3175\n",
      "Epoch [1/1], Step [2650/3504], Loss: 0.2829\n",
      "Epoch [1/1], Step [2651/3504], Loss: 0.3368\n",
      "Epoch [1/1], Step [2652/3504], Loss: 0.3280\n",
      "Epoch [1/1], Step [2653/3504], Loss: 0.3381\n",
      "Epoch [1/1], Step [2654/3504], Loss: 0.4337\n",
      "Epoch [1/1], Step [2655/3504], Loss: 0.3458\n",
      "Epoch [1/1], Step [2656/3504], Loss: 0.4067\n",
      "Epoch [1/1], Step [2657/3504], Loss: 0.3945\n",
      "Epoch [1/1], Step [2658/3504], Loss: 0.3768\n",
      "Epoch [1/1], Step [2659/3504], Loss: 0.3742\n",
      "Epoch [1/1], Step [2660/3504], Loss: 0.4156\n",
      "Epoch [1/1], Step [2661/3504], Loss: 0.3738\n",
      "Epoch [1/1], Step [2662/3504], Loss: 0.3698\n",
      "Epoch [1/1], Step [2663/3504], Loss: 0.3860\n",
      "Epoch [1/1], Step [2664/3504], Loss: 0.5185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [2665/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [2666/3504], Loss: 0.3079\n",
      "Epoch [1/1], Step [2667/3504], Loss: 0.3445\n",
      "Epoch [1/1], Step [2668/3504], Loss: 0.3667\n",
      "Epoch [1/1], Step [2669/3504], Loss: 0.3362\n",
      "Epoch [1/1], Step [2670/3504], Loss: 0.2759\n",
      "Epoch [1/1], Step [2671/3504], Loss: 0.3090\n",
      "Epoch [1/1], Step [2672/3504], Loss: 0.3195\n",
      "Epoch [1/1], Step [2673/3504], Loss: 0.2852\n",
      "Epoch [1/1], Step [2674/3504], Loss: 0.3453\n",
      "Epoch [1/1], Step [2675/3504], Loss: 0.3512\n",
      "Epoch [1/1], Step [2676/3504], Loss: 0.3481\n",
      "Epoch [1/1], Step [2677/3504], Loss: 0.3479\n",
      "Epoch [1/1], Step [2678/3504], Loss: 0.3142\n",
      "Epoch [1/1], Step [2679/3504], Loss: 0.2576\n",
      "Epoch [1/1], Step [2680/3504], Loss: 0.3022\n",
      "Epoch [1/1], Step [2681/3504], Loss: 0.3718\n",
      "Epoch [1/1], Step [2682/3504], Loss: 0.2481\n",
      "Epoch [1/1], Step [2683/3504], Loss: 0.3440\n",
      "Epoch [1/1], Step [2684/3504], Loss: 0.3273\n",
      "Epoch [1/1], Step [2685/3504], Loss: 0.4585\n",
      "Epoch [1/1], Step [2686/3504], Loss: 0.2996\n",
      "Epoch [1/1], Step [2687/3504], Loss: 0.3198\n",
      "Epoch [1/1], Step [2688/3504], Loss: 0.3465\n",
      "Epoch [1/1], Step [2689/3504], Loss: 0.2838\n",
      "Epoch [1/1], Step [2690/3504], Loss: 0.4028\n",
      "Epoch [1/1], Step [2691/3504], Loss: 0.1905\n",
      "Epoch [1/1], Step [2692/3504], Loss: 0.3447\n",
      "Epoch [1/1], Step [2693/3504], Loss: 0.2731\n",
      "Epoch [1/1], Step [2694/3504], Loss: 0.3914\n",
      "Epoch [1/1], Step [2695/3504], Loss: 0.3463\n",
      "Epoch [1/1], Step [2696/3504], Loss: 0.3468\n",
      "Epoch [1/1], Step [2697/3504], Loss: 0.3185\n",
      "Epoch [1/1], Step [2698/3504], Loss: 0.3614\n",
      "Epoch [1/1], Step [2699/3504], Loss: 0.3666\n",
      "Epoch [1/1], Step [2700/3504], Loss: 0.4337\n",
      "Epoch [1/1], Step [2701/3504], Loss: 0.3623\n",
      "Epoch [1/1], Step [2702/3504], Loss: 0.2854\n",
      "Epoch [1/1], Step [2703/3504], Loss: 0.4103\n",
      "Epoch [1/1], Step [2704/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [2705/3504], Loss: 0.3693\n",
      "Epoch [1/1], Step [2706/3504], Loss: 0.4342\n",
      "Epoch [1/1], Step [2707/3504], Loss: 0.2878\n",
      "Epoch [1/1], Step [2708/3504], Loss: 0.4243\n",
      "Epoch [1/1], Step [2709/3504], Loss: 0.2970\n",
      "Epoch [1/1], Step [2710/3504], Loss: 0.2490\n",
      "Epoch [1/1], Step [2711/3504], Loss: 0.3201\n",
      "Epoch [1/1], Step [2712/3504], Loss: 0.3900\n",
      "Epoch [1/1], Step [2713/3504], Loss: 0.2955\n",
      "Epoch [1/1], Step [2714/3504], Loss: 0.2907\n",
      "Epoch [1/1], Step [2715/3504], Loss: 0.3491\n",
      "Epoch [1/1], Step [2716/3504], Loss: 0.4493\n",
      "Epoch [1/1], Step [2717/3504], Loss: 0.3471\n",
      "Epoch [1/1], Step [2718/3504], Loss: 0.3301\n",
      "Epoch [1/1], Step [2719/3504], Loss: 0.2863\n",
      "Epoch [1/1], Step [2720/3504], Loss: 0.3372\n",
      "Epoch [1/1], Step [2721/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [2722/3504], Loss: 0.3235\n",
      "Epoch [1/1], Step [2723/3504], Loss: 0.3206\n",
      "Epoch [1/1], Step [2724/3504], Loss: 0.4983\n",
      "Epoch [1/1], Step [2725/3504], Loss: 0.3046\n",
      "Epoch [1/1], Step [2726/3504], Loss: 0.4473\n",
      "Epoch [1/1], Step [2727/3504], Loss: 0.3157\n",
      "Epoch [1/1], Step [2728/3504], Loss: 0.3630\n",
      "Epoch [1/1], Step [2729/3504], Loss: 0.4423\n",
      "Epoch [1/1], Step [2730/3504], Loss: 0.3260\n",
      "Epoch [1/1], Step [2731/3504], Loss: 0.3859\n",
      "Epoch [1/1], Step [2732/3504], Loss: 0.4150\n",
      "Epoch [1/1], Step [2733/3504], Loss: 0.4183\n",
      "Epoch [1/1], Step [2734/3504], Loss: 0.3092\n",
      "Epoch [1/1], Step [2735/3504], Loss: 0.3396\n",
      "Epoch [1/1], Step [2736/3504], Loss: 0.3275\n",
      "Epoch [1/1], Step [2737/3504], Loss: 0.4150\n",
      "Epoch [1/1], Step [2738/3504], Loss: 0.3344\n",
      "Epoch [1/1], Step [2739/3504], Loss: 0.2587\n",
      "Epoch [1/1], Step [2740/3504], Loss: 0.3023\n",
      "Epoch [1/1], Step [2741/3504], Loss: 0.3543\n",
      "Epoch [1/1], Step [2742/3504], Loss: 0.3556\n",
      "Epoch [1/1], Step [2743/3504], Loss: 0.3168\n",
      "Epoch [1/1], Step [2744/3504], Loss: 0.3312\n",
      "Epoch [1/1], Step [2745/3504], Loss: 0.3875\n",
      "Epoch [1/1], Step [2746/3504], Loss: 0.3935\n",
      "Epoch [1/1], Step [2747/3504], Loss: 0.4072\n",
      "Epoch [1/1], Step [2748/3504], Loss: 0.2931\n",
      "Epoch [1/1], Step [2749/3504], Loss: 0.3184\n",
      "Epoch [1/1], Step [2750/3504], Loss: 0.3364\n",
      "Epoch [1/1], Step [2751/3504], Loss: 0.3433\n",
      "Epoch [1/1], Step [2752/3504], Loss: 0.3659\n",
      "Epoch [1/1], Step [2753/3504], Loss: 0.3378\n",
      "Epoch [1/1], Step [2754/3504], Loss: 0.4143\n",
      "Epoch [1/1], Step [2755/3504], Loss: 0.3635\n",
      "Epoch [1/1], Step [2756/3504], Loss: 0.3316\n",
      "Epoch [1/1], Step [2757/3504], Loss: 0.2912\n",
      "Epoch [1/1], Step [2758/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [2759/3504], Loss: 0.3270\n",
      "Epoch [1/1], Step [2760/3504], Loss: 0.3053\n",
      "Epoch [1/1], Step [2761/3504], Loss: 0.2768\n",
      "Epoch [1/1], Step [2762/3504], Loss: 0.3118\n",
      "Epoch [1/1], Step [2763/3504], Loss: 0.3080\n",
      "Epoch [1/1], Step [2764/3504], Loss: 0.2971\n",
      "Epoch [1/1], Step [2765/3504], Loss: 0.3488\n",
      "Epoch [1/1], Step [2766/3504], Loss: 0.3122\n",
      "Epoch [1/1], Step [2767/3504], Loss: 0.3496\n",
      "Epoch [1/1], Step [2768/3504], Loss: 0.3270\n",
      "Epoch [1/1], Step [2769/3504], Loss: 0.3751\n",
      "Epoch [1/1], Step [2770/3504], Loss: 0.3779\n",
      "Epoch [1/1], Step [2771/3504], Loss: 0.3731\n",
      "Epoch [1/1], Step [2772/3504], Loss: 0.3921\n",
      "Epoch [1/1], Step [2773/3504], Loss: 0.3186\n",
      "Epoch [1/1], Step [2774/3504], Loss: 0.3701\n",
      "Epoch [1/1], Step [2775/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [2776/3504], Loss: 0.3073\n",
      "Epoch [1/1], Step [2777/3504], Loss: 0.4203\n",
      "Epoch [1/1], Step [2778/3504], Loss: 0.2755\n",
      "Epoch [1/1], Step [2779/3504], Loss: 0.2582\n",
      "Epoch [1/1], Step [2780/3504], Loss: 0.3380\n",
      "Epoch [1/1], Step [2781/3504], Loss: 0.3734\n",
      "Epoch [1/1], Step [2782/3504], Loss: 0.3020\n",
      "Epoch [1/1], Step [2783/3504], Loss: 0.3245\n",
      "Epoch [1/1], Step [2784/3504], Loss: 0.3268\n",
      "Epoch [1/1], Step [2785/3504], Loss: 0.3414\n",
      "Epoch [1/1], Step [2786/3504], Loss: 0.3541\n",
      "Epoch [1/1], Step [2787/3504], Loss: 0.3940\n",
      "Epoch [1/1], Step [2788/3504], Loss: 0.3686\n",
      "Epoch [1/1], Step [2789/3504], Loss: 0.4190\n",
      "Epoch [1/1], Step [2790/3504], Loss: 0.2931\n",
      "Epoch [1/1], Step [2791/3504], Loss: 0.4055\n",
      "Epoch [1/1], Step [2792/3504], Loss: 0.3608\n",
      "Epoch [1/1], Step [2793/3504], Loss: 0.3399\n",
      "Epoch [1/1], Step [2794/3504], Loss: 0.3621\n",
      "Epoch [1/1], Step [2795/3504], Loss: 0.3607\n",
      "Epoch [1/1], Step [2796/3504], Loss: 0.3759\n",
      "Epoch [1/1], Step [2797/3504], Loss: 0.3247\n",
      "Epoch [1/1], Step [2798/3504], Loss: 0.3247\n",
      "Epoch [1/1], Step [2799/3504], Loss: 0.4331\n",
      "Epoch [1/1], Step [2800/3504], Loss: 0.4164\n",
      "Epoch [1/1], Step [2801/3504], Loss: 0.2881\n",
      "Epoch [1/1], Step [2802/3504], Loss: 0.3400\n",
      "Epoch [1/1], Step [2803/3504], Loss: 0.3405\n",
      "Epoch [1/1], Step [2804/3504], Loss: 0.3632\n",
      "Epoch [1/1], Step [2805/3504], Loss: 0.3808\n",
      "Epoch [1/1], Step [2806/3504], Loss: 0.3633\n",
      "Epoch [1/1], Step [2807/3504], Loss: 0.3356\n",
      "Epoch [1/1], Step [2808/3504], Loss: 0.3071\n",
      "Epoch [1/1], Step [2809/3504], Loss: 0.3261\n",
      "Epoch [1/1], Step [2810/3504], Loss: 0.3577\n",
      "Epoch [1/1], Step [2811/3504], Loss: 0.2963\n",
      "Epoch [1/1], Step [2812/3504], Loss: 0.3230\n",
      "Epoch [1/1], Step [2813/3504], Loss: 0.4052\n",
      "Epoch [1/1], Step [2814/3504], Loss: 0.3070\n",
      "Epoch [1/1], Step [2815/3504], Loss: 0.3656\n",
      "Epoch [1/1], Step [2816/3504], Loss: 0.2613\n",
      "Epoch [1/1], Step [2817/3504], Loss: 0.3186\n",
      "Epoch [1/1], Step [2818/3504], Loss: 0.3511\n",
      "Epoch [1/1], Step [2819/3504], Loss: 0.4152\n",
      "Epoch [1/1], Step [2820/3504], Loss: 0.3454\n",
      "Epoch [1/1], Step [2821/3504], Loss: 0.3312\n",
      "Epoch [1/1], Step [2822/3504], Loss: 0.3085\n",
      "Epoch [1/1], Step [2823/3504], Loss: 0.3446\n",
      "Epoch [1/1], Step [2824/3504], Loss: 0.4219\n",
      "Epoch [1/1], Step [2825/3504], Loss: 0.3052\n",
      "Epoch [1/1], Step [2826/3504], Loss: 0.2890\n",
      "Epoch [1/1], Step [2827/3504], Loss: 0.2782\n",
      "Epoch [1/1], Step [2828/3504], Loss: 0.3616\n",
      "Epoch [1/1], Step [2829/3504], Loss: 0.2370\n",
      "Epoch [1/1], Step [2830/3504], Loss: 0.3472\n",
      "Epoch [1/1], Step [2831/3504], Loss: 0.3736\n",
      "Epoch [1/1], Step [2832/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [2833/3504], Loss: 0.4349\n",
      "Epoch [1/1], Step [2834/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [2835/3504], Loss: 0.3299\n",
      "Epoch [1/1], Step [2836/3504], Loss: 0.4194\n",
      "Epoch [1/1], Step [2837/3504], Loss: 0.2770\n",
      "Epoch [1/1], Step [2838/3504], Loss: 0.3567\n",
      "Epoch [1/1], Step [2839/3504], Loss: 0.3047\n",
      "Epoch [1/1], Step [2840/3504], Loss: 0.3834\n",
      "Epoch [1/1], Step [2841/3504], Loss: 0.2885\n",
      "Epoch [1/1], Step [2842/3504], Loss: 0.2465\n",
      "Epoch [1/1], Step [2843/3504], Loss: 0.2778\n",
      "Epoch [1/1], Step [2844/3504], Loss: 0.3322\n",
      "Epoch [1/1], Step [2845/3504], Loss: 0.3120\n",
      "Epoch [1/1], Step [2846/3504], Loss: 0.4128\n",
      "Epoch [1/1], Step [2847/3504], Loss: 0.3233\n",
      "Epoch [1/1], Step [2848/3504], Loss: 0.4200\n",
      "Epoch [1/1], Step [2849/3504], Loss: 0.3233\n",
      "Epoch [1/1], Step [2850/3504], Loss: 0.3499\n",
      "Epoch [1/1], Step [2851/3504], Loss: 0.2362\n",
      "Epoch [1/1], Step [2852/3504], Loss: 0.2730\n",
      "Epoch [1/1], Step [2853/3504], Loss: 0.3393\n",
      "Epoch [1/1], Step [2854/3504], Loss: 0.4173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [2855/3504], Loss: 0.2860\n",
      "Epoch [1/1], Step [2856/3504], Loss: 0.2583\n",
      "Epoch [1/1], Step [2857/3504], Loss: 0.3026\n",
      "Epoch [1/1], Step [2858/3504], Loss: 0.3621\n",
      "Epoch [1/1], Step [2859/3504], Loss: 0.3103\n",
      "Epoch [1/1], Step [2860/3504], Loss: 0.4038\n",
      "Epoch [1/1], Step [2861/3504], Loss: 0.3012\n",
      "Epoch [1/1], Step [2862/3504], Loss: 0.4350\n",
      "Epoch [1/1], Step [2863/3504], Loss: 0.2910\n",
      "Epoch [1/1], Step [2864/3504], Loss: 0.4779\n",
      "Epoch [1/1], Step [2865/3504], Loss: 0.3039\n",
      "Epoch [1/1], Step [2866/3504], Loss: 0.4061\n",
      "Epoch [1/1], Step [2867/3504], Loss: 0.3623\n",
      "Epoch [1/1], Step [2868/3504], Loss: 0.4013\n",
      "Epoch [1/1], Step [2869/3504], Loss: 0.4308\n",
      "Epoch [1/1], Step [2870/3504], Loss: 0.3772\n",
      "Epoch [1/1], Step [2871/3504], Loss: 0.3059\n",
      "Epoch [1/1], Step [2872/3504], Loss: 0.2855\n",
      "Epoch [1/1], Step [2873/3504], Loss: 0.3239\n",
      "Epoch [1/1], Step [2874/3504], Loss: 0.3438\n",
      "Epoch [1/1], Step [2875/3504], Loss: 0.3855\n",
      "Epoch [1/1], Step [2876/3504], Loss: 0.2762\n",
      "Epoch [1/1], Step [2877/3504], Loss: 0.3447\n",
      "Epoch [1/1], Step [2878/3504], Loss: 0.3378\n",
      "Epoch [1/1], Step [2879/3504], Loss: 0.3767\n",
      "Epoch [1/1], Step [2880/3504], Loss: 0.2535\n",
      "Epoch [1/1], Step [2881/3504], Loss: 0.3412\n",
      "Epoch [1/1], Step [2882/3504], Loss: 0.3472\n",
      "Epoch [1/1], Step [2883/3504], Loss: 0.3982\n",
      "Epoch [1/1], Step [2884/3504], Loss: 0.3434\n",
      "Epoch [1/1], Step [2885/3504], Loss: 0.4116\n",
      "Epoch [1/1], Step [2886/3504], Loss: 0.2929\n",
      "Epoch [1/1], Step [2887/3504], Loss: 0.3768\n",
      "Epoch [1/1], Step [2888/3504], Loss: 0.3038\n",
      "Epoch [1/1], Step [2889/3504], Loss: 0.3777\n",
      "Epoch [1/1], Step [2890/3504], Loss: 0.3504\n",
      "Epoch [1/1], Step [2891/3504], Loss: 0.4037\n",
      "Epoch [1/1], Step [2892/3504], Loss: 0.2902\n",
      "Epoch [1/1], Step [2893/3504], Loss: 0.2951\n",
      "Epoch [1/1], Step [2894/3504], Loss: 0.3257\n",
      "Epoch [1/1], Step [2895/3504], Loss: 0.3193\n",
      "Epoch [1/1], Step [2896/3504], Loss: 0.3085\n",
      "Epoch [1/1], Step [2897/3504], Loss: 0.3049\n",
      "Epoch [1/1], Step [2898/3504], Loss: 0.3882\n",
      "Epoch [1/1], Step [2899/3504], Loss: 0.3921\n",
      "Epoch [1/1], Step [2900/3504], Loss: 0.3356\n",
      "Epoch [1/1], Step [2901/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [2902/3504], Loss: 0.3369\n",
      "Epoch [1/1], Step [2903/3504], Loss: 0.3038\n",
      "Epoch [1/1], Step [2904/3504], Loss: 0.2944\n",
      "Epoch [1/1], Step [2905/3504], Loss: 0.4261\n",
      "Epoch [1/1], Step [2906/3504], Loss: 0.3546\n",
      "Epoch [1/1], Step [2907/3504], Loss: 0.3702\n",
      "Epoch [1/1], Step [2908/3504], Loss: 0.3474\n",
      "Epoch [1/1], Step [2909/3504], Loss: 0.2985\n",
      "Epoch [1/1], Step [2910/3504], Loss: 0.2775\n",
      "Epoch [1/1], Step [2911/3504], Loss: 0.4024\n",
      "Epoch [1/1], Step [2912/3504], Loss: 0.3237\n",
      "Epoch [1/1], Step [2913/3504], Loss: 0.3309\n",
      "Epoch [1/1], Step [2914/3504], Loss: 0.3434\n",
      "Epoch [1/1], Step [2915/3504], Loss: 0.3261\n",
      "Epoch [1/1], Step [2916/3504], Loss: 0.2874\n",
      "Epoch [1/1], Step [2917/3504], Loss: 0.3502\n",
      "Epoch [1/1], Step [2918/3504], Loss: 0.2831\n",
      "Epoch [1/1], Step [2919/3504], Loss: 0.3559\n",
      "Epoch [1/1], Step [2920/3504], Loss: 0.3313\n",
      "Epoch [1/1], Step [2921/3504], Loss: 0.3376\n",
      "Epoch [1/1], Step [2922/3504], Loss: 0.4422\n",
      "Epoch [1/1], Step [2923/3504], Loss: 0.3389\n",
      "Epoch [1/1], Step [2924/3504], Loss: 0.3858\n",
      "Epoch [1/1], Step [2925/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [2926/3504], Loss: 0.3379\n",
      "Epoch [1/1], Step [2927/3504], Loss: 0.2994\n",
      "Epoch [1/1], Step [2928/3504], Loss: 0.3680\n",
      "Epoch [1/1], Step [2929/3504], Loss: 0.3644\n",
      "Epoch [1/1], Step [2930/3504], Loss: 0.3646\n",
      "Epoch [1/1], Step [2931/3504], Loss: 0.3364\n",
      "Epoch [1/1], Step [2932/3504], Loss: 0.4208\n",
      "Epoch [1/1], Step [2933/3504], Loss: 0.3373\n",
      "Epoch [1/1], Step [2934/3504], Loss: 0.3823\n",
      "Epoch [1/1], Step [2935/3504], Loss: 0.2663\n",
      "Epoch [1/1], Step [2936/3504], Loss: 0.3742\n",
      "Epoch [1/1], Step [2937/3504], Loss: 0.3979\n",
      "Epoch [1/1], Step [2938/3504], Loss: 0.2907\n",
      "Epoch [1/1], Step [2939/3504], Loss: 0.3314\n",
      "Epoch [1/1], Step [2940/3504], Loss: 0.4399\n",
      "Epoch [1/1], Step [2941/3504], Loss: 0.3333\n",
      "Epoch [1/1], Step [2942/3504], Loss: 0.2936\n",
      "Epoch [1/1], Step [2943/3504], Loss: 0.4008\n",
      "Epoch [1/1], Step [2944/3504], Loss: 0.3052\n",
      "Epoch [1/1], Step [2945/3504], Loss: 0.3460\n",
      "Epoch [1/1], Step [2946/3504], Loss: 0.3435\n",
      "Epoch [1/1], Step [2947/3504], Loss: 0.3072\n",
      "Epoch [1/1], Step [2948/3504], Loss: 0.3465\n",
      "Epoch [1/1], Step [2949/3504], Loss: 0.3292\n",
      "Epoch [1/1], Step [2950/3504], Loss: 0.3704\n",
      "Epoch [1/1], Step [2951/3504], Loss: 0.3647\n",
      "Epoch [1/1], Step [2952/3504], Loss: 0.3340\n",
      "Epoch [1/1], Step [2953/3504], Loss: 0.4407\n",
      "Epoch [1/1], Step [2954/3504], Loss: 0.4374\n",
      "Epoch [1/1], Step [2955/3504], Loss: 0.3664\n",
      "Epoch [1/1], Step [2956/3504], Loss: 0.3398\n",
      "Epoch [1/1], Step [2957/3504], Loss: 0.3466\n",
      "Epoch [1/1], Step [2958/3504], Loss: 0.3570\n",
      "Epoch [1/1], Step [2959/3504], Loss: 0.3751\n",
      "Epoch [1/1], Step [2960/3504], Loss: 0.3234\n",
      "Epoch [1/1], Step [2961/3504], Loss: 0.3060\n",
      "Epoch [1/1], Step [2962/3504], Loss: 0.3364\n",
      "Epoch [1/1], Step [2963/3504], Loss: 0.3260\n",
      "Epoch [1/1], Step [2964/3504], Loss: 0.4277\n",
      "Epoch [1/1], Step [2965/3504], Loss: 0.3821\n",
      "Epoch [1/1], Step [2966/3504], Loss: 0.3951\n",
      "Epoch [1/1], Step [2967/3504], Loss: 0.3175\n",
      "Epoch [1/1], Step [2968/3504], Loss: 0.2620\n",
      "Epoch [1/1], Step [2969/3504], Loss: 0.3344\n",
      "Epoch [1/1], Step [2970/3504], Loss: 0.3187\n",
      "Epoch [1/1], Step [2971/3504], Loss: 0.3638\n",
      "Epoch [1/1], Step [2972/3504], Loss: 0.3343\n",
      "Epoch [1/1], Step [2973/3504], Loss: 0.3252\n",
      "Epoch [1/1], Step [2974/3504], Loss: 0.3798\n",
      "Epoch [1/1], Step [2975/3504], Loss: 0.3525\n",
      "Epoch [1/1], Step [2976/3504], Loss: 0.2673\n",
      "Epoch [1/1], Step [2977/3504], Loss: 0.3833\n",
      "Epoch [1/1], Step [2978/3504], Loss: 0.2689\n",
      "Epoch [1/1], Step [2979/3504], Loss: 0.3417\n",
      "Epoch [1/1], Step [2980/3504], Loss: 0.3566\n",
      "Epoch [1/1], Step [2981/3504], Loss: 0.3574\n",
      "Epoch [1/1], Step [2982/3504], Loss: 0.2521\n",
      "Epoch [1/1], Step [2983/3504], Loss: 0.3968\n",
      "Epoch [1/1], Step [2984/3504], Loss: 0.4005\n",
      "Epoch [1/1], Step [2985/3504], Loss: 0.4390\n",
      "Epoch [1/1], Step [2986/3504], Loss: 0.3722\n",
      "Epoch [1/1], Step [2987/3504], Loss: 0.3044\n",
      "Epoch [1/1], Step [2988/3504], Loss: 0.3870\n",
      "Epoch [1/1], Step [2989/3504], Loss: 0.3079\n",
      "Epoch [1/1], Step [2990/3504], Loss: 0.2946\n",
      "Epoch [1/1], Step [2991/3504], Loss: 0.3773\n",
      "Epoch [1/1], Step [2992/3504], Loss: 0.2879\n",
      "Epoch [1/1], Step [2993/3504], Loss: 0.3246\n",
      "Epoch [1/1], Step [2994/3504], Loss: 0.3507\n",
      "Epoch [1/1], Step [2995/3504], Loss: 0.2904\n",
      "Epoch [1/1], Step [2996/3504], Loss: 0.3015\n",
      "Epoch [1/1], Step [2997/3504], Loss: 0.3678\n",
      "Epoch [1/1], Step [2998/3504], Loss: 0.3212\n",
      "Epoch [1/1], Step [2999/3504], Loss: 0.3521\n",
      "Epoch [1/1], Step [3000/3504], Loss: 0.3247\n",
      "Epoch [1/1], Step [3001/3504], Loss: 0.3191\n",
      "Epoch [1/1], Step [3002/3504], Loss: 0.3619\n",
      "Epoch [1/1], Step [3003/3504], Loss: 0.3509\n",
      "Epoch [1/1], Step [3004/3504], Loss: 0.3515\n",
      "Epoch [1/1], Step [3005/3504], Loss: 0.2960\n",
      "Epoch [1/1], Step [3006/3504], Loss: 0.4719\n",
      "Epoch [1/1], Step [3007/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [3008/3504], Loss: 0.3064\n",
      "Epoch [1/1], Step [3009/3504], Loss: 0.3059\n",
      "Epoch [1/1], Step [3010/3504], Loss: 0.4962\n",
      "Epoch [1/1], Step [3011/3504], Loss: 0.3290\n",
      "Epoch [1/1], Step [3012/3504], Loss: 0.2986\n",
      "Epoch [1/1], Step [3013/3504], Loss: 0.4110\n",
      "Epoch [1/1], Step [3014/3504], Loss: 0.3497\n",
      "Epoch [1/1], Step [3015/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [3016/3504], Loss: 0.3242\n",
      "Epoch [1/1], Step [3017/3504], Loss: 0.4438\n",
      "Epoch [1/1], Step [3018/3504], Loss: 0.3512\n",
      "Epoch [1/1], Step [3019/3504], Loss: 0.3234\n",
      "Epoch [1/1], Step [3020/3504], Loss: 0.4095\n",
      "Epoch [1/1], Step [3021/3504], Loss: 0.4704\n",
      "Epoch [1/1], Step [3022/3504], Loss: 0.3271\n",
      "Epoch [1/1], Step [3023/3504], Loss: 0.3220\n",
      "Epoch [1/1], Step [3024/3504], Loss: 0.3091\n",
      "Epoch [1/1], Step [3025/3504], Loss: 0.3822\n",
      "Epoch [1/1], Step [3026/3504], Loss: 0.3397\n",
      "Epoch [1/1], Step [3027/3504], Loss: 0.3589\n",
      "Epoch [1/1], Step [3028/3504], Loss: 0.2921\n",
      "Epoch [1/1], Step [3029/3504], Loss: 0.4885\n",
      "Epoch [1/1], Step [3030/3504], Loss: 0.3319\n",
      "Epoch [1/1], Step [3031/3504], Loss: 0.3899\n",
      "Epoch [1/1], Step [3032/3504], Loss: 0.4010\n",
      "Epoch [1/1], Step [3033/3504], Loss: 0.4038\n",
      "Epoch [1/1], Step [3034/3504], Loss: 0.3090\n",
      "Epoch [1/1], Step [3035/3504], Loss: 0.2746\n",
      "Epoch [1/1], Step [3036/3504], Loss: 0.3373\n",
      "Epoch [1/1], Step [3037/3504], Loss: 0.3719\n",
      "Epoch [1/1], Step [3038/3504], Loss: 0.3754\n",
      "Epoch [1/1], Step [3039/3504], Loss: 0.3105\n",
      "Epoch [1/1], Step [3040/3504], Loss: 0.4301\n",
      "Epoch [1/1], Step [3041/3504], Loss: 0.3092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [3042/3504], Loss: 0.3658\n",
      "Epoch [1/1], Step [3043/3504], Loss: 0.3619\n",
      "Epoch [1/1], Step [3044/3504], Loss: 0.3949\n",
      "Epoch [1/1], Step [3045/3504], Loss: 0.2430\n",
      "Epoch [1/1], Step [3046/3504], Loss: 0.3571\n",
      "Epoch [1/1], Step [3047/3504], Loss: 0.2799\n",
      "Epoch [1/1], Step [3048/3504], Loss: 0.4089\n",
      "Epoch [1/1], Step [3049/3504], Loss: 0.3139\n",
      "Epoch [1/1], Step [3050/3504], Loss: 0.3171\n",
      "Epoch [1/1], Step [3051/3504], Loss: 0.3555\n",
      "Epoch [1/1], Step [3052/3504], Loss: 0.4067\n",
      "Epoch [1/1], Step [3053/3504], Loss: 0.3787\n",
      "Epoch [1/1], Step [3054/3504], Loss: 0.3702\n",
      "Epoch [1/1], Step [3055/3504], Loss: 0.4102\n",
      "Epoch [1/1], Step [3056/3504], Loss: 0.3155\n",
      "Epoch [1/1], Step [3057/3504], Loss: 0.3526\n",
      "Epoch [1/1], Step [3058/3504], Loss: 0.3746\n",
      "Epoch [1/1], Step [3059/3504], Loss: 0.2989\n",
      "Epoch [1/1], Step [3060/3504], Loss: 0.3508\n",
      "Epoch [1/1], Step [3061/3504], Loss: 0.3745\n",
      "Epoch [1/1], Step [3062/3504], Loss: 0.2764\n",
      "Epoch [1/1], Step [3063/3504], Loss: 0.2988\n",
      "Epoch [1/1], Step [3064/3504], Loss: 0.3215\n",
      "Epoch [1/1], Step [3065/3504], Loss: 0.3917\n",
      "Epoch [1/1], Step [3066/3504], Loss: 0.3945\n",
      "Epoch [1/1], Step [3067/3504], Loss: 0.3544\n",
      "Epoch [1/1], Step [3068/3504], Loss: 0.3059\n",
      "Epoch [1/1], Step [3069/3504], Loss: 0.3063\n",
      "Epoch [1/1], Step [3070/3504], Loss: 0.2879\n",
      "Epoch [1/1], Step [3071/3504], Loss: 0.3575\n",
      "Epoch [1/1], Step [3072/3504], Loss: 0.3700\n",
      "Epoch [1/1], Step [3073/3504], Loss: 0.3462\n",
      "Epoch [1/1], Step [3074/3504], Loss: 0.4108\n",
      "Epoch [1/1], Step [3075/3504], Loss: 0.3711\n",
      "Epoch [1/1], Step [3076/3504], Loss: 0.3497\n",
      "Epoch [1/1], Step [3077/3504], Loss: 0.3758\n",
      "Epoch [1/1], Step [3078/3504], Loss: 0.2897\n",
      "Epoch [1/1], Step [3079/3504], Loss: 0.3267\n",
      "Epoch [1/1], Step [3080/3504], Loss: 0.3389\n",
      "Epoch [1/1], Step [3081/3504], Loss: 0.3910\n",
      "Epoch [1/1], Step [3082/3504], Loss: 0.5159\n",
      "Epoch [1/1], Step [3083/3504], Loss: 0.2936\n",
      "Epoch [1/1], Step [3084/3504], Loss: 0.4016\n",
      "Epoch [1/1], Step [3085/3504], Loss: 0.2644\n",
      "Epoch [1/1], Step [3086/3504], Loss: 0.3004\n",
      "Epoch [1/1], Step [3087/3504], Loss: 0.3487\n",
      "Epoch [1/1], Step [3088/3504], Loss: 0.3363\n",
      "Epoch [1/1], Step [3089/3504], Loss: 0.2850\n",
      "Epoch [1/1], Step [3090/3504], Loss: 0.3810\n",
      "Epoch [1/1], Step [3091/3504], Loss: 0.3867\n",
      "Epoch [1/1], Step [3092/3504], Loss: 0.3979\n",
      "Epoch [1/1], Step [3093/3504], Loss: 0.2959\n",
      "Epoch [1/1], Step [3094/3504], Loss: 0.3199\n",
      "Epoch [1/1], Step [3095/3504], Loss: 0.3719\n",
      "Epoch [1/1], Step [3096/3504], Loss: 0.2901\n",
      "Epoch [1/1], Step [3097/3504], Loss: 0.3729\n",
      "Epoch [1/1], Step [3098/3504], Loss: 0.3775\n",
      "Epoch [1/1], Step [3099/3504], Loss: 0.3769\n",
      "Epoch [1/1], Step [3100/3504], Loss: 0.3513\n",
      "Epoch [1/1], Step [3101/3504], Loss: 0.4232\n",
      "Epoch [1/1], Step [3102/3504], Loss: 0.2774\n",
      "Epoch [1/1], Step [3103/3504], Loss: 0.2379\n",
      "Epoch [1/1], Step [3104/3504], Loss: 0.3578\n",
      "Epoch [1/1], Step [3105/3504], Loss: 0.3654\n",
      "Epoch [1/1], Step [3106/3504], Loss: 0.3372\n",
      "Epoch [1/1], Step [3107/3504], Loss: 0.3816\n",
      "Epoch [1/1], Step [3108/3504], Loss: 0.3996\n",
      "Epoch [1/1], Step [3109/3504], Loss: 0.3559\n",
      "Epoch [1/1], Step [3110/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [3111/3504], Loss: 0.2653\n",
      "Epoch [1/1], Step [3112/3504], Loss: 0.2909\n",
      "Epoch [1/1], Step [3113/3504], Loss: 0.3542\n",
      "Epoch [1/1], Step [3114/3504], Loss: 0.3191\n",
      "Epoch [1/1], Step [3115/3504], Loss: 0.3325\n",
      "Epoch [1/1], Step [3116/3504], Loss: 0.3293\n",
      "Epoch [1/1], Step [3117/3504], Loss: 0.4105\n",
      "Epoch [1/1], Step [3118/3504], Loss: 0.3559\n",
      "Epoch [1/1], Step [3119/3504], Loss: 0.3225\n",
      "Epoch [1/1], Step [3120/3504], Loss: 0.3213\n",
      "Epoch [1/1], Step [3121/3504], Loss: 0.4490\n",
      "Epoch [1/1], Step [3122/3504], Loss: 0.2621\n",
      "Epoch [1/1], Step [3123/3504], Loss: 0.3722\n",
      "Epoch [1/1], Step [3124/3504], Loss: 0.2436\n",
      "Epoch [1/1], Step [3125/3504], Loss: 0.2997\n",
      "Epoch [1/1], Step [3126/3504], Loss: 0.2697\n",
      "Epoch [1/1], Step [3127/3504], Loss: 0.3846\n",
      "Epoch [1/1], Step [3128/3504], Loss: 0.3815\n",
      "Epoch [1/1], Step [3129/3504], Loss: 0.4032\n",
      "Epoch [1/1], Step [3130/3504], Loss: 0.2595\n",
      "Epoch [1/1], Step [3131/3504], Loss: 0.2735\n",
      "Epoch [1/1], Step [3132/3504], Loss: 0.3340\n",
      "Epoch [1/1], Step [3133/3504], Loss: 0.3691\n",
      "Epoch [1/1], Step [3134/3504], Loss: 0.3687\n",
      "Epoch [1/1], Step [3135/3504], Loss: 0.2924\n",
      "Epoch [1/1], Step [3136/3504], Loss: 0.3964\n",
      "Epoch [1/1], Step [3137/3504], Loss: 0.3630\n",
      "Epoch [1/1], Step [3138/3504], Loss: 0.2439\n",
      "Epoch [1/1], Step [3139/3504], Loss: 0.2898\n",
      "Epoch [1/1], Step [3140/3504], Loss: 0.3452\n",
      "Epoch [1/1], Step [3141/3504], Loss: 0.3557\n",
      "Epoch [1/1], Step [3142/3504], Loss: 0.3802\n",
      "Epoch [1/1], Step [3143/3504], Loss: 0.4042\n",
      "Epoch [1/1], Step [3144/3504], Loss: 0.3250\n",
      "Epoch [1/1], Step [3145/3504], Loss: 0.3424\n",
      "Epoch [1/1], Step [3146/3504], Loss: 0.4323\n",
      "Epoch [1/1], Step [3147/3504], Loss: 0.3282\n",
      "Epoch [1/1], Step [3148/3504], Loss: 0.3368\n",
      "Epoch [1/1], Step [3149/3504], Loss: 0.3501\n",
      "Epoch [1/1], Step [3150/3504], Loss: 0.2779\n",
      "Epoch [1/1], Step [3151/3504], Loss: 0.3457\n",
      "Epoch [1/1], Step [3152/3504], Loss: 0.3635\n",
      "Epoch [1/1], Step [3153/3504], Loss: 0.4478\n",
      "Epoch [1/1], Step [3154/3504], Loss: 0.4216\n",
      "Epoch [1/1], Step [3155/3504], Loss: 0.3825\n",
      "Epoch [1/1], Step [3156/3504], Loss: 0.3393\n",
      "Epoch [1/1], Step [3157/3504], Loss: 0.3464\n",
      "Epoch [1/1], Step [3158/3504], Loss: 0.3070\n",
      "Epoch [1/1], Step [3159/3504], Loss: 0.2544\n",
      "Epoch [1/1], Step [3160/3504], Loss: 0.3234\n",
      "Epoch [1/1], Step [3161/3504], Loss: 0.2500\n",
      "Epoch [1/1], Step [3162/3504], Loss: 0.3822\n",
      "Epoch [1/1], Step [3163/3504], Loss: 0.4352\n",
      "Epoch [1/1], Step [3164/3504], Loss: 0.2445\n",
      "Epoch [1/1], Step [3165/3504], Loss: 0.2783\n",
      "Epoch [1/1], Step [3166/3504], Loss: 0.2778\n",
      "Epoch [1/1], Step [3167/3504], Loss: 0.3179\n",
      "Epoch [1/1], Step [3168/3504], Loss: 0.3126\n",
      "Epoch [1/1], Step [3169/3504], Loss: 0.3430\n",
      "Epoch [1/1], Step [3170/3504], Loss: 0.3295\n",
      "Epoch [1/1], Step [3171/3504], Loss: 0.4094\n",
      "Epoch [1/1], Step [3172/3504], Loss: 0.3400\n",
      "Epoch [1/1], Step [3173/3504], Loss: 0.3525\n",
      "Epoch [1/1], Step [3174/3504], Loss: 0.4152\n",
      "Epoch [1/1], Step [3175/3504], Loss: 0.3996\n",
      "Epoch [1/1], Step [3176/3504], Loss: 0.2953\n",
      "Epoch [1/1], Step [3177/3504], Loss: 0.3318\n",
      "Epoch [1/1], Step [3178/3504], Loss: 0.3034\n",
      "Epoch [1/1], Step [3179/3504], Loss: 0.3597\n",
      "Epoch [1/1], Step [3180/3504], Loss: 0.3793\n",
      "Epoch [1/1], Step [3181/3504], Loss: 0.3284\n",
      "Epoch [1/1], Step [3182/3504], Loss: 0.3565\n",
      "Epoch [1/1], Step [3183/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [3184/3504], Loss: 0.4024\n",
      "Epoch [1/1], Step [3185/3504], Loss: 0.3671\n",
      "Epoch [1/1], Step [3186/3504], Loss: 0.2766\n",
      "Epoch [1/1], Step [3187/3504], Loss: 0.4705\n",
      "Epoch [1/1], Step [3188/3504], Loss: 0.3422\n",
      "Epoch [1/1], Step [3189/3504], Loss: 0.3349\n",
      "Epoch [1/1], Step [3190/3504], Loss: 0.2977\n",
      "Epoch [1/1], Step [3191/3504], Loss: 0.3676\n",
      "Epoch [1/1], Step [3192/3504], Loss: 0.3242\n",
      "Epoch [1/1], Step [3193/3504], Loss: 0.3761\n",
      "Epoch [1/1], Step [3194/3504], Loss: 0.3554\n",
      "Epoch [1/1], Step [3195/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [3196/3504], Loss: 0.3638\n",
      "Epoch [1/1], Step [3197/3504], Loss: 0.4254\n",
      "Epoch [1/1], Step [3198/3504], Loss: 0.3737\n",
      "Epoch [1/1], Step [3199/3504], Loss: 0.3291\n",
      "Epoch [1/1], Step [3200/3504], Loss: 0.3518\n",
      "Epoch [1/1], Step [3201/3504], Loss: 0.4228\n",
      "Epoch [1/1], Step [3202/3504], Loss: 0.3123\n",
      "Epoch [1/1], Step [3203/3504], Loss: 0.4032\n",
      "Epoch [1/1], Step [3204/3504], Loss: 0.2666\n",
      "Epoch [1/1], Step [3205/3504], Loss: 0.3075\n",
      "Epoch [1/1], Step [3206/3504], Loss: 0.3651\n",
      "Epoch [1/1], Step [3207/3504], Loss: 0.3811\n",
      "Epoch [1/1], Step [3208/3504], Loss: 0.3788\n",
      "Epoch [1/1], Step [3209/3504], Loss: 0.4035\n",
      "Epoch [1/1], Step [3210/3504], Loss: 0.2908\n",
      "Epoch [1/1], Step [3211/3504], Loss: 0.2882\n",
      "Epoch [1/1], Step [3212/3504], Loss: 0.2125\n",
      "Epoch [1/1], Step [3213/3504], Loss: 0.3222\n",
      "Epoch [1/1], Step [3214/3504], Loss: 0.3763\n",
      "Epoch [1/1], Step [3215/3504], Loss: 0.3002\n",
      "Epoch [1/1], Step [3216/3504], Loss: 0.4485\n",
      "Epoch [1/1], Step [3217/3504], Loss: 0.3886\n",
      "Epoch [1/1], Step [3218/3504], Loss: 0.4538\n",
      "Epoch [1/1], Step [3219/3504], Loss: 0.2553\n",
      "Epoch [1/1], Step [3220/3504], Loss: 0.3553\n",
      "Epoch [1/1], Step [3221/3504], Loss: 0.3655\n",
      "Epoch [1/1], Step [3222/3504], Loss: 0.3839\n",
      "Epoch [1/1], Step [3223/3504], Loss: 0.2998\n",
      "Epoch [1/1], Step [3224/3504], Loss: 0.2815\n",
      "Epoch [1/1], Step [3225/3504], Loss: 0.3180\n",
      "Epoch [1/1], Step [3226/3504], Loss: 0.4445\n",
      "Epoch [1/1], Step [3227/3504], Loss: 0.3469\n",
      "Epoch [1/1], Step [3228/3504], Loss: 0.3085\n",
      "Epoch [1/1], Step [3229/3504], Loss: 0.2900\n",
      "Epoch [1/1], Step [3230/3504], Loss: 0.4129\n",
      "Epoch [1/1], Step [3231/3504], Loss: 0.3559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [3232/3504], Loss: 0.3767\n",
      "Epoch [1/1], Step [3233/3504], Loss: 0.3686\n",
      "Epoch [1/1], Step [3234/3504], Loss: 0.3345\n",
      "Epoch [1/1], Step [3235/3504], Loss: 0.2976\n",
      "Epoch [1/1], Step [3236/3504], Loss: 0.3657\n",
      "Epoch [1/1], Step [3237/3504], Loss: 0.2873\n",
      "Epoch [1/1], Step [3238/3504], Loss: 0.2510\n",
      "Epoch [1/1], Step [3239/3504], Loss: 0.4376\n",
      "Epoch [1/1], Step [3240/3504], Loss: 0.3469\n",
      "Epoch [1/1], Step [3241/3504], Loss: 0.3344\n",
      "Epoch [1/1], Step [3242/3504], Loss: 0.3347\n",
      "Epoch [1/1], Step [3243/3504], Loss: 0.3183\n",
      "Epoch [1/1], Step [3244/3504], Loss: 0.2843\n",
      "Epoch [1/1], Step [3245/3504], Loss: 0.3073\n",
      "Epoch [1/1], Step [3246/3504], Loss: 0.4021\n",
      "Epoch [1/1], Step [3247/3504], Loss: 0.3325\n",
      "Epoch [1/1], Step [3248/3504], Loss: 0.3579\n",
      "Epoch [1/1], Step [3249/3504], Loss: 0.4036\n",
      "Epoch [1/1], Step [3250/3504], Loss: 0.2990\n",
      "Epoch [1/1], Step [3251/3504], Loss: 0.2818\n",
      "Epoch [1/1], Step [3252/3504], Loss: 0.3494\n",
      "Epoch [1/1], Step [3253/3504], Loss: 0.2568\n",
      "Epoch [1/1], Step [3254/3504], Loss: 0.3352\n",
      "Epoch [1/1], Step [3255/3504], Loss: 0.3010\n",
      "Epoch [1/1], Step [3256/3504], Loss: 0.3826\n",
      "Epoch [1/1], Step [3257/3504], Loss: 0.3742\n",
      "Epoch [1/1], Step [3258/3504], Loss: 0.3816\n",
      "Epoch [1/1], Step [3259/3504], Loss: 0.3960\n",
      "Epoch [1/1], Step [3260/3504], Loss: 0.3406\n",
      "Epoch [1/1], Step [3261/3504], Loss: 0.3284\n",
      "Epoch [1/1], Step [3262/3504], Loss: 0.3266\n",
      "Epoch [1/1], Step [3263/3504], Loss: 0.2625\n",
      "Epoch [1/1], Step [3264/3504], Loss: 0.3602\n",
      "Epoch [1/1], Step [3265/3504], Loss: 0.2961\n",
      "Epoch [1/1], Step [3266/3504], Loss: 0.3037\n",
      "Epoch [1/1], Step [3267/3504], Loss: 0.3771\n",
      "Epoch [1/1], Step [3268/3504], Loss: 0.3391\n",
      "Epoch [1/1], Step [3269/3504], Loss: 0.3798\n",
      "Epoch [1/1], Step [3270/3504], Loss: 0.3086\n",
      "Epoch [1/1], Step [3271/3504], Loss: 0.3889\n",
      "Epoch [1/1], Step [3272/3504], Loss: 0.3800\n",
      "Epoch [1/1], Step [3273/3504], Loss: 0.3478\n",
      "Epoch [1/1], Step [3274/3504], Loss: 0.3392\n",
      "Epoch [1/1], Step [3275/3504], Loss: 0.2923\n",
      "Epoch [1/1], Step [3276/3504], Loss: 0.3987\n",
      "Epoch [1/1], Step [3277/3504], Loss: 0.3714\n",
      "Epoch [1/1], Step [3278/3504], Loss: 0.3440\n",
      "Epoch [1/1], Step [3279/3504], Loss: 0.3574\n",
      "Epoch [1/1], Step [3280/3504], Loss: 0.3299\n",
      "Epoch [1/1], Step [3281/3504], Loss: 0.3311\n",
      "Epoch [1/1], Step [3282/3504], Loss: 0.3580\n",
      "Epoch [1/1], Step [3283/3504], Loss: 0.3739\n",
      "Epoch [1/1], Step [3284/3504], Loss: 0.3765\n",
      "Epoch [1/1], Step [3285/3504], Loss: 0.3315\n",
      "Epoch [1/1], Step [3286/3504], Loss: 0.2904\n",
      "Epoch [1/1], Step [3287/3504], Loss: 0.3255\n",
      "Epoch [1/1], Step [3288/3504], Loss: 0.2968\n",
      "Epoch [1/1], Step [3289/3504], Loss: 0.4145\n",
      "Epoch [1/1], Step [3290/3504], Loss: 0.3320\n",
      "Epoch [1/1], Step [3291/3504], Loss: 0.3939\n",
      "Epoch [1/1], Step [3292/3504], Loss: 0.3633\n",
      "Epoch [1/1], Step [3293/3504], Loss: 0.3615\n",
      "Epoch [1/1], Step [3294/3504], Loss: 0.3539\n",
      "Epoch [1/1], Step [3295/3504], Loss: 0.3349\n",
      "Epoch [1/1], Step [3296/3504], Loss: 0.3135\n",
      "Epoch [1/1], Step [3297/3504], Loss: 0.3557\n",
      "Epoch [1/1], Step [3298/3504], Loss: 0.4432\n",
      "Epoch [1/1], Step [3299/3504], Loss: 0.3399\n",
      "Epoch [1/1], Step [3300/3504], Loss: 0.3545\n",
      "Epoch [1/1], Step [3301/3504], Loss: 0.3561\n",
      "Epoch [1/1], Step [3302/3504], Loss: 0.3836\n",
      "Epoch [1/1], Step [3303/3504], Loss: 0.3634\n",
      "Epoch [1/1], Step [3304/3504], Loss: 0.3679\n",
      "Epoch [1/1], Step [3305/3504], Loss: 0.4460\n",
      "Epoch [1/1], Step [3306/3504], Loss: 0.3629\n",
      "Epoch [1/1], Step [3307/3504], Loss: 0.4687\n",
      "Epoch [1/1], Step [3308/3504], Loss: 0.3230\n",
      "Epoch [1/1], Step [3309/3504], Loss: 0.3023\n",
      "Epoch [1/1], Step [3310/3504], Loss: 0.3487\n",
      "Epoch [1/1], Step [3311/3504], Loss: 0.3289\n",
      "Epoch [1/1], Step [3312/3504], Loss: 0.3686\n",
      "Epoch [1/1], Step [3313/3504], Loss: 0.3177\n",
      "Epoch [1/1], Step [3314/3504], Loss: 0.2937\n",
      "Epoch [1/1], Step [3315/3504], Loss: 0.3124\n",
      "Epoch [1/1], Step [3316/3504], Loss: 0.3327\n",
      "Epoch [1/1], Step [3317/3504], Loss: 0.3633\n",
      "Epoch [1/1], Step [3318/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [3319/3504], Loss: 0.3659\n",
      "Epoch [1/1], Step [3320/3504], Loss: 0.3020\n",
      "Epoch [1/1], Step [3321/3504], Loss: 0.3084\n",
      "Epoch [1/1], Step [3322/3504], Loss: 0.3157\n",
      "Epoch [1/1], Step [3323/3504], Loss: 0.3434\n",
      "Epoch [1/1], Step [3324/3504], Loss: 0.4623\n",
      "Epoch [1/1], Step [3325/3504], Loss: 0.2928\n",
      "Epoch [1/1], Step [3326/3504], Loss: 0.3450\n",
      "Epoch [1/1], Step [3327/3504], Loss: 0.2567\n",
      "Epoch [1/1], Step [3328/3504], Loss: 0.4164\n",
      "Epoch [1/1], Step [3329/3504], Loss: 0.3600\n",
      "Epoch [1/1], Step [3330/3504], Loss: 0.3069\n",
      "Epoch [1/1], Step [3331/3504], Loss: 0.2688\n",
      "Epoch [1/1], Step [3332/3504], Loss: 0.3267\n",
      "Epoch [1/1], Step [3333/3504], Loss: 0.3306\n",
      "Epoch [1/1], Step [3334/3504], Loss: 0.3575\n",
      "Epoch [1/1], Step [3335/3504], Loss: 0.3417\n",
      "Epoch [1/1], Step [3336/3504], Loss: 0.4074\n",
      "Epoch [1/1], Step [3337/3504], Loss: 0.3640\n",
      "Epoch [1/1], Step [3338/3504], Loss: 0.3479\n",
      "Epoch [1/1], Step [3339/3504], Loss: 0.3339\n",
      "Epoch [1/1], Step [3340/3504], Loss: 0.3283\n",
      "Epoch [1/1], Step [3341/3504], Loss: 0.3762\n",
      "Epoch [1/1], Step [3342/3504], Loss: 0.3654\n",
      "Epoch [1/1], Step [3343/3504], Loss: 0.3347\n",
      "Epoch [1/1], Step [3344/3504], Loss: 0.4146\n",
      "Epoch [1/1], Step [3345/3504], Loss: 0.4339\n",
      "Epoch [1/1], Step [3346/3504], Loss: 0.4113\n",
      "Epoch [1/1], Step [3347/3504], Loss: 0.4717\n",
      "Epoch [1/1], Step [3348/3504], Loss: 0.4333\n",
      "Epoch [1/1], Step [3349/3504], Loss: 0.3095\n",
      "Epoch [1/1], Step [3350/3504], Loss: 0.3214\n",
      "Epoch [1/1], Step [3351/3504], Loss: 0.3832\n",
      "Epoch [1/1], Step [3352/3504], Loss: 0.2661\n",
      "Epoch [1/1], Step [3353/3504], Loss: 0.3660\n",
      "Epoch [1/1], Step [3354/3504], Loss: 0.3331\n",
      "Epoch [1/1], Step [3355/3504], Loss: 0.3450\n",
      "Epoch [1/1], Step [3356/3504], Loss: 0.3573\n",
      "Epoch [1/1], Step [3357/3504], Loss: 0.2993\n",
      "Epoch [1/1], Step [3358/3504], Loss: 0.3919\n",
      "Epoch [1/1], Step [3359/3504], Loss: 0.2867\n",
      "Epoch [1/1], Step [3360/3504], Loss: 0.4284\n",
      "Epoch [1/1], Step [3361/3504], Loss: 0.4143\n",
      "Epoch [1/1], Step [3362/3504], Loss: 0.3130\n",
      "Epoch [1/1], Step [3363/3504], Loss: 0.2889\n",
      "Epoch [1/1], Step [3364/3504], Loss: 0.3265\n",
      "Epoch [1/1], Step [3365/3504], Loss: 0.3477\n",
      "Epoch [1/1], Step [3366/3504], Loss: 0.4398\n",
      "Epoch [1/1], Step [3367/3504], Loss: 0.2475\n",
      "Epoch [1/1], Step [3368/3504], Loss: 0.3281\n",
      "Epoch [1/1], Step [3369/3504], Loss: 0.4035\n",
      "Epoch [1/1], Step [3370/3504], Loss: 0.3268\n",
      "Epoch [1/1], Step [3371/3504], Loss: 0.2938\n",
      "Epoch [1/1], Step [3372/3504], Loss: 0.3482\n",
      "Epoch [1/1], Step [3373/3504], Loss: 0.2936\n",
      "Epoch [1/1], Step [3374/3504], Loss: 0.3241\n",
      "Epoch [1/1], Step [3375/3504], Loss: 0.3512\n",
      "Epoch [1/1], Step [3376/3504], Loss: 0.3876\n",
      "Epoch [1/1], Step [3377/3504], Loss: 0.4478\n",
      "Epoch [1/1], Step [3378/3504], Loss: 0.4197\n",
      "Epoch [1/1], Step [3379/3504], Loss: 0.3280\n",
      "Epoch [1/1], Step [3380/3504], Loss: 0.2662\n",
      "Epoch [1/1], Step [3381/3504], Loss: 0.3133\n",
      "Epoch [1/1], Step [3382/3504], Loss: 0.3543\n",
      "Epoch [1/1], Step [3383/3504], Loss: 0.2946\n",
      "Epoch [1/1], Step [3384/3504], Loss: 0.3130\n",
      "Epoch [1/1], Step [3385/3504], Loss: 0.3961\n",
      "Epoch [1/1], Step [3386/3504], Loss: 0.3386\n",
      "Epoch [1/1], Step [3387/3504], Loss: 0.4955\n",
      "Epoch [1/1], Step [3388/3504], Loss: 0.3581\n",
      "Epoch [1/1], Step [3389/3504], Loss: 0.3135\n",
      "Epoch [1/1], Step [3390/3504], Loss: 0.3216\n",
      "Epoch [1/1], Step [3391/3504], Loss: 0.4441\n",
      "Epoch [1/1], Step [3392/3504], Loss: 0.3670\n",
      "Epoch [1/1], Step [3393/3504], Loss: 0.3081\n",
      "Epoch [1/1], Step [3394/3504], Loss: 0.4822\n",
      "Epoch [1/1], Step [3395/3504], Loss: 0.3611\n",
      "Epoch [1/1], Step [3396/3504], Loss: 0.3365\n",
      "Epoch [1/1], Step [3397/3504], Loss: 0.3775\n",
      "Epoch [1/1], Step [3398/3504], Loss: 0.3101\n",
      "Epoch [1/1], Step [3399/3504], Loss: 0.3048\n",
      "Epoch [1/1], Step [3400/3504], Loss: 0.3147\n",
      "Epoch [1/1], Step [3401/3504], Loss: 0.3458\n",
      "Epoch [1/1], Step [3402/3504], Loss: 0.4316\n",
      "Epoch [1/1], Step [3403/3504], Loss: 0.3413\n",
      "Epoch [1/1], Step [3404/3504], Loss: 0.3081\n",
      "Epoch [1/1], Step [3405/3504], Loss: 0.4709\n",
      "Epoch [1/1], Step [3406/3504], Loss: 0.4135\n",
      "Epoch [1/1], Step [3407/3504], Loss: 0.3435\n",
      "Epoch [1/1], Step [3408/3504], Loss: 0.2769\n",
      "Epoch [1/1], Step [3409/3504], Loss: 0.2856\n",
      "Epoch [1/1], Step [3410/3504], Loss: 0.3094\n",
      "Epoch [1/1], Step [3411/3504], Loss: 0.3639\n",
      "Epoch [1/1], Step [3412/3504], Loss: 0.4108\n",
      "Epoch [1/1], Step [3413/3504], Loss: 0.3825\n",
      "Epoch [1/1], Step [3414/3504], Loss: 0.3375\n",
      "Epoch [1/1], Step [3415/3504], Loss: 0.3540\n",
      "Epoch [1/1], Step [3416/3504], Loss: 0.3118\n",
      "Epoch [1/1], Step [3417/3504], Loss: 0.3861\n",
      "Epoch [1/1], Step [3418/3504], Loss: 0.4111\n",
      "Epoch [1/1], Step [3419/3504], Loss: 0.3419\n",
      "Epoch [1/1], Step [3420/3504], Loss: 0.3206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [3421/3504], Loss: 0.3677\n",
      "Epoch [1/1], Step [3422/3504], Loss: 0.3064\n",
      "Epoch [1/1], Step [3423/3504], Loss: 0.3467\n",
      "Epoch [1/1], Step [3424/3504], Loss: 0.2486\n",
      "Epoch [1/1], Step [3425/3504], Loss: 0.3291\n",
      "Epoch [1/1], Step [3426/3504], Loss: 0.3319\n",
      "Epoch [1/1], Step [3427/3504], Loss: 0.3988\n",
      "Epoch [1/1], Step [3428/3504], Loss: 0.4001\n",
      "Epoch [1/1], Step [3429/3504], Loss: 0.4043\n",
      "Epoch [1/1], Step [3430/3504], Loss: 0.3626\n",
      "Epoch [1/1], Step [3431/3504], Loss: 0.4242\n",
      "Epoch [1/1], Step [3432/3504], Loss: 0.3481\n",
      "Epoch [1/1], Step [3433/3504], Loss: 0.3698\n",
      "Epoch [1/1], Step [3434/3504], Loss: 0.3374\n",
      "Epoch [1/1], Step [3435/3504], Loss: 0.3982\n",
      "Epoch [1/1], Step [3436/3504], Loss: 0.3442\n",
      "Epoch [1/1], Step [3437/3504], Loss: 0.4166\n",
      "Epoch [1/1], Step [3438/3504], Loss: 0.3628\n",
      "Epoch [1/1], Step [3439/3504], Loss: 0.3480\n",
      "Epoch [1/1], Step [3440/3504], Loss: 0.3119\n",
      "Epoch [1/1], Step [3441/3504], Loss: 0.3623\n",
      "Epoch [1/1], Step [3442/3504], Loss: 0.3530\n",
      "Epoch [1/1], Step [3443/3504], Loss: 0.3072\n",
      "Epoch [1/1], Step [3444/3504], Loss: 0.3358\n",
      "Epoch [1/1], Step [3445/3504], Loss: 0.2617\n",
      "Epoch [1/1], Step [3446/3504], Loss: 0.2878\n",
      "Epoch [1/1], Step [3447/3504], Loss: 0.3463\n",
      "Epoch [1/1], Step [3448/3504], Loss: 0.4228\n",
      "Epoch [1/1], Step [3449/3504], Loss: 0.4688\n",
      "Epoch [1/1], Step [3450/3504], Loss: 0.6211\n",
      "Epoch [1/1], Step [3451/3504], Loss: 0.3340\n",
      "Epoch [1/1], Step [3452/3504], Loss: 0.3423\n",
      "Epoch [1/1], Step [3453/3504], Loss: 0.2858\n",
      "Epoch [1/1], Step [3454/3504], Loss: 0.2951\n",
      "Epoch [1/1], Step [3455/3504], Loss: 0.4505\n",
      "Epoch [1/1], Step [3456/3504], Loss: 0.3732\n",
      "Epoch [1/1], Step [3457/3504], Loss: 0.3472\n",
      "Epoch [1/1], Step [3458/3504], Loss: 0.3178\n",
      "Epoch [1/1], Step [3459/3504], Loss: 0.4328\n",
      "Epoch [1/1], Step [3460/3504], Loss: 0.2837\n",
      "Epoch [1/1], Step [3461/3504], Loss: 0.3216\n",
      "Epoch [1/1], Step [3462/3504], Loss: 0.2421\n",
      "Epoch [1/1], Step [3463/3504], Loss: 0.3246\n",
      "Epoch [1/1], Step [3464/3504], Loss: 0.3674\n",
      "Epoch [1/1], Step [3465/3504], Loss: 0.2989\n",
      "Epoch [1/1], Step [3466/3504], Loss: 0.2459\n",
      "Epoch [1/1], Step [3467/3504], Loss: 0.3659\n",
      "Epoch [1/1], Step [3468/3504], Loss: 0.3178\n",
      "Epoch [1/1], Step [3469/3504], Loss: 0.4105\n",
      "Epoch [1/1], Step [3470/3504], Loss: 0.2956\n",
      "Epoch [1/1], Step [3471/3504], Loss: 0.3581\n",
      "Epoch [1/1], Step [3472/3504], Loss: 0.3271\n",
      "Epoch [1/1], Step [3473/3504], Loss: 0.4220\n",
      "Epoch [1/1], Step [3474/3504], Loss: 0.3364\n",
      "Epoch [1/1], Step [3475/3504], Loss: 0.4594\n",
      "Epoch [1/1], Step [3476/3504], Loss: 0.2652\n",
      "Epoch [1/1], Step [3477/3504], Loss: 0.2912\n",
      "Epoch [1/1], Step [3478/3504], Loss: 0.3105\n",
      "Epoch [1/1], Step [3479/3504], Loss: 0.3014\n",
      "Epoch [1/1], Step [3480/3504], Loss: 0.3874\n",
      "Epoch [1/1], Step [3481/3504], Loss: 0.3654\n",
      "Epoch [1/1], Step [3482/3504], Loss: 0.3370\n",
      "Epoch [1/1], Step [3483/3504], Loss: 0.3534\n",
      "Epoch [1/1], Step [3484/3504], Loss: 0.3255\n",
      "Epoch [1/1], Step [3485/3504], Loss: 0.3591\n",
      "Epoch [1/1], Step [3486/3504], Loss: 0.3192\n",
      "Epoch [1/1], Step [3487/3504], Loss: 0.3301\n",
      "Epoch [1/1], Step [3488/3504], Loss: 0.2997\n",
      "Epoch [1/1], Step [3489/3504], Loss: 0.3607\n",
      "Epoch [1/1], Step [3490/3504], Loss: 0.2689\n",
      "Epoch [1/1], Step [3491/3504], Loss: 0.2964\n",
      "Epoch [1/1], Step [3492/3504], Loss: 0.3449\n",
      "Epoch [1/1], Step [3493/3504], Loss: 0.3347\n",
      "Epoch [1/1], Step [3494/3504], Loss: 0.3074\n",
      "Epoch [1/1], Step [3495/3504], Loss: 0.3565\n",
      "Epoch [1/1], Step [3496/3504], Loss: 0.3682\n",
      "Epoch [1/1], Step [3497/3504], Loss: 0.4199\n",
      "Epoch [1/1], Step [3498/3504], Loss: 0.3755\n",
      "Epoch [1/1], Step [3499/3504], Loss: 0.3500\n",
      "Epoch [1/1], Step [3500/3504], Loss: 0.3728\n",
      "Epoch [1/1], Step [3501/3504], Loss: 0.3944\n",
      "Epoch [1/1], Step [3502/3504], Loss: 0.3081\n",
      "Epoch [1/1], Step [3503/3504], Loss: 0.2534\n",
      "Epoch [1/1], Step [3504/3504], Loss: 0.0992\n",
      "Epoch [1/1], Train Accuracy: 0.2979\n",
      "Epoch [1/1], Test Accuracy: 0.2950\n"
     ]
    }
   ],
   "source": [
    "train_accs2, test_accs2 = trainer(paper_model, train_dataloader, test_dataloader, 1, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d250f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
